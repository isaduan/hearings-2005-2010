S.  HRG.  110–1158 

IMPROVING  THE  CAPACITY  OF 
U.S.  CLIMATE  MODELING  FOR 

DECISION-MAKERS  AND  END-USERS 

HEARING 

BEFORE THE 

COMMITTEE ON COMMERCE, 

SCIENCE, AND TRANSPORTATION 

UNITED STATES SENATE 
ONE  HUNDRED  TENTH  CONGRESS 

SECOND  SESSION 

MAY  8,  2008 

Printed  for  the  use  of  the  Committee  on  Commerce,  Science,  and  Transportation 

( 

75–346 PDF 

WASHINGTON  : 

2012 

U.S.  GOVERNMENT  PRINTING  OFFICE

For sale by the Superintendent of Documents, U.S. Government Printing Office

Internet: bookstore.gpo.gov Phone: toll free (866) 512–1800; DC area (202) 512–1800

Fax: (202) 512–2104 Mail: Stop IDCC, Washington, DC 20402–0001

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00001 Fmt 5011 Sfmt 5011 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

SENATE  COMMITTEE  ON  COMMERCE,  SCIENCE,  AND  TRANSPORTATION 

ONE  HUNDRED  TENTH  CONGRESS 

SECOND  SESSION 

DANIEL  K.  INOUYE,  Hawaii,  Chairman 

JOHN  D.  ROCKEFELLER  IV,  West  Virginia 
JOHN  F.  KERRY,  Massachusetts 
BYRON  L.  DORGAN,  North  Dakota 
BARBARA  BOXER,  California 
BILL  NELSON,  Florida 
MARIA  CANTWELL,  Washington 
FRANK  R.  LAUTENBERG,  New  Jersey 
MARK  PRYOR,  Arkansas 
THOMAS  R.  CARPER,  Delaware 
CLAIRE  MCCASKILL,  Missouri 
AMY  KLOBUCHAR,  Minnesota 

TED  STEVENS,  Alaska,  Vice  Chairman 
JOHN  MCCAIN,  Arizona 
KAY  BAILEY  HUTCHISON,  Texas 
OLYMPIA  J.  SNOWE,  Maine 
GORDON  H.  SMITH,  Oregon 
JOHN  ENSIGN,  Nevada 
JOHN  E.  SUNUNU,  New  Hampshire 
JIM  DEMINT,  South  Carolina 
DAVID  VITTER,  Louisiana 
JOHN  THUNE,  South  Dakota 
ROGER  F.  WICKER,  Mississippi 

MARGARET L.  CUMMISKY,  Democratic  Staff  Director  and  Chief  Counsel 

LILA HARPER HELMS,  Democratic  Deputy  Staff  Director  and  Policy  Director 

CHRISTINE D.  KURTH,  Republican  Staff  Director  and  General  Counsel 

PAUL NAGLE,  Republican  Chief  Counsel 

(II) 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00002 Fmt 5904 Sfmt 5904 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

C O N T E N T S 

Hearing held on May 8, 2008 
.................................................................................
Statement of Senator Kerry  ....................................................................................
Statement of Senator Stevens 
................................................................................

WITNESSES 

Carlisle,  Bruce  K.,  Assistant  Director,  Office  of  Coastal  Zone  Management, 
Executive  Office  of  Energy,  and  Environmental  Affairs,  Commonwealth 
of Massachusetts  ..................................................................................................
Prepared statement  ..........................................................................................
Hack, Ph. D., James J., Director, National Center for Computational Sciences, 
Oak Ridge National Laboratory  ..........................................................................
Prepared statement  ..........................................................................................
MacDonald,  Dr.  Alexander  (Sandy),  Deputy  Assistant  Administrator  for  Lab-
oratories  and  Cooperative  Institutes,  Office  of  Oceanic  and  Atmospheric 
Research, NOAA, U.S. Department of Commerce  .............................................
Prepared statement  ..........................................................................................
Reed, Daniel A., Chair, Computing Research Association (CRA)  ........................
Prepared statement  ..........................................................................................
Sarachik,  Edward,  Emeritus  Professor  of  Atmospheric  Science,  Adjunct  Pro-
fessor  of  Oceanography,  and  Adjunct  Professor  of  Applied  Mathematics 
at  the  University  of  Washington  and  Co-Director,  Center  for  Science  in 
the Earth System  .................................................................................................
Prepared statement  ..........................................................................................
Walsh,  John  E.,  Director,  Cooperative  Institute  for  Arctic  Research,  Inter-
national Arctic Research Center, University of Alaska  ....................................
Prepared statement  ..........................................................................................

APPENDIX 

Response  to  written  questions  submitted  by  Hon.  Daniel  K.  Inouye  to 
Dr. Sandy MacDonald  ..........................................................................................

Response to written questions submitted by Hon. John F. Kerry to: 

Bruce K. Carlisle  ..............................................................................................
Dr. Alexander (Sandy) MacDonald 
.................................................................
Dr. Daniel A. Reed 
...........................................................................................
Edward Sarachik  ..............................................................................................

Page 
1 
1 
3 

30 
32 

11 
13 

3 
5 
19 
21 

26 
28 

50 
52 

68 

67 
68 
71 
71 

(III) 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00003 Fmt 5904 Sfmt 5904 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00004 Fmt 5904 Sfmt 5904 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

IMPROVING  THE  CAPACITY  OF 
U.S.  CLIMATE  MODELING  FOR 

DECISION-MAKERS  AND  END-USERS 

THURSDAY,  MAY  8,  2008 

COMMITTEE ON COMMERCE, SCIENCE, AND TRANSPORTATION, 

U.S. SENATE, 

Washington, DC. 

The  Committee  met,  pursuant  to  notice,  at  2:41  p.m.,  in  room 
SR–253,  Russell  Senate  Office  Building,  Hon.  John  F.  Kerry,  pre-
siding. 

OPENING STATEMENT OF HON. JOHN F. KERRY, 

U.S. SENATOR FROM MASSACHUSETTS 

Senator  KERRY.  We  will  officially  begin.  I  will  not  call  you  to 
order  because  I  have  never  seen  such  a  quiet,  almost  somnam-
bulant crew. Are you all right? This whole audience is quiet. 

Welcome. We are really, really happy to have you here today and 
I  apologize  for  the  delay.  We  just  have  a  little  bit  too  much  going 
on right now. 

This is really an essential component of our Nation’s effort to try 
to  understand  climate  change,  and  as  you  know,  we  are  going  to 
be  having  a  very  important  debate  here  in  a  matter  of  weeks  on 
this  subject  in  the  form  of  our  cap  and  trade  legislation.  A  lot  of 
issues  are  being  raised  and  are  continually  being  raised  with  re-
spect to our ability to be able to understand future climate impacts 
and  what  our  response  ought  to  be  to  those  impacts.  A  lot  of  that 
will enter into the debate. No question. 

We are in good company, in a sense, today. There may not be as 
many  of  us  as  there  are  in  London,  but  in  England  today  150  of 
the  world’s  top  climate  modelers  are  meeting,  focusing  on  exactly 
the same set of issues that we are talking about here today. 

As  our  panel  well  knows,  but  just  for  the  public’s  understanding 
and  the  record,  we  want  to  emphasize  that  climate  modeling  has 
been  a  subject  of  this  Committee’s  inquiry  for  over  20  years  now. 
Al  Gore  and  I  held  the  first  hearings  back  in  1987,  and  then  in 
1988 Jim Hanson made the first comments with respect to climate 
change  being  upon  us.  Subsequently  we  have  had  many  different 
hearings and meetings to try to better understand how we can de-
fine  to  people  what  we  are  looking  at  and  what  to  expect.  For  the 
public, it is obviously very important in terms of policy. 

These models are the basis for the predictions of future tempera-
ture  increases,  sea  level  rise,  storm  surge,  and  the  other  impacts 
of global climate change. To date, the U.S. Government has played 

(1) 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00005 Fmt 6633 Sfmt 6633 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

2 

a key role in developing several of the world’s best and most accu-
rate  models  which  serve  as  the  basis  for  much  of  the  information 
in  the  Fourth  Assessment  Report  of  the  Intergovernmental  Panel 
on Climate Change. 

However, we are now beginning to understand the limitations of 
these models. They currently cannot provide us with predictions on 
a  regional  or  local  level,  and  they  cannot  provide  us  information 
that is essential for states, communities, and resource managers as 
they  adapt  to  the  localized  impacts  of  climate  change.  The  models 
also  tend  to  provide  information  over  a  long  horizon,  a  long  period 
of  time,  rather  than  on  a  decadal  scale  that  would  be  more  useful 
to some of these end-users. 

In  addition,  the  models  are  currently  not  capable  of  identifying 
potential thresholds or tipping points which could also result in ab-
rupt climate change impacts. 

One  of  the  key  issues  that  we  are  going  to  explore  today  is  the 
issue of computing capacity. There are a number of different limita-
tions.  To  run  the  models  at  the  desired  resolution,  we  need  super-
computers  a  thousand  times  more  powerful  than  we  have  today. 
While  the  United  States  has  some  of  the  most  powerful  supercom-
puting  facilities  in  the  world,  we  do  not  have  a  structure  or  strat-
egy in place to coordinate the hardware, software, networking, and 
data  storage  functions  required  to  produce  the  type  of  information 
that we need. 

As  we  consider  this  multitude  of  overlapping  functions,  our  ef-
forts  have  to  be  driven  by  the  ultimate  needs  of  the  end-users  of 
this  information,  and  I  hope  that  Bruce  Carlisle  is  going  to  keep 
us  focused  on  that  today.  Bruce  is  the  Assistant  Director  of  the 
Massachusetts  Office  of  Coastal  Zone  Management.  He  has  been 
one  of  the  leaders  in  creating  a  new  state  program  called 
StormSmart  Coasts,  and  this  excellent  program  provides  Massa-
chusetts’ cities and towns with the information and tools that they 
need to protect themselves from coastal storm damage and prepare 
for  the  impacts  of  climate  change  and  rising  sea  levels.  This  first- 
of-a-kind  program  will  serve  as  a  model  for  the  country,  and  I  am 
proud of the work that has taken place in my home State. 

I  would  like  to  briefly  also  introduce  the  other  witnesses  and 
then  recognize  our  Ranking  Member  of  the  Committee,  Senator 
Stevens, and then welcome your testimony. 

Dr.  Sandy  MacDonald,  Director  of  the  Earth  System  Research 
Laboratory  at  the  National  Oceanic  and  Atmospheric  Administra-
tion; Dr. James Hack, Director of the National Center for Computa-
tional  Sciences  at  the  Oak  Ridge  National  Laboratory;  Dr.  Daniel 
A. Reed, Scalable and Multicore Computing Strategist at Microsoft; 
Dr.  Edward  Sarachik,  Co-Director  of  the  Center  for  Science  in  the 
Earth  System  at  the  University  of  Washington;  and  Dr.  John 
Walsh, Chief Scientist at the International Arctic Research Center, 
University of Alaska Fairbanks. 

Gentlemen,  we  are  deeply  appreciative  for  your  taking  time, 
some of you to travel considerable distance, and all of you to bring 
your expertise to the Committee today. We appreciate it very, very 
much. 

Senator Stevens? 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00006 Fmt 6633 Sfmt 6633 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

3 

STATEMENT OF HON. TED STEVENS, 

U.S. SENATOR FROM ALASKA 

Senator STEVENS. Thank you very much, Mr. Chairman. Sorry to 
be  a  little  bit  late.  I  thought  we  were  going  to  have  a  vote,  but  it 
was canceled. 

Senator  KERRY.  So  did  I  actually.  I  went  over  there  and  that  is 
why  I  was  late.  I  thought  I  was  going  to  vote  early  and  get  back, 
but I wound up being late. 

Senator  STEVENS.  I  think  everyone  knows  that  Alaskans  depend 
upon  timely,  accurate  climate  information  for  decisionmaking  on  a 
range  of  issues.  The  same  with  the  rest  of  the  country,  home  con-
struction,  transportation.  But  we  particularly  need  it  for  fisheries 
and resource management. And I do support research and develop-
ment  of  climate  models  that  provide  this  information  to  Federal 
and  State  agencies,  as  well  as  local  people  who  depend  on  it  every 
day. 

I  remain  concerned,  however,  that  the  recent  climate  models 
with site-specific data may not be accurate enough for the planning 
on the State and community levels. It is essential that we have not 
only  the  best  model  capabilities,  but  also  comprehensive  climate 
data  to  use  in  those  model  simulations.  Our  Nation  should  make 
it  a  priority  to  improve  both  climate  modeling  and  access  to  the 
necessary supercomputing infrastructure. 

I  welcome  the  witnesses  here  today,  as  the  Chairman  has,  in-
cluding  Dr.  John  Walsh  who  has  traveled  all  the  way  from  the 
International  Arctic  Research  Center  in  Fairbanks  to  be  here.  I 
look forward to your testimony, John. Nice to have you here. 

I  thank  you  very  much  for  holding  the  hearing,  Mr.  Chairman. 
Senator KERRY. Thank you very much, Senator Stevens. 
So  if  we  could  begin  with  you,  Dr.  MacDonald,  and  we  will  just 
run right down the line. And if I could ask everybody—each of your 
testimonies  will  be  placed  in  writing  in  the  record  in  full,  and  if 
you could summarize in approximately 5 minutes, that way we can 
have a little more time to engage in a discussion between the pan-
elists and ourselves. Thank you. Dr. MacDonald? 

STATEMENT  OF  DR.  ALEXANDER 

(SANDY)  MACDONALD, 
DEPUTY  ASSISTANT  ADMINISTRATOR  FOR  LABORATORIES 
AND  COOPERATIVE  INSTITUTES,  OFFICE  OF  OCEANIC  AND 
ATMOSPHERIC  RESEARCH,  NOAA,  U.S.  DEPARTMENT  OF 
COMMERCE 
Dr. MACDONALD. Good afternoon, Senator Kerry and Vice Chair-
man  Stevens.  I  am  Dr.  Alexander  MacDonald.  My  friends  call  me 
Sandy.  My  job  is  Deputy  Assistant  Administrator  for  NOAA  Re-
search. And I thank you for inviting me to discuss the key role that 
NOAA has played in improving understanding and prediction of cli-
mate through the use of models. 

Advancement of the scientific community’s knowledge and under-
standing of the way our planet’s climate system works comes from 
three steps. One of them is that we improve our observations. Sec-
ond, we improve our understanding, and third, computer modeling. 
It is like a three-legged stool—observations, theory, and modeling— 
that  together  provide  the  foundation  for  our  understanding  of  the 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00007 Fmt 6633 Sfmt 6633 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

4 

way  the  climate  system  has  changed  in  the  past  and  how  it  may 
change in the future. 

NOAA  proudly  notes  that  the  world’s  first  global  climate  model 
was  created  by  our  scientists  at  the  Geophysical  Fluid  Dynamics 
Laboratory, or GFDL. This climate model has been identified in the 
popular  literature  as  one  of  the  milestones  of  scientific  computing, 
along  with  advances  like  the  invention  of  the  hand-held  calculator 
and the Internet. 

A  climate  model  really  allows  us  to  create  a  virtual  earth  in  the 
computer. They divide the Earth into three-dimensional boxes, mil-
lions  of  boxes,  that  cover  the  entire  Earth—called  grid  cells.  At 
each  of  these  grid  cells,  many  calculations  are  performed  over  and 
over  in  order  to  simulate  the  processes  that  are  important  to  cli-
mate.  The  size  of  the  grid  cells  determines  the  resolution  of  the 
model.  The  smaller  boxes  give  scientists  information  that  is  more 
refined. 

On this poster, what we see is the current resolution and genera-
tion  of  model,  and  we  are  looking  at  precipitation.  So  it  just  kind 
of  shows  a  big  general  area  of  precipitation.  In  reality,  we  know 
that  in  the  western  United  States  the  mountains  all  get  a  lot  of 
precipitation  and  the  valleys  do  not  get  so  much.  And  of  course, 
there  is  a  great  deal  along  the  coast.  In  our  new  model  resolution 
that  we  would  like  to  run—we  see  that  detail  in  the  precipitation. 
So  this  is  what  we  are  after,  is  getting  the  regional  detail  correct. 
Computer  models  of  the  Earth’s  climate  have  been  central  to 
NOAA’s  pursuit  of  its  goal  to  understand  climate  variability  and 
change and to enhance society’s ability to plan and respond. These 
models have done so well that they have become central to our in-
tegrated  assessments,  such  as  the  2007  Intergovernmental  Panel 
on  Climate  Change  that  is  used  to  inform  industrial  and  govern-
ment climate and energy analysis. We know that these models are 
important.  They  helped  us  understand  that  the  Dust  Bowl  of  the 
1930s  was  due  to  ocean  temperatures.  They  helped  us  to  under-
stand the El Nin˜ o/La Nin˜ a cycle and we are currently learning how 
the Atlantic Ocean circulation works. It is crucial to our science. 

There is an increasing need for the types of information that cli-
mate models provide. We have land managers in the western states 
that  are  dealing  with  prolonged  periods  of  drought  and  requesting 
long-term  regional  temperature  and  precipitation  data.  The  thing 
that we are after is getting that local scale so that we can help our 
decisionmakers in transportation, energy availability and for emer-
gency preparedness. This is not just government. This is the public 
and industry also. 

However, today’s models are limited in providing the level of cli-
mate information by two things. First, there are significant gaps in 
our  understanding  of  how  the  climate  system  works,  and  second, 
we  are  limited  by  computing.  We  do  not  have  the  computing  that 
we need to do some of these very regional kinds of things. The best 
of  today’s  climate  models  really  give  us  information  on  large  geo-
graphic  scale  such  as  continental  scale.  These  limitations  can  be 
addressed to a significant extent by increased access to large super-
computers. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00008 Fmt 6633 Sfmt 6633 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

5 

Our  vision  of  a  greatly  improved  climate  prediction  during  the 
next 5 to 10 years would require approximately 100 times as much 
computing power over what is currently available. 

Climate  models  are  crucial  to  providing  reliable  information  on 
climate variability and change. More accurate projections of future 
climate  will  contribute  to  improved  preparation  at  the  Federal, 
State, and local levels and by the public and by industry. 

I look forward to working with the Committee. 
[The prepared statement of Dr. MacDonald follows:] 

PREPARED STATEMENT OF DR. ALEXANDER (SANDY) MACDONALD, DEPUTY ASSISTANT 
ADMINISTRATOR FOR LABORATORIES AND COOPERATIVE INSTITUTES,  OFFICE OF 
OCEANIC AND ATMOSPHERIC RESEARCH,  NOAA,  U.S.  DEPARTMENT OF COMMERCE 

Introduction 

Good  afternoon,  Mr.  Chairman  and  Members  of  the  Committee.  I  am  Alexander 
MacDonald, Deputy Assistant Administrator for Laboratories and Cooperative Insti-
tutes  in  the  Office  of  Oceanic  and  Atmospheric  Research  at  the  National  Oceanic 
and  Atmospheric  Administration  (NOAA)  in  the  Department  of  Commerce.  Thank 
you  for  inviting  me  to  discuss  climate  modeling  and  NOAA’s  key  role  in  improving 
the understanding and prediction of global climate and how it is changing. 

NOAA’s mission is to understand and predict changes in the Earth’s environment 
and  conserve  and  manage  coastal  and  marine  resources  to  meet  our  Nation’s  eco-
nomic, social, and environmental needs. In support of the mission, NOAA research-
ers develop and use mathematical models and computer simulations to both improve 
our  understanding  and  prediction  of  natural  climate  variability,  as  well  as  to  iden-
tify and predict climate change. Climate models help create an informed society that 
uses  a  comprehensive  understanding  of  the  role  of  the  oceans,  coasts,  and  atmos-
phere  in  the  global  ecosystem  to  make  the  best  social  and  economic  decisions.  The 
ongoing  pursuit  of  these  objectives—of  increasing  our  knowledge  of  the  complex 
global  climate  system  and  communicating  the  relevant  information  to  stake-
holders—is summarized in NOAA’s climate goal ‘‘to understand and describe climate 
variability and change so as to enhance society’s ability to plan and respond.’’ 

Today,  I  will  be  discussing  the  societal  demands  for  climate  change  information, 
how  climate  models  are  used  to  meet  these  demands,  and  how  the  Nation  benefits 
by improving climate models. 
Societal Demands for Climate Change Information 

Climate  variability  and  change  can  have  a  profound  influence  on  society.  Recent 
evidence  of  global  climate  change  includes  multi-year  droughts,  warmer  global  sur-
face  temperatures,  accelerating  sea  level  rise,  decreasing  Arctic  sea  ice,  retreating 
glaciers, the acidification of our oceans, and shifts in ecosystems. 

Federal,  regional,  state,  and  local  decision-makers  need  credible  climate  informa-
tion  at  increasingly  finer  geographic  scales  to  adapt  to  and  mitigate  climate  varia-
bility and change on time scales from seasons to centuries. Land managers in west-
ern states dealing with drought have requested long-term regional temperature and 
precipitation  data,  along  with  easily  accessible  and  understandable  tools  for  deci-
sion-support.  Resource  managers  from  numerous  Federal  agencies  have  requested 
site-specific  information  to  help  plan  for  and  manage  the  effects  of  climate  change. 
Regions  and  municipalities  have  requested  local  information  about  climate  change 
to improve long-term decision-making on transportation, energy availability, and for 
emergency preparedness. 

A  broad  scope  of  industries  face  operational  challenges  due  to  climate  variability 
and  change,  including:  utilities;  integrated  oil  and  gas;  mining  and  metals;  insur-
ance;  pharmaceuticals;  building  and  construction;  and  real  estate.  Our  under-
standing of how climate change impacts U.S. fisheries and the health of the world’s 
ocean  ecosystems  will  aid  in  effective  long-term  fleet  planning  and  enhance  the  se-
curity of the Nation’s food supply. 

More  accurate  predictions  of  future  climate  will  contribute  to  improved  prepara-
tion for and response to phenomena such as drought, hurricane activity, coastal in-
undation associated with storms and sea level rise, heat waves, poor air quality, and 
forest fires. The Nation’s scientific community can provide this key information with 
comprehensive, state-of-the-art climate models (with related computational and data 
storage  capabilities),  that  continue  to  advance  the  understanding  of  climate  change 
and its potential consequences at local to global scales. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00009 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

Using Climate Models to Meet Societal Demands 
Climate Modeling to Inform Society 

6 

Many  advancements  in  the  scientific  community’s  knowledge  and  understanding 
of  the  way  our  planet’s  climate  system  works  come  about  via  a  synthesis  of  im-
proved observations, advancements in theory, and computer modeling. Like a sturdy 
three-legged  stool,  observations,  theory,  and  modeling  together  provide  the  founda-
tion  for  our  understanding  of  the  way  the  climate  system  has  changed  in  the  past, 
and how it may change in the future. 

Why are climate models so important for providing reliable information on climate 
change? Science generally proceeds from observations to theory, then to experiments 
to verify the theory’s predictions against the observations, and finally further refine-
ment  or  even  refutation  of  the  theory.  In  order  to  perform  experiments,  we  need  to 
replicate the system being studied. This poses a problem for the study of the Earth’s 
climate,  for  there  is  only  one  Earth!  The  use  of  a  computer  model  of  the  Earth— 
a ‘‘virtual Earth’’—allows us to perform ‘‘climate experiments.’’ Other fields in which 
it  is  expensive  or  dangerous  to  perform  real  experiments  make  similar  use  of  com-
puter  simulation.  Car  design  is  a  good  example—most  designs  are  tested  for  aero-
dynamic  efficiency  and  crash  testing  on  a  computer,  before  a  design  ever  makes  it 
to the shop floor. The design of nuclear weapons is another excellent example; given 
the  ban  on  tests  of  these  weapons,  the  United  States  is  devoting  significant  re-
sources to develop the ability to model nuclear detonations. 

Climate  science  and  computer  modeling  of  the  Earth’s  climate  have  advanced 
greatly  since  the  world’s  first  coupled  atmosphere-ocean  global  climate  model  was 
created  in  the  late  1960s.  At  NOAA  we  proudly  note  that  the  world’s  first  such  cli-
mate  model  was  created  by  scientists  at  NOAA’s  Geophysical  Fluid  Dynamics  Lab-
oratory  (GFDL).  The  esteemed  journal  Nature  identified  this  first  climate  model  as 
one of the ‘‘Milestones of Scientific Computing’’—along with advances like the inven-
tion of the handheld calculator, the Internet, and CT scanners. 

Over the last four decades, climate models have improved as both scientific brain-
power and high performance computing have been devoted to this work. During that 
time,  climate  modeling  has  gone  from  being  of  interest  primarily  to  a  fairly  small 
segment  of  the  scientific  and  academic  community  to  being  of  great  interest  to  a 
broad section of society—here in the United States and around the world. More than 
fifteen  climate  modeling  centers  now  exist,  including  those  run  by  NOAA  partners 
at  the  National  Science  Foundation’s  National  Center  for  Atmospheric  Research 
(with  additional  support  from  the  Department  of  Energy),  and  the  National  Aero-
nautics  and  Space  Administration.  NOAA  has  remained  at  the  forefront  of  climate 
modeling  through  this  transition.  This  is  evident  in  NOAA/GFDL  having  produced 
not  one,  but  two  of  the  premier  global  climate  models  that  played  an  integral  role 
in  last  year’s  influential  report  issued  by  the  Intergovernmental  Panel  on  Climate 
Change (IPCC), for which the IPCC shared the 2007 Nobel Peace Prize. 

The best of today’s climate models are most reliable on relatively large geographic 
scales  (i.e.,  for  regions  comparable  in  size  to  a  third  of  the  contiguous  48  states,  or 
larger),  with  increasing  uncertainty  associated  with  climate  projections  on  smaller 
scales.  Those  climate  model  results  are  being  used  now  for  an  increasing  range  of 
applications.  Projected  changes  in  surface  temperature  and  precipitation  patterns, 
storm tracks, ocean currents, and Arctic sea ice are only a few of the aspects of cli-
mate  being  examined  intensively  by  experts  in  the  academic,  government,  and  pri-
vate  sector  communities.  The  customer  base  for  high-quality  climate  model  results 
is  rapidly  increasing.  At  NOAA  we  actively  support  these  efforts  by  making  large 
amounts of our climate model output freely available. Consistent with the U.S. Cli-
mate  Change  Science  Program’s  (CCSP)  strategic  plan,  anyone  can  go  to  NOAA 
websites and download data files that document many of our climate model results. 
In this way, the output of NOAA’s climate models becomes input into climate impact 
studies and assessments. 

However,  the  demand  for  scientifically  credible  projections  (based  on  variable 
greenhouse gas scenarios) of future climate change goes beyond what currently can 
be offered. Today’s models are limited in two primary aspects: (a) there remain sig-
nificant gaps in our understanding of how the climate system works, and (b) models 
are  constrained  by  available  computing.  This  latter  limitation  means  that  while 
these  models  are  at  their  best  in  simulating  climate  features  at  scales  of  several 
hundred  miles  and  larger,  there  is  increasing  uncertainty  in  their  simulation  of 
smaller  scale  climatic  features.  In  addition,  some  of  the  processes  operating  in  the 
climate system on small geographic scales are missing, and yet these processes may 
be  important  for  large-scale  climate.  Both  of  these  limitations  can  be  addressed  to 
a significant extent through the use of very large supercomputers. As an additional 
benefit, access to advanced supercomputers makes it easier for NOAA to attract and 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00010 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

7 

retain  the  world’s  best  climate  scientists,  to  run  models  that  resolve  phenomena  at 
the scale of a single state or even city. 
What is a Climate Model? 

Climate  models  divide  the  three-dimensional  global  ocean  and  atmosphere  into 
millions  of  boxes  referred  to  as  grid  cells.  At  each  of  those  grid  cells  many  calcula-
tions  are  performed  over  and  over  again  in  order  to  simulate  the  time  evolution  of 
processes  important  to  the  climate.  The  number  and  size  of  a  climate  model’s  grid 
cells  are  largely  determined  by  the  amount  of  computer  resources  available;  more, 
smaller  boxes  results  in  more  calculations  which  require  more  computing  power. 
Higher geographic resolution (more, smaller boxes) are desirable for climate models 
for  much  the  same  reason  people  prefer  the  picture  quality  of  a  high  definition  TV 
as compared to a grainy YouTube video: higher resolution provides a more detailed 
representation  of  the  features  in  which  we  are  interested,  which  benefits  both  sci-
entific  researchers  and  stakeholders.  As  a  point  of  reference,  in  NOAA’s  recent  cli-
mate models atmospheric grid points were of a size such that one box’s surface area 
covers  about  twice  the  land  surface  area  of  the  Commonwealth  of  Massachusetts. 
That  means  Maine  is  covered  by  two  boxes,  North  Dakota  and  Washington  State 
by  about  4  each,  and  Texas  by  13.  Since  it  takes  several  grid  boxes  to  properly  de-
fine  or  resolve  a  pattern,  we  can  say  that  today’s  global  climate  models  are  limited 
in  their  ability  to  fully  resolve  features  on  spatial  scales  much  less  than  the  size 
of the 48 contiguous states. 

We test our understanding of climate, as expressed in a computer model, by com-
paring how well that model does against observations of past climate. For instance, 
we  might  initialize  our  model  of  the  Earth’s  climate  with  its  known  state  in,  say, 
1750—the  ‘‘preindustrial’’  climate,  then  apply  the  history  of  all  the  known  external 
forces  on  the  climate—solar  variability,  volcanoes,  industrial  emissions,  land  use 
changes—and  see  how  well  we  do  in  predicting  the  known  history  of  the  20th  cen-
tury  climate.  Our  successes  and  failures  help  us  refine  our  theories  and  our  under-
standing.  It  is  possible  to  ‘‘tune’’  a  model  to  perform  well  against  a  given  metric  of 
skill—say the global mean surface temperature but we use a wide range of metrics 
(e.g.,  temperature,  rainfall  patterns,  number  of  storms,  wintertime  snow  cover, 
etc.)—and  the  only  way  to  do  well  against  a  diverse  and  comprehensive  set  of 
metrics is to represent the physical climate system with fidelity. 

Models  of  the  Earth  system  have  many  components  and  feedback  loops.  Today’s 
models  include  interactions  among  many  components,  including  the  ocean,  atmos-
phere, sea ice, vegetation, ecosystems, and reactions between natural and industrial 
chemicals  in  the  atmosphere.  With  increasing  complexity,  new  challenges  appear. 
For  example,  a  key  research  area  in  the  current  generation  of  climate  models  is  to 
capture the effect of aerosols, which include industrial pollutants, soot, dust, and sea 
spray  on  climate.  Aerosols  block  sunlight  directly,  but  they  also  impact  the  forma-
tion  of  clouds,  a  key  player  in  the  climate  system.  Progress  in  this  key  topic  is  de-
layed because our ability to represent such computationally expensive climate proc-
esses in our models has outpaced the available computing resources on which to run 
them. 
Current Modeling Capabilities and Achievements 

Computer  models  of  the  Earth’s  climate  have  been  central  to  NOAA’s  pursuit  of 
its  goal  to  ‘‘understand  climate  variability  and  change  and  enhance  society’s  ability 
to plan and respond.’’ These models have done so well over time that they have now 
become central to the integrated assessments that are used to inform industrial and 
governmental  climate  and  energy  policy.  The  leading  international  assessments 
such  the  IPCC,  and  focused  products  from  the  CCSP,  both  synthesize  results  from 
computer models to answer key questions asked by policymakers. 

At the time of the first IPCC report in 1991, NOAA/GFDL’s model was one of the 
few  models  capable  of  producing  reasonably  realistic  simulations  of  the  Earth’s  cli-
mate.  Since  then,  several  centers  around  the  world  have  developed  climate  models, 
and  the  assessment  reports  are  now  based  on  ‘‘model  intercomparison  projects,’’ 
where  coordinated  computations  are  independently  run  by  different  centers  around 
the  world.  It  is  a  testimony  to  NOAA/GFDL  models’  pre-eminence  in  the  field  that 
in  2007,  at  the  time  of  the  IPCC  Fourth  Assessment  Report,  they  are  still  seen  as 
being at the very apex of climate modeling, on the basis of independent evaluations 
of their performance against a wide range of metrics of skill. 

Specific achievements of NOAA’s current climate models are manifold. NOAA cli-
mate  modeling  has  helped  demonstrate  that  the  U.S.  Dust  Bowl  in  the  1930s  and 
the drought in the African Sahel of the 1980s were both caused in part by changes 
in the temperatures of the oceans. Our current understanding of El Nin˜ o and of how 
El  Nin˜ o  affects  the  U.S.  climate  is  based  in  large  part  on  NOAA  research  with  cli-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00011 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

8 

mate models. NOAA climate modeling first pointed to the importance of the circula-
tion of the Atlantic Ocean as a potential source for abrupt climate change. Further, 
NOAA  models  have  clarified  the  competition  between  warming  due  to  increasing 
concentrations  of  long-lived  greenhouse  gases  and  cooling  due  to  short-lived  atmos-
pheric particles generated by human activity. 

NOAA models have also been major contributors to the most recent Ozone Assess-
ments  conducted  by  the  World  Meteorological  Organization  (WMO),  evaluating  the 
response  of  the  Antarctic  ozone  hole  to  the  reductions  in  the  emissions  of 
chlorofluorocarbons  that  followed  the  Montreal  Protocol  and  projecting  the  future 
evolution of the ozone shield. NOAA has also developed climate models with higher 
geographic  resolution  that  are  currently  being  used  to  develop  climate  change  pro-
jections  over  North  America,  as  part  of  the  North  American  Regional  Climate 
Change and Assessment Program. 

The  computer  models  themselves  represent  an  important  NOAA  product.  NOAA/ 
GFDL’s  Modular  Ocean  Model  (MOM)  is  the  world’s  most  widely  used  numerical 
model for simulating ocean circulation at the global scale and for understanding and 
predicting ocean climate phenomena. Significant recent advances include the ability 
to directly predict sea-level changes as well as improved representations of the com-
plex  features  of  the  ocean’s  heat  and  chemical  distributions.  Over  400  scientists 
around  the  world  are  now  using  MOM  to  perform  oceanographic,  weather,  and  cli-
mate  studies.  It  is  used  for  operational  weather  forecasting  at  NOAA’s  National 
Weather Service. 
Benefits from Improving Climate Models 

NOAA’s  state-of-the-art  climate  models  were  used  extensively  in  the  latest  IPCC 
assessment, the most recent WMO ozone assessment, and the ongoing North Amer-
ican  Regional  Climate  Change  Assessment  Program.  But  despite  recognition  from 
independent  experts  as  being  among  the  highest  quality  climate  models  in  the 
world,  the  models  are  not  able  to  meet  the  growing  suite  of  societal  demands  for 
climate  change  predictions.  Current  models  are  limited  by  some  remaining  gaps  in 
our understanding of how the climate system works, and in computer resources. The 
lack  of  adequate  computer  power  prevents  us  from  making  optimal  use  of  existing 
knowledge  by  extending  our  simulations  to  smaller  geographic  scales  and  including 
a more complete set of climate processes. 

An  example  of  a  gap  in  understanding  that  is  holding  back  progress  in  climate 
modeling  is  our  lack  of  understanding  of  the  Greenland  and  Antarctic  ice  sheets, 
a major source of uncertainty in predicting the future sea level. Recent observations 
have highlighted the potential for rapid changes in the ice sheets and the inadequa-
cies  of  current  theories  of  ice  sheet  dynamics.  Coordinated  progress  will  be  needed 
in ice sheet observations, a buildup of the human capacity in this research field, and 
experimentation incorporating new models of the ice sheets into our climate models. 
Another  key  gap  is  our  inadequate  understanding  of  the  factors  that  control  the 
Earth’s cloud cover and how it might change as the Earth warms. This gap is a key 
source  of  uncertainty  in  predicting  the  magnitude  of  the  warming  resulting  from  a 
given change in atmospheric carbon dioxide. 

Improving  understanding  on  such  central  questions  is  fundamental  to  progress, 
and we are confident that our climate models will improve as they begin to explicitly 
resolve smaller geographic scales. The scales that our models resolve are determined 
by  the  available  computer  resources.  With  currently  available  computer  resources, 
our  models  are  most  reliable  at  simulating  climatic  features  with  geographic  scales 
of  several  hundred  miles  and  larger,  with  increasing  uncertainty  in  the  simulation 
of  smaller  scale  phenomena.  The  following  are  some  of  the  benefits  related  to  the 
inclusion of smaller scale processes in models: 

1. Projections of temperature and precipitation on smaller scales than those cur-
rently  resolved  adequately  by  climate  models  to  aid  decision-makers  and  plan-
ners  at  the  regional  and  local  levels.  For  example,  trends  in  many  local  water 
resources are affected by small-scale topographic features and land-use patterns 
that are not represented in current climate models. 
2.  Many  of  the  greatest  effects  of  climate  change  may  come  about  through 
changes  in  extreme  events,  such  as  hurricanes,  heat  waves,  droughts,  and 
floods.  The  climate  models  used  in  the  recent  IPCC  assessment  do  not  provide 
adequate simulations of hurricanes, for example. Other extreme events, such as 
droughts  and  floods,  are  strongly  influenced  by  small-scale  processes  that  are 
not well resolved in these models. 
3.  It  is  likely  that  small-scale  ocean  currents  and  other  ocean  processes  may 
play  a  crucial  role  in  the  future  behavior  and  stability  of  the  Antarctic  and 
Greenland ice sheet, with large potential influences on sea level rise. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00012 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

9 

4. The response of ecosystems to climate change, including the cycling of carbon 
through  the  system,  is  highly  uncertain  in  current  models.  This  is  strongly  in-
fluenced by limited computational resources, preventing the inclusion of impor-
tant  small-scale  processes,  such  as  intense  ocean  upwelling  near  the  coasts, 
which are crucial to the global cycling of carbon. 
5. Improved predictive capability to support integrated national air quality pol-
icy  and  regional  emission  management  strategies  for  air  quality  and  climate. 
The prediction of climate change impacts on air quality could be better assessed 
by including smaller scale processes into models. 

Pathways to Climate Model Improvements 

The  next  generation  of  climate  models  that  explicitly  include  smaller-scale  proc-
esses  has  been  developed  in  NOAA.  Prototypes  of  these  models  have  been  tested, 
but  computer  resources  in  NOAA  are  inadequate  to  use  these  models  for  the  com-
prehensive simulations of climate change that are necessary to provide stakeholders 
with  robust  predictions  of  climate  change.  We  cite  here  two  examples  of  next  gen-
eration  models  that  have  been  developed  but  are  too  computationally  expensive  to 
run extensively given current resources: 

1.  A  new  climate  model  has  been  developed  that  resolves  important  ocean  fea-
tures  on  scales  as  small  as  20  miles  (Figure  1).  For  comparison,  models  used 
in  the  most  recent  IPCC  assessment  resolve  ocean  features  on  scales  of  200– 
300 miles. The inclusion of the small-scale ocean features may produce large im-
provements in understanding how ocean circulation responds to global warming, 
with  major  climatic  impacts.  This  includes  how  much  carbon  dioxide  the  ocean 
will absorb (or outgas) in the future, the response of marine ecosystems to glob-
al warming, how El Nin˜ o will respond to global warming, and the potential for 
abrupt  climate  change  due  to  changes  in  the  circulation  of  the  Atlantic  Ocean. 

Figure 1. A new climate model has been developed that resolves crucially important ocean fea-
tures on scales as small as 20 miles. Application of this model for comprehensive climate change 
predictions would deliver much more credible predictions of the ocean’s response to global warm-
ing, including the effect on marine ecosystems, carbon uptake, and ocean acidification. 

Application of this model for comprehensive climate change predictions would 
deliver  much  more  credible  predictions  of  the  ocean’s  response  to  global  warm-
ing, including the effect on marine ecosystems, carbon uptake, and ocean acidifi-
cation.  This  would  also  greatly  improve  the  prediction  of  decadal  scale  changes 
in  the  ocean  that  may  strongly  influence  hurricanes  and  droughts,  as  well  as 
predictions  of  Arctic  climate  change  and  sea  ice.  However,  NOAA  does  not  cur-
rently  possess  the  computational  capability  to  use  this  model.  Applying  this 
model  for  the  next  IPCC  assessment  report  would  require  approximately  10 
times  NOAA’s  current  computing  resource,  which  is  comparable  to  the  largest 
machines  in  the  United  States.  NOAA  does  not  now  have  access  to  these  sys-
tems. 
2.  A  global  atmospheric  model  is  being  developed  that  resolves  processes  on  a 
geographic  scale  of  about  10  miles.  A  regional  version  of  this  model  faithfully 
simulates  Atlantic  hurricane  activity  (Figure  2).  The  global  version  will  simu-
late  important  high  impact  climate  phenomena  and  small-scale  variations  of 
rainfall  around  the  world.  Use  of  such  a  model  for  comprehensive  predictions 
of climate change would increase our confidence in the prediction of how hurri-
canes will change in the future. This model would also be a great improvement 
in our ability to predict regional climate change over the United States, includ-
ing  such  features  as  future  changes  in  western  U.S.  snowpack  with  associated 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00013 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
1
O
D
C
A
M
8
0
5

10 

water resource implications (Figure 3). The output from this model would be of 
substantial  value  across  a  wide  suite  of  applications,  from  water  resource  and 
infrastructure development to agricultural planning. 

Figure 2. A regional version of a global model with 10 mile resolution can faithfully simulate 
Atlantic hurricane activity. The global version will simulate important high impact climate phe-
nomena and small-scale variations of rainfall around the world. 

Figure  3.  A  prototype  model  with  a  resolution  of  30  miles  was  used  to  support  the  North 
American  Regional  Climate  Change  Assessment  Program  (NARCCAP)  and  simulates  substan-
tially more of the features in the precipitation field in the western U.S. than do current models. 
A global model with 10 mile resolution is expected to improve the capture of the amount, timing, 
location, and type of precipitation in order to better predict water resource issues arising in the 
western U.S., a key concern that has been identified by NOAA customers. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00014 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

s
p
e

.

3
O
D
C
A
M
8
0
5

.

s
p
e
2
O
D
C
A
M
8
0
5

11 

The  use  of  this  model  in  comprehensive  climate  change  predictions  would 
provide  climate  change  predictions  on  geographic  scales  of  ten  to  twenty  miles. 
However,  it  would  require  approximately  50  times  NOAA’s  current  computing 
capability  to  apply  this  model  to  the  next  IPCC  assessment  report.  Although 
this  level  of  computing  corresponds  to  roughly  half  of  the  Nation’s  entire  re-
search  high  performance  computing  capacity,  a  limited  set  of  climate  integra-
tions  with  this  model  could  be  used  to  advance  our  understanding  of  how  cli-
mate change affects high-impact phenomena. 

These fine resolution oceanic and atmospheric climate model components will ad-
vance  our  understanding  of  and  ability  to  predict  climate.  But  our  ambition  is  to 
combine  them  into  a  fine  resolution  coupled  climate  prediction  system  that  is  com-
mensurate  with  the  requests  of  policymakers  and  stakeholders  at  the  regional  and 
local  levels.  In  the  next  5–10  years,  NOAA  will  work  toward  advancing  the  fidelity 
and  utility  of  our  climate  models  and  combining  the  advantages  of  finer  resolution 
in  both  the  oceans  and  the  atmosphere  while  fully  capturing  their  complex  inter-
actions.  Fulfilling  such  a  vision  would  require  approximately  100  times  as  much 
computing power as is currently available. 
Conclusion 

We  now  have  a  deeper  understanding  of  the  climate  system  and  the  delivery  of 
climate  information  to  the  Nation  as  a  direct  result  of  NOAA  scientists  and  their 
collaborators  using  high  performance  computing  for  numerical  simulation.  Climate 
models  have  demonstrably  improved  our  ability  to  simulate  the  Earth’s  climate. 
However, the demand for scientifically credible projections of future climate change 
goes beyond what currently can be offered. Scientific advancements and the genera-
tion  of  new  climate  information  products  that  arise  from  better  climate  models  are 
intimately  tied  to  the  state-of-the-art  computers  that  are  devoted  to  running  them. 
NOAA is poised to run advanced climate models that resolve regional scale features 
in  the  atmosphere  and  ocean,  incorporate  the  effects  of  chemistry  and  aerosols  on 
climate,  and  provide  long  lead-time  predictions  of  high-impact  climate  phenomena 
such as drought and hurricane activity. 

Thank you again for inviting me to discuss climate modeling and NOAA’s key role 
in  improving  the  understanding  and  prediction  of  global  climate.  Robust  climate 
models  help  NOAA  to  provide  reliable  information  on  climate  change.  Many  ad-
vancements  in  the  scientific  community’s  knowledge  and  understanding  of  the  way 
our planet’s climate system works have come about via a synthesis of improved ob-
servations, advancements in theory, and computer modeling. I look forward to work-
ing with the Committee on any further information you may require for your delib-
erations on this topic. 

Senator KERRY. Thank you, Doctor. 
Dr. Hack? 

STATEMENT  OF  JAMES  J.  HACK,  PH.D.,  DIRECTOR,  NATIONAL 
CENTER  FOR  COMPUTATIONAL  SCIENCES,  OAK  RIDGE 
NATIONAL LABORATORY 
Dr.  HACK.  Thank  you,  Senator  Kerry,  and  Vice  Chairman  Ste-
vens,  for  the  opportunity  to  speak  with  you  today  on  ways  to  im-
prove  the  capacity  of  U.S.  climate  modeling.  My  name  is  James  J. 
Hack  and  I  serve  as  Director  of  the  National  Center  for  Computa-
tional Sciences, which is located at Oak Ridge National Laboratory, 
and  provides  the  most  powerful  computing  resources  in  the  world 
for  open  scientific  research.  One  of  the  prominent  NCCS  research 
focus areas is the exploration of the Earth’s climate system and cli-
mate change. 

There  are  many  scientific  and  technical  challenges  related  to 
monitoring,  understanding,  predicting,  and  adapting  to  climate 
change.  Observations  of  the  climate  system  are  the  foundation  for 
our improved understanding of climate change and for building the 
computer  models  that  are  used  to  project  the  evolution  of  the  cli-
mate system. Computational research associated with the modeling 
and  prediction  of  Earth’s  climate  includes  developing  methods  for 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00015 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

12 

simulating  complex  multiphase  flow  over  a  wide  range  of  scales 
with high fidelity, with high efficiency on the most powerful super-
computer  systems  available,  and  in  a  software  environment  that 
needs  to  continually  incorporate  new  knowledge  and  new  theo-
retical concepts into the models. 

State-of-the-art climate models embody our best understanding of 
the many complex processes that are central to the climate system. 
The goal of such modeling efforts is to accurately represent the col-
lective  behavior  of  these  processes  as  an  interactive  system.  The 
models are continually developed, tested, and evaluated against ob-
servations.  They  are  the  best  available  tools  for  exploring  how  the 
climate system works and how it is likely to evolve. 

Despite their imperfections, climate models are remarkably capa-
ble  of  reproducing  the  climate  of  the  past,  which  builds  confidence 
in  their  projections  of  future  climate.  They  are  also  remarkably 
consistent  in  their  projections  of  continued  warming  of  the  climate 
system  for  the  remainder  of  this  century,  which  was  more  com-
pletely  discussed  in  the  Fourth  Assessment  Report  of  the  U.N. 
Intergovernmental  Panel  on  Climate  Change.  The  release  of  the 
IPCC  report  signaled  that  the  detection  and  attribution  of  climate 
change at global scales has essentially been resolved. 

So  the  community  is  now  faced  with  a  new  set  of  urgent  prob-
lems  relating  climate  change  to  human  health,  water  resources, 
food  supplies,  and  changing  risks  to  manage  the  natural  eco-
systems.  Central  to  these  problems  is  the  demand  for  much  more 
regional detail about climate change on time scales of resource and 
infrastructure  planning.  In  order  to  address  these  issues,  along 
with  important  questions  on  mitigation  and  adaptation  strategies, 
the  climate  community  needs  to  develop  and  undertake  a  new  co-
ordinated research program that is balanced and integrated among 
observation, theory, and computation. 

Meeting  these  future  challenges  will  require  advances  in  every 
aspect  of  the  models’  theoretical,  observational,  and  computational 
foundation.  Many  of  society’s  questions  will  require  the  develop-
ment  of  a  new  generation  of  more  comprehensive  climate  models, 
frequently  referred  to  as  Earth  System  Models,  that  predict  the 
coupled chemical, biogeochemical, and physical evolution of the cli-
mate system. Addressing the science issues will require new obser-
vations  and  new  methods  of  analysis,  new  theoretical  under-
standing, and new features and models of the earth system that in-
clude  the  interactions  between  human  and  natural  systems.  These 
models  will  play  an  important  role  in  synthesizing  a  broad  range 
of  observations  and  projecting  the  future  responses  of  human  soci-
ety and the natural world to the evolving climate regimes. 

The models will also need to be exercised at unprecedented high 
resolution.  The  needed  increases  in  complexity  and  resolution  will 
require  transformational  changes  in  computational  capability.  A 
flexible  leadership-class  computational  science  infrastructure  will 
continue to be essential to making these advances possible. 

So  in  conclusion,  there  is  no  single  pacing  item  to  the  advance-
ment  of  climate  change  science,  but  a  collection  of  interrelated 
science and technology challenges. Many of the issues discussed in 
this testimony speak to the need for a balanced investment in com-
putational infrastructure, climate science and modeling, climate ob-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00016 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

13 

servations,  computer  science,  and  applied  mathematics.  In  the 
short and long term, computational capability remains a significant 
bottleneck and should remain a high priority investment. 

But  as  the  science  and  complexity  of  climate  simulation  con-
tinues  to  grow,  so  will  new  technical  and  scientific  challenges. 
Proactive  investments  in  climate  modeling  science,  software,  algo-
rithms, data management, and other pacing items will ensure that 
scientific  progress  can  keep  pace  with  the  rapidly  evolving  com-
putational  environment.  Strategic  programmatic  management  of 
such  a  broad  multidisciplinary  activity  will  also  likely  prove  to  be 
the  most  effective  way  to  ensure  that  any  new  investments  have 
the desired impact on accelerating progress. 

This  is  an  exciting  opportunity  for  the  Nation  to  lead  the  world 
in developing a better understanding of the consequences of climate 
change.  Thank  you  again  for  the  opportunity  to  address  the  Com-
mittee, and I look forward to answering any questions. 

[The prepared statement of Dr. Hack follows:] 

PREPARED STATEMENT OF JAMES J. HACK, PH.D., DIRECTOR, NATIONAL CENTER FOR 

COMPUTATIONAL SCIENCES, OAK RIDGE NATIONAL LABORATORY 

I thank Chairman Inouye, Vice Chairman Stevens, and the other Members of the 
Committee  for  the  opportunity  to  speak  with  you  today  on  ways  to  improve  the  ca-
pacity  of  U.S.  climate  modeling  for  decision-makers  and  other  end-users.  My  testi-
mony  draws  on  over  two  decades  of  developing  global  models  of  the  climate  system 
at  the  National  Center  for  Atmospheric  Research.  My  name  is  James  J.  Hack  and 
I  currently  serve  as  director  of  the  National  Center  for  Computational  Sciences 
(NCCS)  at  the  Oak  Ridge  National  Laboratory  (ORNL).  The  ORNL  NCCS  provides 
the  most  powerful  computing  resources  in  the  world  for  open  scientific  research.  It 
is  one  of  the  world’s  premier  computational  science  research  environments  sup-
porting advances in our understanding of the physical world and using that knowl-
edge  to  address  our  most  pressing  national  and  international  concerns.  My  role  as 
NCCS  Director  provides  a  unique  perspective  on  how  the  application  of  leadership- 
class computing technology in a computational science partnership with scientific in-
vestigators  can  radically  accelerate  basic  progress  for  a  variety  of  extremely  de-
manding scientific domains. Examples of NCCS research focus areas are the simula-
tion  of  complex  biomolecular  systems  with  applications  to  pharmaceuticals  as  well 
as  more  efficient  biofuel  generation,  simulations  that  investigate  the  fundamental 
properties of materials, such as high temperature superconductors, and simulations 
exploring  the  processes  that  maintain  and  regulate  Earth’s  global  climate  system. 
There  are  many  scientific  and  technical  challenges  related  to  monitoring,  under-
standing, predicting and adapting to climate change, especially on local and regional 
scales.  Observations  of  the  entire  Earth,  for  instance,  are  the  foundation  for  im-
proved  understanding  of  climate  change  and  for  computer  models  that  accurately 
predict weather and climate. A newly emerging issue is the development of optimal 
methods for assimilating this broad range of physical, chemical, and biogeochemical 
observations  into  models  of  the  Earth  system  in  order  to  more  completely  describe 
the  current  state  of  the  system.  This  is  but  one  example  of  how  the  synthesis  of 
models  and  observations  is  critical  both  for  understanding  the  present  climate  and 
for  simulating  its  evolution  over  the  next  several  decades.  Computational  research 
associated  with  the  modeling  and  prediction  of  Earth’s  climate  system  includes  de-
veloping  methods  for  simulating  complex  multiphase  fluid  motions  over  a  wide 
range of scales with high fidelity and with high computational efficiency, as well as 
by the need to continually incorporate new theoretical and observational knowledge 
into  global  models.  The  rapid  evolution  of  computer  architectures  creates  its  own 
challenge  to  fielding  stable  computational  environments  that  support  Earth  system 
science. 

State-of-the-art climate models, such as those developed by NSF, NOAA, DOE Of-
fice of Science, and NASA programs embody our best understanding of the physical 
and  biogeochemical  processes  that  are  central  to  the  climate  system.  The  goal  of 
such  modeling  efforts  is  to  accurately  represent  the  collective  behavior  of  these  cli-
mate  processes  as  an  interactive  system.  These  models  are  continually  developed, 
tested,  and  evaluated  against  observations.  Although  they  are  the  best  available 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00017 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

14 

tools for exploring how the climate system works, they are not perfect. Uncertainties 
arise  from  shortcomings  in  our  scientific  understanding  of  the  climate  system,  and 
in identifying the best mathematical approaches for representing those processes we 
do understand in numerical models. 

Despite these imperfections, climate models are still able to reproduce the climate 
of the past, which gives considerable confidence in their ability to simulate changes 
in future climate. For instance, climate modelers are able to test the role of various 
forcings  in  producing  observed  changes  in  climate  over  the  past  century.  Such  sim-
ulations  have  now  reliably  shown  that  global  surface  warming  of  recent  decades  is 
a  response  to  the  increased  concentrations  of  greenhouse  gases  in  the  atmosphere. 
They are also remarkably consistent in their projections of continued warming of the 
climate system for the remainder of this century, as discussed in the Fourth Assess-
ment  Report  (AR4)  of  the  United  Nations  Intergovernmental  Panel  on  Climate 
Change  (IPCC).  The  release  of  the  IPCC  AR4  report,  along  with  release  of  a  series 
of  Climate  Change  Science  Program  (CCSP)  reports,  signal  that  the  detection  and 
attribution  of  climate  change  at  global  scales  has  essentially  been  resolved.  The 
global  community  is  now  faced  with  a  new  set  of  urgent  problems  relating  climate 
change  to  human  health,  water  resources,  food  supplies,  changing  risks  of  forests 
to fires and insect disease, and threats to managed and natural ecosystems. Central 
to  these  problems  is  the  demand  for  much  more  regional  detail  on  climate  change 
on the time scales of resource and infrastructure planning. In order to address these 
issues,  the  community  needs  to  develop  and  undertake  a  coordinated  research  pro-
gram balanced and integrated among observation, theory and computation. Meeting 
future challenges in climate change science will require qualitatively different levels 
of  scientific  understanding,  modeling  capabilities,  and  computational  infrastructure 
than  are  currently  available  to  the  climate  science  community.  Many  of  society’s 
questions  will  require  the  development  of  a  new  generation  of  more  comprehensive 
climate models, frequently referred to as Earth System Models (ESMs) that predict 
the  coupled  chemical,  biogeochemical,  and  physical  evolution  of  the  climate  system. 
These  models  will  also  need  to  be  exercised  at  unprecedented  high  resolution.  The 
needed increases in complexity and resolution will require transformational changes 
in computational capability. 

Over the last 30 years, modeling capabilities have advanced considerably in their 
treatment of complexity, and the ability to treat ever finer scales of motion. Modern 
atmospheric  models  represent  the  observed  equator  to  pole  energy  transport  much 
more  realistically  than  did  earlier  model  generations.  They  also  do  a  much  better 
job  of  representing  many  detailed  features  of  the  observed  mean  climate  state. 
These  improvements  have  meant  that  global  climate  models  are  now  routinely  run 
with  fully-interacting  atmosphere,  ocean,  land  surface,  and  sea  ice  components. 
These  more  realistic  and  complex  models  can  now  not  only  simulate  observed 
changes  over  the  past  century  in  global  mean  climate,  but  also  climate  variability 
and  change  on  continental  scales.  This  includes  the  attribution  of  many  of  the  ob-
served large-scale changes in indicators of climate extremes consistent with a warm-
ing  climate,  such  as  the  annual  number  of  frost  days,  warm  and  cold  days,  and 
warm and cold nights. Models that contributed simulation results to the IPCC AR4 
also  generally  agree  that  regions  like  the  subtropics  will  dry,  including  the  U.S. 
Southwest,  while  polar  latitudes  will  receive  more  precipitation  related  to  large 
changes and shifts in the extratropical storm tracks. 

On  finer  spatial  scales,  however,  state-of-the-art  climate  models  don’t  always 
agree  on  projected  climate  change  impacts,  either  on  decadal  or  longer  time  scales. 
It  is  also  not  clear  that  they  can  accurately  project  changes  in  extreme  events,  or 
can  reliably  simulate  changes  in  low-frequency  climate  variability  or  the  likelihood 
of  abrupt  change.  Near-term  investments  in  the  climate  science  enterprise  could 
lead  to  a  significant  quantitative  improvement  in  the  scientific  community’s  ability 
to address these difficult but societally relevant questions, leading to improved guid-
ance to policymakers and stakeholders charged with developing strategies for adapt-
ing to climate change. 

One  immediate  scientific  challenge  and  opportunity  is  the  incorporation  of  chem-
ical  and  biogeochemical  processes  in  climate  models.  The  science  surrounding  the 
chemical  and  biogeochemical  coupling  of  climate  has  become  central  to  answering 
climate change questions, particularly those associated with the global carbon cycle. 
Addressing  the  science  issues  will  require  new  observations  and  methods  of  anal-
ysis,  new  theoretical  understanding,  and  new  models  of  the  Earth  system  that  in-
clude the interactions between human and natural systems. These models will play 
pivotal  roles  in  interpreting  the  paleoclimate  records,  in  synthesizing  and  inte-
grating  observational  measurements  to  study  the  current  carbon  cycle,  and  in  pro-
jecting the future responses of human society and the natural world to evolving cli-
mate regimes. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00018 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

15 

Another example of a pressing scientific challenge is the rate of sea level rise and 
the  impact  of  that  rise  on  coastal  communities.  Recent  observations  indicate  ice 
sheets  can  dissipate  on  much  more  rapid  timescales  than  from  melting  alone  due 
to dynamical processes in large outlet glaciers and ice streams within the ice sheet. 
Faster disintegration of the ice sheets will contribute to faster sea level rise and will 
pose a greater risk of abrupt changes in the climate system. Abrupt climate change 
can also result from thresholds and nonlinearities in the response of climate to slow-
er time scale forcing of the climate system. Examples include rapid changes in ocean 
circulation,  large  scale  vegetation  mortality  and  succession,  release  of  methane  fro-
zen  in  ocean  and  permafrost,  and  megadroughts.  The  climate  community  will  need 
to use models to identify thresholds of forcing in the climate system and explore the 
likelihood and impacts of such scenarios. The community’s efforts to advance climate 
modeling  and  its  application  to  science  and  technology  options  for  mitigation  and 
adaptation  will  require  advances  in  essentially  every  aspect  of  the  models’  theo-
retical,  observational,  and  computational  foundation.  Quantifying  uncertainties  in 
predictions  will  require  a  new  level  of  integration  between  modeling  and  observa-
tional  science.  New  mathematical  methods  and  algorithmic  techniques  will  also  be 
required  to  address  the  fundamental  challenges  of  multi-scale  coupling  of  physical, 
dynamical,  chemical  and  biogeochemical  processes.  A  flexible  leadership-class  com-
puting infrastructure has been and will continue to be a key factor in making these 
advances possible. 

As  mentioned  earlier,  today’s  climate  models  are  in  strong  agreement  that  global 
and continental-scale temperatures will continue to rise as a result of human activi-
ties.  However,  it  is  also  important  to  improve  our  understanding  of  the  likely 
changes in regional climate over the next few decades. Climate forecasts on decadal 
time  scales  are  governed  primarily  by  the  history  of  the  ocean  circulation  and  the 
current  atmospheric  forcing.  Therefore,  climate  forecasts  on  these  time  scales  will 
require retrospective analyses of the global oceans to be able to accurately initialize 
the  forecasts.  The  ocean  is  responsible  for  much  of  the  inertia  or  near-term  ‘‘mem-
ory’’ in the climate system. The development of ocean data assimilation techniques, 
largely an applied mathematics and algorithmic challenge, will be necessary to pro-
vide  an  initial  ocean  state  for  decadal  prediction  and  represents  a  pacing  item  for 
seasonal,  inter-annual,  and  decadal  prediction.  While  assimilation  has  been  exten-
sively  developed  and  used  in  the  weather  community,  the  climate  community  will 
need  to  evaluate  which  assimilation  methodology  is  best  suited  for  climate  simula-
tion and the creation of realistic initial states for climate change scenarios. Optimal 
interpolation  and  simple  methods  have  so  far  been  adequate  for  the  ocean  due  to 
sparseness of data, particularly for salinity and for ocean properties at depths below 
1000m.  With  the  influx  of  new  ocean  data  sets,  advanced  techniques  will  need  to 
be  examined.  Recent  progress  in  deploying  large  numbers  of  floats  and  the  launch 
of new satellites that together will measure salinity profiles will greatly improve our 
ability to effectively constrain ocean models with assimilation. For example, assimi-
lation of data from ARGO floats with a fully coupled climate model has shown great 
promise  in  determining  the  state  of  the  climate  system,  although  the  assimilation 
process is extremely computationally demanding. 

Accurate projections of changes in the frequency of climate extremes at relatively 
high geographic and temporal resolution will be essential for the development of ro-
bust adaptation strategies. However, current climate models have been designed pri-
marily  to  predict  patterns  of  change  at  a  coarser  level.  Much  more  research  is  re-
quired  to  understand  how  increasing  model  resolution  and  employing  increasingly 
sophisticated  parameterized  treatments  of  non-resolvable  processes  may  affect  the 
ability  of  models  to  more  accurately  simulate  changes  in  local  extremes.  In  par-
ticular,  the  relationships  between  extreme  statistics  and  synoptic-scale  low-fre-
quency variability are not understood. 

A  better  understanding  of  low-frequency  variability  is  critical  for  the  detection  of 
climate-change  signals.  For  Earth  system  modeling,  it  is  important  to  characterize 
the  natural  modes  of  coupled  variability  in  the  carbon  cycle,  terrestrial  ecosystems, 
and  dynamic  vegetation.  It  is  also  important  to  develop  a  better  understanding  of 
external forcing mechanisms, such as the role of solar variability in the broader con-
text  of  the  Sun-Earth  system.  Current  understanding  of  these  complex  systems  is 
limited  by  the  length  of  the  observational  record.  The  wide  dynamic  range  in  the 
relevant space and time scales further complicates the coupling issues. New mathe-
matical methods designed for multiscale systems hold promise for this class of prob-
lems,  and  these  methods  should  be  explored  for  efficient  implementation  in  next 
generation models. 

As  suggested  earlier,  a  large  number  of  significant  impacts  could  follow  from  ab-
rupt  changes  in  the  climate  system.  These  occur  when  the  gradual  increases  in  cli-
mate  forcing  trigger  an  abrupt  transition  of  the  coupled  system  to  a  new  state.  Po-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00019 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

16 

tential examples of abrupt change include dynamic dissolution of the ice sheets and 
bifurcations  of  the  ocean  circulation  system.  Characterization  of  abrupt  climate 
change  requires  a  new  paradigm  for  climate  change  modeling,  one  in  which  the 
models  are  integrated  over  the  full  range  of  uncertainties  in  forcing  and 
parameterized physics. Exploration of this phase space will require implicit formula-
tions of the coupled system designed for fast equilibration combined with new math-
ematical techniques and a sustained petascale computing capability. 

Multiscale  interactions  also  complicate  treatment  of  the  climate  system.  As  with 
the  broader  issues  of  climate  variability,  process-level  understanding  of  things  like 
the  water  cycle  is  limited  by  the  lack  of  basic  observations.  While  the  absence  of 
these  data  still  represents  a  barrier  to  progress,  near-term  enhancements  in  com-
putational  capacity  would  permit  the  resolution  of  fundamental  phenomena  at  the 
process  level.  Targeted  investments  in  observational  programs  can  provide  much  of 
the  necessary  data  to  validate  high-resolution  process  modeling  studies  of  critical 
topics  like  aerosol-cloud  interactions,  central  to  the  climate  model  sensitivities  that 
lead to discrepancies in projections of future climate on century-long time scales. 

Finally, there are significant software and computational hardware infrastructure 
challenges pacing progress in climate science. Many scientists have found the grow-
ing  requirements  to  support  the  software  on  high  performance  computers  as  a  dis-
traction from the central scientific goals of improving climate models and answering 
fundamental  questions  about  climate  feedbacks  and  variability.  This  drawback  is 
offset by the new scientific opportunities provided by dramatic increases in computa-
tional  power.  This  becomes  an  issue  of  scientific  productivity.  What  is  needed  is  a 
software  framework  that  not  only  scales  from  desktop  to  petascale,  but  also  that 
supports multi-scale model development and process integration. As a closer connec-
tion  with  observational  data  and  process  studies  is  required  to  advance  the  science 
of  regional  climate  prediction,  the  software  must  also  become  more  closely  inte-
grated  and  supported  across  scales.  A  flexible  and  powerful  software  development 
environment  will  increasingly  be  required  to  support  data  assimilation  and  other 
data  intensive  frameworks.  The  limitations  of  existing  software  environments  have 
emerged as key bottlenecks to progress where near-term investment would have im-
portant scientific payoffs. 

There  are  real  opportunities  to  invest  in  climate  change  science  to  improve  the 
utility  of  global  models  for  decisionmakers  and  the  broader  end-user  community. 
High  impact  opportunities  for  investment  include  computational  facilities,  theo-
retical  efforts  associated  with  model  development,  targeted  observational  programs 
and  the  development  of  novel  computational  algorithms.  Investments  in  modeling 
will  accelerate  progress  on  improving  the  predictive  skill  of  global  climate  models. 
The climate community needs to develop a new generation of Earth System Models 
based  upon  new  and  expansive  requirements  including  the  ability  to  more  accu-
rately  reproduce  major  modes  of  natural  variability,  incorporating  functionality  for 
decadal-scale ensemble forecasts at very high spatial resolution, the flexibility to in-
corporate  new  data  on  the  physical,  chemical,  and  ecological  climate  system  in  the 
form  of  process  representation  (thereby  increasing  the  fidelity  of  climate  simula-
tions),  stronger  connectivity  with  user  communities  for  exploring  adaptation  and 
mitigation  strategies,  and  the  capability  for  two-way  interactions  among  emissions, 
impacts, adaptation, and mitigation. 

Modeling over a large range of time scales to fully evaluate the couplings between 
biogeochemical  cycles,  chemistry,  and  ecology  will  present  a  significant  computa-
tional  challenge.  The  growth  requirement  of  characteristic  applications  of  climate 
change  prediction  models  already  more  than  doubles  every  year.  High-resolution 
ocean circulation studies and cloud system resolving atmospheric simulations are al-
ready  pushing  the  limits  of  petaFLOP  systems  that  utilize  many  tens  of  thousands 
of  processors.  As  regional  climate  prediction  on  decadal  to  century  time  scales  be-
comes  more  important,  the  required  computational  power  will  approach  the 
exaFLOP scale (one quintillion floating point operations per second) that will utilize 
100K–1M processors. This will require a continued focus on fielding state-of-the-art 
leadership  class  computing  facilities  so  that  computational  capability  does  not  be-
come  a  more  critical  pacing  factor.  Ancillary  investments  in  software,  networking, 
data storage, collaborative tool, and visualization technologies are necessary for bal-
ance. For example, climate science is both distributed and collaborative. As interest 
in  climate  science  continues  to  grow  and  its  scope  broadens  to  encompass  issues  of 
ecosystem  and  economic  impacts,  and  the  evaluation  of  mitigation  and  adaptation 
strategies, the number of participants will also increase. The overall productivity of 
researchers  and  the  quality  of  the  research  output  can  likely  be  improved  signifi-
cantly by the use of advanced collaboration technologies that distribute applications 
and  data  across  the  network.  It  is  easy  to  project  that  climate  research  demands 
on networks will grow yet further as data volumes increase. With a growing number 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00020 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

17 

of participants in the climate science enterprise, and a growing diversity and volume 
of climate data, the need for new data and network resource management strategies 
and technologies will emerge. Modern visualization capabilities can also play an im-
portant  role  in  the  discovery  of  new  scientific  results  and  in  the  communication  of 
the  science  to  a  broader  community  of  stakeholders.  For  an  area  like  climate  mod-
eling  this  is  particularly  important  because  of  the  societal  relevance  of  our  results 
to policymakers and those concerned about the consequences of climate change. 

New  observational  programs  and  data  assimilation  systems  represent  opportuni-
ties to improve our understanding of a variety of physical, chemical, biogeochemical, 
and ecological processes, reducing key uncertainties in modeling assumptions. Mete-
orological  and  oceanic  analyses  have  become  important  tools  for  studying  the  mean 
state and variability of the current physical climate. These analyses are constructed 
using  a  model  that  is  adjusted  by  incorporating  observations  during  its  numerical 
integration.  These  analyses  have  proved  particularly  useful  for  understanding  the 
relationship  between  observations  and  the  underlying  dynamics  of  the  climate  sys-
tem. It would be especially valuable to have a comparable analysis of biogeochemical 
and  chemical  cycles  that  could  relate  local  and  global  biogeochemical  processes  to 
more  completely  describe  the  state  of  the  global  system.  However,  there  are  no  ex-
isting analyses that encompass the physical, chemical, and biogeochemical processes 
in the climate system. Development of these analyses will require significant invest-
ment  in  assimilation  systems  for  chemical  and  biogeochemical  observations  from  in 
situ and satellite platforms. Much more advanced models will be required to under-
stand the fidelity of the analysis system, which will further push the sophistication 
of global modeling activities. 

Investments  in  computational  algorithms  will  increase  scientific  productivity 
using  leadership-class  computers  for  climate  change  simulation  studies  and  im-
proved  simulation  accuracy.  There  is  a  broad  class  of  mathematical  and  numerical 
algorithms that are ready to be explored for application to the climate problem. For 
example,  there  are  strong  arguments  for  exploiting  higher-resolution  variable 
gridding configurations for the atmospheric component of a climate model. The com-
putational demands of uniform ultra-high resolution configuration of a global atmos-
pheric  model  would  outstrip  existing  computational  capability.  An  intermediate 
practical  approach  to  dealing  with  resolution  issues  is  to  use  a  multi-resolution  ap-
proach, such as nested refinement. These approaches will allow scientists to improve 
understanding of the multi-scale interactions in the climate system, to identify those 
of  greatest  importance,  and  to  document  their  effects  on  climate.  Ultimately,  such 
research  will  help  determine  the  best  methods  of  including  these  multi-scale  inter-
actions in climate models, and it will help differentiate between those processes that 
can be better or newly parameterized versus those that cannot. Such techniques are 
already being explored by several research groups including the National Center for 
Atmospheric Research. With a nested or adaptive resolution approach the computa-
tional  capability  required  could  be  reduced  by  an  order  of  magnitude  or  more,  and 
could make the goal of computing with such ultra-high resolution models more fea-
sible.  A  final  example  of  algorithm  opportunities  is  the  need  to  better  characterize 
the  uncertainty  in  simulation  results.  Ensembles  and  basic  statistics  are  currently 
used to assess uncertainty due to natural internal variability intrinsic to the climate 
system. More formal methods for verification, validation and uncertainty quantifica-
tion are needed from the computer science, mathematics and statistical science com-
munities.  A  particular  challenge  is  the  sparse  nature  of  observational  data  nec-
essary to validate models. 

The Nation’s climate modeling enterprise is likely to be increasingly driven by the 
need to obtain scientific results for a large and diverse group of users, including gov-
ernment  officials,  in  a  timely  fashion.  In  such  an  environment,  the  development  of 
innovative  models,  algorithms,  and  software  must  be  managed  as  a  project,  as  op-
posed  to  an  open-ended  research  program.  Some  aspects  of  such  an  approach  are 
well-understood,  such  as  the  need  for  planning,  schedule  visibility,  and  milestones. 
A  more  difficult  problem  is  the  potential  dependence  of  success  on  delivering  high- 
risk  products  in  models,  algorithms,  and  software  on  a  particular  schedule.  Many 
of  these  products,  such  as  new  approximation  methods,  or  new  programming  mod-
els, represent non-incremental departures from the current methods used in produc-
tion climate models, but may be necessary to achieve National goals. Risk manage-
ment in such a setting requires careful planning and a close and continuing collabo-
ration  between  the  climate,  facilities,  applied  mathematics,  and  computer  science 
communities.  In  addition  to  the  research  management  needs,  there  will  also  be  a 
need  to  ensure  that  end-users  are  sufficiently  involved  in  the  prioritization  of  re-
search efforts, and that the resources and institutions exist to transfer the large vol-
umes  of  information  into  the  decisionmaking  processes  of  various  private  and  gov-
ernmental users. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00021 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

18 

In conclusion, there is no single pacing item to the advancement of climate change 
science,  but  a  collection  of  interrelated  science  and  technology  challenges.  Many  of 
the  issues  discussed  in  this  testimony  speak  to  the  need  for  a  balanced  investment 
portfolio in computational infrastructure, climate science, computer science, and ap-
plied  mathematics.  In  the  short  and  long  term,  computational  capability  remains  a 
significant  bottleneck  and  should  remain  a  high  priority  investment.  But  as  the 
science  and  complexity  of  climate  simulation  grows,  so  will  new  technical  and  sci-
entific challenges. Immediate proactive investments in climate science, software, al-
gorithms,  data  management,  and  other  pacing  items  are  needed  for  accelerated 
progress  that  can  keep  pace  with  the  rapidly  evolving  computational  environment. 
The  management  of  these  investments  is  also  critical  to  success.  Strategic  manage-
ment of such a broad multidisciplinary activity will likely prove to be the most effec-
tive  way  to  ensure  that  new  investments  have  the  desired  impact  on  accelerating 
progress. 

THE CLIMATE PREDICTION PROJECT 

Global  Climate  Information  for  Regional  Adaptation  and  Decision-Making 

in the 21st Century 

Challenge 

The  world  recognizes  that  the  threat  of  global  climate  change  is  one  of  the  most 
important  problems  facing  humanity.  To  cope  with  the  consequences  of  climate 
change, the peoples, governments, and economies of the world must develop mitiga-
tion and adaptation strategies, which will require investments of trillions of dollars. 
The  development  of  science-based  adaptation  and  mitigation  strategies  will  only  be 
possible through a revolution in regional climate predictions. 
The Summit 

The  World  Modeling  Summit  for  Climate  Prediction  was  organized  to  develop  a 
strategy  to  revolutionize  prediction  of  the  climate  through  the  21st  century  to  help 
address the threat of global climate change. 
Summit Declaration 

1.  Improved  prediction  of  the  changes  in  the  statistics  of  regional  climate,  espe-
cially  of  extreme  events  and  high-impact  weather,  are  required  to  assess  the  im-
pacts of climate change and variations, and to develop adaptive strategies to amelio-
rate their effects on water resources, food security, energy, transport, coastal integ-
rity, environment and health. 

2. Our current inadequacy in the provision of robust estimates of the risk to soci-
ety,  particularly  from  possible  catastrophic  changes  in  regional  climate,  is  strongly 
influenced by limitations in computer power and the size of the scientific workforce. 
3.  Climate  prediction  is  among  the  most  computationally  demanding  problems  in 
science.  It  is  both  necessary  and  possible  to  revolutionize  climate  prediction:  nec-
essary  because  of  the  grand  challenge  posed  by  the  changing  climate,  and  possible 
building  on  the  past  accomplishments  of  prediction  of  weather  and  climate.  How-
ever,  the  scientific  expertise  and  the  computing  capability  is  not  available  in  any 
single nation, and a comprehensive international effort is essential. Investing today 
in  climate  science  will  lead  to  significantly  reduced  costs  of  coping  with  climate 
change tomorrow. 

4.  A  Climate  Prediction  Project  coordinated  by  WCRP,  in  collaboration  with 
WWRP  and  the  IGBP  and  involving  the  national  weather  and  climate  centers 
should be initiated to provide global climate information for regional adaptation and 
decision-making in the 21st century. 

5.  As  a  part  of  the  Climate  Prediction  Project,  and  in  addition  to  enhancing  the 
capacity  of  the  world’s  existing  national  climate  research  centers,  a  World  Climate 
Research  Facility  (WCRF)  for  climate  prediction  should  be  established  that  will  en-
able  the  national  centers  to  accelerate  progress  in  improving  operational  climate 
prediction  at  decadal  to  multi-  decadal  lead  times,  enhancing  understanding  of  the 
climate  system,  building  global  capacity,  developing  a  trained  scientific  workforce, 
and engaging the global user community. The WCRF will argue for sustained, long- 
term, global observations that are needed to initialize, constrain and verify the mod-
els.  An  important  component  of  the  WCRF  will  be  an  archive  of  observations  and 
model  data  with  appropriate  user  interface  and  knowledge-discovery  tools  for  diag-
nostic tests. 

6.  The  central  component  of  the  WCRF  will  be  one  or  more  dedicated  high-end 
computing  facilities  that  will  enable  the  revolution  in  climate  prediction  by  sup-
porting the model resolution and complexity required for the most advanced and re-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00022 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

19 

liable  representations  of  the  climate  system  that  technology  and  our  scientific  un-
derstanding  of  the  problem  can  deliver.  This  computing  capability,  with  systems  at 
least  10,000  times  more  powerful  than  the  currently  available  computers,  is  vital 
for  regional  climate  predictions  to  underpin  mitigation  policies  and  local  and  re-
gional adaptation needs with robust estimates of risk. The computing capability will 
help  advance  our  understanding  of  the  processes  responsible  for  climate  variability 
and  predictability,  and  provide  a  quantum  leap  in  the  exploration  of  the  limits  in 
our  ability  to  reliably  predict  climate  with  a  level  of  detail  and  complexity  that  is 
not  possible  at  the  national  centers.  It  will  also  make  it  possible  to  bring  to  bear 
the latest and most innovative computer technology on the climate change problem, 
and provide a common modeling framework through an international computing lab-
oratory  and  make  it  possible  to  conduct  specialized  experiments  to  advise  decision- 
making in adaptation, mitigation. This project will permit scientists to strive toward 
kilometer-scale  modeling  of  the  global  climate  system,  which  will  particularly  ben-
efit  the  simulation  and  prediction  of  tropical  climate,  helping  many  of  the  world’s 
developing countries that are especially vulnerable to climate change. 

7. The WCRF will make it possible for the first time to deliver climate predictions 
with  a  reliable  estimate  of  their  uncertainty.  To  estimate  the  quality  of  a  climate 
prediction  requires  an  assessment  of  how  accurately  we  know  the  current  phase  of 
natural climate variability, on which anthropogenic climate change is superimposed. 
But also the WCRF will enable the climate research community to assess how model 
uncertainties  limit  the  skill  of  climate  predictions.  All  elements  of  estimating  the 
uncertainty  in  climate  predictions  pose  an  extreme  burden  on  computing  resources 
but also on the availability of observational data. 

8. The methodology of initializing weather and short-term climate prediction mod-
els  with  observations  must  be  seamlessly  extended  to  predictions  of  decadal  vari-
ations  and  climate  change.  The  understanding  and  representation  of  physical  and 
biogeochemical  processes  and  feedbacks  must  be  improved  to  make  reliable  centen-
nial projections. 

9.  It  may  be  possible  that  the  WCRF  will  be  funded  in  different  ways,  e.g., 
through  public-private  partnerships  with  corporate  and  foundation  resources  and 
through governmental treaties and agreements. 

The  Climate  Prediction  Project  has  the  potential  to  help  humanity  cope  with  the 

consequences of climate change. 

The  lasting  legacy  of  the  Project  will  be  to  help  the  citizens  of  the  world  in  the 

21st century. 

Senator KERRY. Thanks very much, Dr. Hack. 
Dr. Reed? 

STATEMENT OF DR. DANIEL A. REED, CHAIR, BOARD OF 
DIRECTORS, COMPUTING RESEARCH ASSOCIATION (CRA) 
Dr.  REED.  Good  afternoon,  Mr.  Chairman,  and  Mr.  Vice  Chair-
man. I am Daniel Reed, Chair of the Board of Directors of the Com-
puting Research Association and a high performance computing re-
searcher. 

Today I would like to make four points regarding the status and 
future of high performance computing for climate change modeling. 
It is clear we now face life and death questions, the potential ef-
fects of human activities and natural processes on our climate and 
their  regional  impacts.  I  believe  high  performance  computing  and 
computational  science  are  among  our  best  options  to  gain  that  un-
derstanding.  HPC  systems  now  bring  detailed  computational  cli-
mate models to life. However, a recent Department of Energy study 
estimated  that  climate  modeling  could  effectively  use  an  exascale 
HPC system effectively. That is a computer 1,000 times faster than 
today’s most powerful systems, and one nearly a billion times fast-
er than today’s PC’s. 

Why are these climate models so complex? First, one must simu-
late many years to validate the models against observational data. 
Second,  to  understand  possible  environmental  changes,  one  must 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00023 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

20 

model  sensitivity  to  many  conditions,  including  carbon  dioxide 
emissions.  Third,  to  understand  the  interplay  of  biogeochemical 
processes with public policies, one must evaluate model ensembles. 
And finally, one must study detailed regional effects such as hurri-
canes  and  storm  surge,  not  just  global  ones.  And  I  would  add  par-
enthetically  that  when  I  was  at  North  Carolina,  I  spent  a  great 
deal  of  time  working  on  precisely  those  issues,  looking  at  the  re-
gional effects of storm surge and hurricanes. 

This leads to my second point, HPC availability for climate stud-
ies.  In  the  1980s,  the  importance  of  computing  to  science  and  the 
dearth of HPC facilities for research stimulated creation of the Na-
tional  Science  Foundation  and  the  Department  of  Energy’s  Office 
of Science Supercomputing Centers. They now provide much of the 
U.S.  scientific  HPC  resources,  including  for  climate  change.  With-
out  question,  our  HPC  infrastructure  is  enormously  greater  than 
20 years ago, but so too are our expectations and our needs. Equal-
ly  tellingly,  most  HPC  resources  are  shared  across  many  scientific 
disciplines,  and  only  a  portion  of  them  support  climate  change 
studies. 

This  brings  me  to  my  third  point,  high  performance  computing 
technology  trends.  Until  the  mid-1980s,  high  performance  com-
puting was defined by custom designed vector processors, those de-
signed by the legendary Seymour Cray. The ubiquitous PC changed 
that, creating a new high performance computing model, one based 
on large clusters of PCs. By analogy, this was a shift from a single 
bulldozer  to  1,000  shovels.  However,  our  20-year  free  ride  of  in-
creasing  microprocessor  performance,  which  is  to  say  bigger  shov-
els,  has  ended,  and  a  second  transition,  multiple  processors  per 
chip,  lots  of  small  shovels,  is  underway.  This  multicore  revolution 
will  be  even  more  disruptive,  profoundly  affecting  the  computing 
industry  and,  more  pointedly,  climate  researchers.  Simply  put,  we 
are  now  suffering  the  delayed  consequences  of  limited  Federal  re-
search investment in this domain. 

Moreover,  the  scientific  data  deluge  from  new  instruments 
threatens to overwhelm our research institutions and the ability of 
climate  researchers  to  integrate  data  with  multidisciplinary  mod-
els. 

This leads to my last point, climate high performance computing 
research  and  development  and  procurement  models.  We  must  ex-
plore new HPC hardware designs that better support scientific and 
national  defense  applications,  recognizing  that  the  design  cost  for 
these  systems  are  rarely  repaid  by  commercial  sales.  Thus,  we 
must  rethink  our  models  for  high  performance  computing  research 
and development and procurement. Simply put, a million rowboats 
is no substitute for an aircraft carrier. 

We also need new programming models that simplify application 
development  for  multicore  processors.  Today  climate  modeling 
teams spend inordinate amounts of time tailoring software to HPC 
systems,  time  better  spent  on  climate  research.  Climate  analysis 
also requires diverse investments, as Dr. Hack mentioned. HPC fa-
cilities must be balanced with investments in software, storage, al-
gorithms, and tools. 

In 2005, I chaired the President’s IT Advisory Committee Report 
on  Computational  Science  and  in  2007  co-chaired  the  President’s 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00024 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

21 

Council  of  Advisors  on  Science  and  Technology,  PCAST,  review  of 
computing  research.  Both  of  those  reports  recommended  an  inter-
agency  strategic  road  map  for  research  computing  and  high  per-
formance computing infrastructure. 

In summary, our challenges are to sustain both the research and 
the  deployment  of  HPC  systems  needed  to  ensure  our  planet’s 
health. 

Thank you very much for your time and attention. I look forward 

to questions. 

[The prepared statement of Dr. Reed follows:] 

PREPARED STATEMENT OF DANIEL A. REED, CHAIR, BOARD OF DIRECTORS, 

COMPUTING RESEARCH ASSOCIATION (CRA) 

Good  afternoon,  Mr.  Chairman  and  Members  of  the  Committee.  Thank  you  for 
granting me this opportunity to comment on current U.S. computational capabilities 
and the research and infrastructure needs to support climate modeling. I am Daniel 
Reed,  Chair  of  the  Board  of  Directors  for  the  Computing  Research  Association 
(CRA).  I  am  a  researcher  in  high-performance  computing;  a  member  of  the  Presi-
dent’s Council of Advisors on Science and Technology (PCAST); the former Director 
of the National Center for Supercomputing Applications (NCSA), one of NSF’s high- 
performance  computing  centers;  and  Director  of  Scalable  and  Multicore  Computing 
Strategy at Microsoft. 

I  would  like  to  make  five  points  today  regarding  the  status  and  future  of  high- 
performance computing (HPC) for climate change modeling, beginning with the rela-
tionship between HPC and climate change models. 
1. High-end Computational Science: Enabling Climate Change Studies 

We  know  the  Earth’s  climate  has  changed  during  the  planet’s  history,  due  to  the 
complex  interplay  of  the  oceans,  land  masses  and  atmosphere,  the  solar  flux  and 
the  biosphere.  Recently,  the  U.S.  Climate  Change  Science  Program  and  the  Inter-
governmental Panel on Climate Change (IPCC) 1 concluded that climate change will 
accelerate  rapidly  during  the  21st  century  unless  there  are  dramatic  reductions  in 
greenhouse  emissions.  We  now  face  true  life  and  death  questions—the  potential  ef-
fects  of  human  activities  and  natural  processes  on  our  planet’s  ecosystem.  I  believe 
HPC  tools  and  technologies  provide  one  of  our  best  options  for  gaining  that  under-
standing. 

In  2005,  I  was  privileged  to  chair  the  computational  science  subcommittee  of  the 
President’s  Information  Technology  Advisory  Committee  (PITAC),  which  examined 
the  competitive  position  of  the  U.S.  in  computing-enabled  science.  In  our  report, 
Computational Science: Ensuring America’s Competitiveness,2 we noted that 

Computational  science  is  now  indispensable  to  the  solution  of  complex  problems 
in  every  sector,  from  traditional  science  and  engineering  domains  to  such  key 
areas  as  national  security,  homeland  security,  and  public  health.  Advances  in 
computing  and  connectivity  make  it  possible  to  develop  computational  models 
and  capture  and  analyze  unprecedented  amounts  of  experimental  and  observa-
tional  data  to  address  problems  previously  deemed  intractable  or  beyond  imagi-
nation. 

Computational science now constitutes the third pillar of the scientific enterprise, 
a  peer  alongside  theory  and  physical  experimentation.  This  is  especially  important 
in a field such as climate change studies, where the models are complex—multidisci-
plinary  and  multivariate—and  one  cannot  conduct  parametric  experiments  at  plan-
etary scale. 

Why  then,  is  HPC  especially  critical  to  climate  change  studies?  First,  one  must 
simulate  hundreds  to  thousands  of  Earth  years  to  validate  models  and  to  assess 
long-term  consequences.  This  is  practical  only  if  one  can  simulate  a  year  of  climate 
in  at  most  a  few  hours  of  elapsed  time.  Each  of  these  simulations  must  be  of  suffi-
cient  fidelity  (i.e.,  temporal  and  spatial  resolution)  to  capture  salient  features. 
Today, for example, most climate models that are run for several hundred to several 

1 R.  Alley  et  al,  Climate  Change  2007:  The  Physical  Science  Basis,  IPCC,  Working  Group  1 

for the Fourth Assessment, WMO. 

2 Computational  Science:  Ensuring  America’s  Competitiveness  President’s  Information  Tech-
nology  Advisory  Committee  (PITAC),  June  2005,  http://www.nitrd.gov/pitac/reports/2005 
0609lcomputational/computational.pdf. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00025 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

22 

thousand  simulated  years  do  not  explicitly  resolve  important  regional  features  like 
hurricanes. These are large-scale, capability computing problems (i.e., ones requiring 
the most powerful computing systems). 

Second, to understand the effects of environmental changes and to validate climate 
models,  one  must  conduct  parameter  studies  (e.g.,  to  assess  sensitivity  to  different 
conditions such as the rate of CO2 emissions or changes in the planet’s albedo). Each 
of  these  studies  involves  hundreds  to  thousands  of  individual  simulations.  This  is 
only  practical  if  each  simulation  in  the  ensemble  takes  a  modest  amount  of  time. 
These  are  large-scale,  capacity  computing  problems  (i.e.,  ones  requiring  ongoing  ac-
cess to multiple, large-scale computing systems). 

Third,  understanding  the  sensitivity  of  physical  and  biogeochemical  processes  to 
social,  behavioral  and  economic  policies  requires  evaluation  of  statistical  ensembles 
and many model variants. These are hypothesis-driven computational scenarios that 
are only possible after the physical and biogeochemical processes are understood, re-
quiring additional capacity and capability computing. 

This  is  a  daunting  problem—developing,  validating  and  evaluating  multidisci-
plinary  climate  models  in  time  to  provide  the  necessary  answers  to  critical  ques-
tions: 

• How many simulation scenarios are necessary (minimally and optimally). 
• What model elements are needed for each scenario? 
• What temporal and spatial resolution, along with physical models, is affordable? 
• What are the errors and uncertainties in model predictions? 
• When  must  research  end  and  production  simulation  begin  to  produce  policy 

guidance? 

Underlying  these  questions  is  the  need  for  powerful  computers  to  model  climate 
change  at  regional  and  fine  scales,  and  to  support  the  sophisticated  and 
computationally  expensive  algorithms  needed  to  represent  the  complexities  of  both 
natural and human effects. We must also manage the tsunami of observational data 
now being captured via a new generation of environmental sensors, integrating high- 
resolution  Earth  system  models  with  assimilated  satellite  and  other  data,  supported 
by large data archives and intelligent data mining and management systems. 

Finally,  we  must  develop  the  multiphysics  algorithms  and  models  needed  to  rep-
resent the complex interactions of biological, geophysical, chemical and human activi-
ties.  New  scientific  and  mathematical  advances  will  also  be  required  to  quantify 
model  uncertainty  for  such  complex  systems.  This  fusion  of  sensor  data  with  com-
plex models is large-scale computational science in its clearest and most compelling 
form. Equally importantly, those HPC systems must be available for researcher use. 
2. High-Performance Computing Resource Availability 

In  the  early  1980s,  HPC  facilities  were  accessible  only  by  a  handful  of  U.S.  re-
searchers.  Most  access  required  both  a  national  security  clearance  and  partnership 
with one of the U.S. weapons laboratories or international travel—for access to com-
puting  research  facilities  outside  the  U.S.  The  rising  importance  of  computing  to 
science  and  the  dearth  of  HPC  facilities  for  open  scientific  research  stimulated  cre-
ation of the National Science Foundation (NSF) supercomputing centers and similar 
facility investments by the Department of Energy’s (DOE) Office of Science. Although 
other  agencies  also  support  HPC  facilities,  NSF  and  DOE  now  provide  the  over-
whelming fraction of the unclassified resources for computational science, including 
climate change. 

This NSF program and its descendents, the Partnerships for Advanced Computa-
tional  Infrastructure  (PACI)  and  the  TeraGrid,  continue  to  support  academic  re-
searchers  via  consulting,  HPC  systems  and  archival  storage.  All  of  the  NSF-sup-
ported  resources,  with  the  exception  of  the  majority  at  the  National  Center  for  At-
mospheric Research (NCAR), are allocated by peer review across all disciplines. The 
computing facilities at NCAR include peer-reviewed resources allocated for weather 
and  climate  research  and  the  Climate  Simulation  laboratory  (CSL)  resources  dedi-
cated  to  climate  change  research.  Historically,  all  NSF  computing  resources  have 
been substantially over-subscribed, with unmet demand from academic researchers. 
Recently,  however,  NSF  has  funded  a  series  of  competitive  hardware  acquisitions 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00026 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

23 

to  help  address  this  shortfall,  with  the  largest  slated  to  sustain  one  petaflop 3 on 
selected applications. 4 

The  DOE  Office  of  Science  also  maintains  a  set  of  unclassified  computing  facili-
ties,  anchored  by  the  National  Energy  Research  Scientific  Computing  Center 
(NERSC),  two  leadership-class  computing  systems  at  Oak  Ridge  and  Argonne  Na-
tional Laboratories, and a smaller facility at the Pacific Northwest Research Labora-
tory.  The  majority  of  DOE’s  NERSC  resources  are  also  allocated  by  peer  review, 
with the requirement that the proposed use be relevant to the DOE Office of Science 
mission.  Finally,  the  DOE  leadership-class  facilities  target  focused  projects  that 
could  benefit  from  access  to  the  largest-scale  facilities  in  the  country,  including  the 
climate  change  modeling  program.  Most  of  these  resources  are  allocated  by  the  IN-
CITE initiative.5 

Our  computational  science  infrastructure  is  enormously  greater  than  twenty  years 
ago.  However,  so  are  our  expectations  and  needs—science  and  computing  are  now 
synonymous.  Equally  tellingly,  because  almost  all  of  our  NSF  and  DOE  HPC  re-
sources are shared across disciplines, only a modest fraction of these systems is dedi-
cated  to  climate  change  studies.  Rather,  researchers  rely  on  a  combination  of  pro-
posal  peer  review  and  programmatic  resource  allocation  to  conduct  climate  change 
studies on a diverse array of HPC systems. 

At  present,  there  is  no  truly  large  scale  U.S.  climate  change  computing  research 
facility,  architected,  configured  and  dedicated  to  multidisciplinary  climate  change 
studies  that  can  deliver  timely  and  accurate  predictions.  A  recent  DOE  study  esti-
mated that climate and environmental modeling could use an exascale system effec-
tively  (i.e.,  one  thousand  times  faster  than  any  extant  computer  system).  Simply 
put, change modeling is a deep and challenging scientific problem that requires com-
puting infrastructure at the largest scale. 
3. Computing Evolution: Lessons and Challenges 

In the late 1970s and the 1980s, HPC was defined by vector processors, as exem-
plified  by  the  eponymously  named  systems  designed  by  the  legendary  Seymour 
Cray. These systems combined high-speed, custom processor design with fast memo-
ries  and  innovative  packaging.  Researchers  and  software  developers  were  able  to 
tune  selected  portions  of  their  codes  to  the  vector  hardware,  achieving  unprece-
dented performance with modest effort. 

With  the  birth  of  the  PC,  a  new  approach  to  HPC  began  to  emerge  in  the  1980s. 
The increasing performance and low cost of commodity microprocessors—the ‘‘Attack 
of the Killer Micros’’—transformed HPC. This new model of massive parallelism par-
titions computations across large numbers of processors. Via this approach, one can 
increase  peak  hardware  performance  to  levels  limited  only  by  economics  and  reli-
ability. However, achieving high performance on complex applications is more prob-
lematic and challenging, particularly for multidisciplinary applications. The climate 
change  community  expressed  great  concern  about  this  disruptive  technology  transi-
tion during the 1990s, with concomitant political controversy. 

Recognizing  this  technological  shift,  the  associated  challenges  and  the  opportuni-
ties, the Defense Advanced Research Projects Agency (DARPA) launched an aggres-
sive  research  and  development  program  that  engaged  academia,  industry  and  na-
tional laboratories. Other Federal agencies, notably the National Science Foundation 
(NSF),  the  Department  of  Energy’s  (DOE)  Office  of  Science  and  the  National  Aero-
nautics  and  Space  Administration  (NASA),  joined  in  the  High-Performance  Com-
puting and Communications (HPCC) program.6 

In  the  1990s,  research  flourished  in  computer  architecture,  system  software,  pro-
gramming  models,  algorithms  and  applications.  Computer  vendors  launched  new 
initiatives, and parallel computing startup companies were born. Planning began for 
petascale  systems,  based  on  integrated  hardware,  architecture,  software  and  algo-
rithms  research.  After  a  promising  start,  much  of  the  initiative  faded  and  attention 
shifted elsewhere. The most notable exception was DOE’s National Nuclear Security 
Administration  (NNSA).  Needing  to  certify  the  weapons  stockpile  without  testing, 
NNSA embraced HPC to verify and validate weapon safety and readiness. The com-

3 One  teraflop  is  1012 floating  point  operations/second;  one  petaflop  is  one  thousand  teraflops, 
or  1015 floating  point  operations/second;  one  exaflop  is  one  thousand  petaflops,  or  1018 floating 
point operations/second. 

4 See  the  NSF  Office  of  Cyberinfrastructure,  http://www.nsf.gov/dir/index.jsp?org=OCI  for 

details on the NSF acquisition program. 

5 Department  of  Energy  Innovative  and  Novel  Computational  Impact  on  Theory  and  Experi-

ment (INCITE) initiative, http://hpc.science.doe.gov/ 

6 The  High-Performance  Computing  and  Communications  (HPCC)  program  became  the  Net-
working  and  IT  Research  and  Development  (NITRD)  program,  http://www.nitrd.gov/about/ 
aboutlNITRD.html. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00027 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

24 

plex physics drove new algorithm and software development and acquisition of some 
of the world’s most power computing systems, all based on massive parallelism and 
commodity microprocessors. 

While  the  U.S.  computing  industry  largely  abandoned  purpose-built  supercom-
puters  in  favor  of  commodity  designs,  Japanese  vendors,  notably  Hitachi  and 
Fujitsu, continued to develop and evolve vector supercomputers. In 2002, Japan an-
nounced  the  Earth  Simulator—then  the  world’s  fastest  computer.  The  Earth  Simu-
lator was designed specifically for large-scale climate and weather studies and drew 
on many years of vector computing research and development. 

Although the Japanese plan had long been public, it precipitated considerable con-
cern.  The  interagency  High-End  Computing  Revitalization  Task  Force  (HECRTF) 
was  chartered  to  assess  the  competitive  position  of  the  United  States.  I  was  privi-
leged  to  chair  the  2003  HECRTF  community  workshop  and  edited  the  associated 
community  report.7 The  Federal  agencies  produced  a  complementary  report  and  a 
proposed action plan. Several agencies launched new programs, of which the largest 
and  most  visible  were  the  NSF  OCI  petascale  initiative  and  the  DOE  Office  of 
Science’s  Scientific  Discovery  through  Scientific  Computing  (SciDAC) 8 and  INCITE 
programs. 

Today, the majority of the world’s largest HPC systems, dominated by U.S. labora-
tory  and  academic  holdings,  remain  based  on  commodity  building  blocks  and  com-
munity-developed  software.  In  this  high-performance  ‘‘monoculture,’’  vendor  profit 
margins  are  small,  and  competition  for  sales  is  intense,  with  limited  vendor  oppor-
tunity to recover research and development investments in alternative architectures. 
Equally  worrisome,  the  pool  of  academic  researchers  in  HPC  and  computational 
science is small, and research funding is limited. 

Without  doubt,  the  explosive  growth  of  scientific  computing  based  on  clusters  of 
commodity  microprocessors  has  reshaped  the  HPC  market.  The  U.S.  remains  the 
undisputed world leader in this space. Petascale systems are being deployed by NSF 
and  DOE  for  academic  and  laboratory  research,  and  feasibility  assessments  of 
exascale  systems 9 are  underway.  Although  this  democratization  of  HPC  has  had 
many salutatory effects, including broad access to commodity clusters across labora-
tories and universities, it is not without its negatives. 

Not  all  aspects  of  climate  change  models  map  efficiently  to  the  cluster  program-
ming  model  of  loosely  coupled,  message-based  communication.  It  is  also  unclear  if 
we have the resources needed to address the climate change problem at appropriate 
scale  and  in  a  timely  manner,  particularly  given  dramatic  changes  now  underway 
in computing technology. 
4. The Brave New World: Multicore and Massive Data 

Over  the  past  twenty  years,  computational  science  and  HPC  have  exploited  the 
ever-increasing  performance  of  commodity  microprocessors.  Each  new  processor  gen-
eration combined greater transistor density, new architectural techniques and higher 
chip  power  to  deliver  greater  single  processor  performance.  This  tripartite  evolution 
is now over. Although transistor densities on chip will continue to rise, physics and 
power  constraints  make  it  impractical  to  increase  clock  frequencies  further.  Future 
chip performance increases will depend on explicit parallelism and architectural in-
novations.  No  longer  will  current  software  execute  faster  in  the  future  without 
change.  Parallelism  is  now  required,  even  at  the  chip  level,  to  deliver  greater  per-
formance. 

This  multicore  revolution—the  placement  of  multiple,  slower  processors  on  each 
chip—poses major new challenges for the computing industry. It is just as disruptive 
as  the  transition  from  vector  to  parallel  computing  was  fifteen  years  ago.  Today’s 
quad-core  chips  will  soon  be  replaced  by  chips  containing  tens,  then  hundreds  and 
perhaps thousands of cores (processors). The technical challenges are daunting, and 
we  have  no  straightforward  technical  solutions  that  will  hide  this  complexity  from 
software developers.10 This will profoundly affect the software industry and scientific 
researchers. 

7 The documents for the High-End Computing Revitalization Task Force (HECRTF), including 
the  community  workshop  report,  can  be  found  at  http://www.nitrd.gov/subcommittee/hec/ 
hecrtf-outreach. 

8 Department  of  Energy,  Scientific  Discovery  through  Scientific  Computing  (SciDAC),  http:// 

www.scidac.gov/. 

9 Modeling  and  Simulation  at  the  Exascale  for  Energy  and  the  Environment,  Summer  2007, 

http://www.sc.doe.gov/ascr/ProgramDocuments/TownHall.pdf. 

10 This  realization  recently  motivated  Microsoft  and  Intel  to  invest  $20M  in  academic 
multicore  research  at  the  University  of  Illinois  at  Urbana-Champaign  and  the  University  of 
California  at  Berkeley,  http://www.microsoft.com/presspass/press/2008/mar08/03-18UPC 
RCPR.mspx. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00028 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

25 

For  multicore  chips,  new  programming  models  and  tools  are  needed  to  develop 
parallel  applications,  and  existing  software  must  be  retrofitted.  New  chip  architec-
tures are needed to exploit rising transistor densities, support parallel execution and 
enable  heterogeneous  processing.  New  memory  technologies  and  interconnects  are 
needed  to  support  chips  with  tens  to  hundreds  of  cores.  Equally  importantly,  new 
algorithms  are  needed  that  map  efficiently  to  these  new  architectures.  All  of  these 
changes  will  affect  parallel  climate  models  now  being  developed  and  executed  on 
clustered  commodity  systems.  Today,  we  are  suffering  some  of  the  delayed  con-
sequences of limited research investment in parallel computing—architecture, system 
software, programming tools, data management and algorithms. 

In  addition  to  dramatic  changes  in  processors  and  computation,  our  models  of 
data  capture  and  management  are  in  flux.  We  can  now  generate,  transmit,  and 
store  data  at  rates  and  scales  unprecedented  in  human  history.  Many  of  our  new 
environmental  instruments  can  routinely  produce  many  tens  to  hundreds  of 
petabytes of data annually. The scientific data deluge threatens to overwhelm the ca-
pacity of our Federal institutions to manage, preserve and process and of our climate 
modeling researchers to access and integrate the data with multidisciplinary models. 
This data integration is critical to climate model validation. 

Although industry is developing massive data centers to host Internet search, so-
cial  networks  and  software  as  a  service,  our  research  data  infrastructure  has  not 
kept pace. Climate researchers need better data management tools, including prove-
nance tracking, translation, mining, fusion, visualization, and analysis. We must not 
focus  exclusively  on  computing,  but  on  the  fusion  of  sensors  and  data  management 
with computing hardware and rich climate models. 
5. Actions: A Sustainable, Integrated Approach 

One can and must draw several important, salutary lessons from the changing na-
ture of computing technology. The U.S. HPC industry is now driven by business and 
consumer  technology  economics,  with  concomitant  advantages  and  disadvantages. 
Large product volumes and amortized research and development costs lead to rapid 
innovation  and  technological  change.  However,  those  same  consumer  economics 
mean  that  today’s  HPC  systems  are  built  from  commodity  hardware  and  software 
components, and they are often ill-suited to the numerically and communication in-
tensive nature of climate change models. In consequence, they rarely deliver a large 
fraction of their advertized peak performance. 

Given their unique attributes, the highest capability computing systems have a very 
limited commercial market. The high non-recurring engineering costs to design HPC 
systems  matched  to  scientific  and  government  needs  are  not  repaid  by  sales  in  the 
commercial  marketplace.  Hence,  we  must  rethink  our  models  for  research,  develop-
ment,  procurement  and  operation  of  high-end  systems.  We  must  target  exploration 
of  new  systems  that  better  support  the  needs  of  scientific  and  national  defense  ap-
plications and sustain the Federal investment needed to design, develop and procure 
those  systems.  Today’s  approach  is  unlikely  to  provide  the  necessary  resources  to 
address the climate change model problem fully. 

New  programming  models  and  tools  are  also  needed  that  simplify  application  de-
velopment  and  maintenance  and  that  target  emerging  multicore  processors.  Today, 
almost  all  parallel  scientific  applications  are  developed  using  low-level  message- 
passing libraries. Climate modeling teams must have deep knowledge of application 
software behavior and its interaction with the underlying computing hardware, and 
they  often  spend  inordinate  amounts  of  time  tailoring  algorithms  and  software  to 
hardware and software idiosyncrasies, time more profitably spent on science and en-
gineering research. 

Climate  change  analysis  requires  large-scale  data  archives,  connections  to  sci-
entific  instruments  and  collaboration  infrastructure  to  couple  distributed  scientific 
groups.  Any  investment  in  HPC  facilities  must  be  balanced  with  appropriate  invest-
ments  in  hardware,  software,  storage,  algorithms  and  collaboration  environments. 
Simply  put,  climate  change  modeling,  as  with  all  scientific  discovery,  requires  a  ju-
dicious  match  of  computer  architecture,  system  software,  algorithms  and  software 
development tools. 

These  facts  illustrate  the  importance  of  a  long-term,  integrated  research  and  de-
velopment  program  that  considers  the  entire  computational  science  ecosystem, 
something  I  advocated  as  chair  and  co-chair  of  two  recent  PITAC  and  PCAST  sub-
committees,  respectively.  Both  the  2005  President’s  IT  Advisory  Committee  (PITAC) 
report  on  computational  science  and  the  2007  President’s  Council  of  Advisors  on 
Science  and  Technology  (PCAST)  review  of  the  Networking  and  Information  Tech-
nology  Research  and  Development  (NITRD)  program  recommended  creation  of  an 
interagency strategic roadmap for computational science and computing research. In 
particular, the 2005 PITAC report found that 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00029 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

26 

The  continued  health  of  this  dynamic  computational  science  ‘‘ecosystem’’  de-
mands  long-term  planning,  participation,  and  collaboration  by  Federal  R&D 
agencies and computational scientists in academia and industry. Instead, today’s 
Federal investments remain short-term in scope, with limited strategic planning 
and a paucity of cooperation across disciplines and agencies. 

The report also recommended creation of a long-term, interagency roadmap to 

. . .  address  not  only  computing  system  hardware,  networking,  software,  data 
acquisition and storage, and visualization, but also science, engineering, and hu-
manities algorithms and applications. The roadmap must identify and prioritize 
the  difficult  technical  problems  and  establish  a  timeline  and  milestones  for  suc-
cessfully addressing them. 

In  that  same  spirit,  the  2007  PCAST  review  of  the  NITRD  program,  Leadership 
Under  Challenge:  Information  Technology  R&D  in  a  Competitive  World,11 which  I 
co-chaired,  reiterated  the  need  for  a  strategic  plan  and  roadmap  for  high-perform-
ance computing and noted that 

The  Federal  NIT  R&D  portfolio  is  currently  imbalanced  in  favor  of  low-risk 
projects;  too  many  are  small-scale  and  short-term  efforts.  The  number  of  large- 
scale,  multidisciplinary  activities  with  long  time  horizons  is  limited  and  vision-
ary projects are few. 

Based on these studies, I believe we face both great opportunities and great chal-
lenges in high-end computing for climate change. Computational science truly is the 
‘‘third  pillar’’  of  the  scientific  process.  The  challenges  are  for  us  to  sustain  the  re-
search, development and deployment of the high-end computing infrastructure need-
ed to enable discoveries and to ensure the health of our planet. 

In  conclusion,  Mr.  Chairman,  let  me  thank  you  for  this  Committee’s  interest  in 
this  question  and  its  continue  support  for  scientific  innovation.  Thank  you  very 
much  for  your  time  and  attention.  I  would  be  pleased  to  answer  any  questions  you 
might have. 

Senator KERRY. Thank you very much, Dr. Reed. 
Dr. Sarachik? 

STATEMENT  OF  EDWARD  SARACHIK,  EMERITUS  PROFESSOR 
OF  ATMOSPHERIC  SCIENCE,  ADJUNCT  PROFESSOR  OF 
OCEANOGRAPHY,  AND  ADJUNCT  PROFESSOR  OF  APPLIED 
MATHEMATICS  AT  THE  UNIVERSITY  OF  WASHINGTON 
AND  CO-DIRECTOR,  CENTER  FOR  SCIENCE 
IN  THE 
EARTH SYSTEM 
Dr.  SARACHIK.  Thank  you,  Senator  Kerry  and  Senator  Stevens. 
My  name  is  Ed  Sarachik.  I  thought  I  retired  6  months  ago,  but  I 
seem to have not. I hold various appointments on the faculty at the 
University  of  Washington  and  I  am  Co-Director,  along  with  Ed 
Miles, of the Center for Science in the Earth System at the Univer-
sity  of  Washington.  It  is  a  very  interesting  center  because  it  goes 
from climate information to climate impacts, to dealing with stake-
holders,  and  to  raising  the  consciousness  of  stakeholders,  both  in 
the public and private domain. 

Basically  I  can  say  that  although  each  region  of  the  Pacific 
Northwest—and there are many climates within the Pacific North-
west—has  unique  problems.  All  of  them  need  a  skillful  prediction 
of  next  season’s  climate—that  is  precipitation  and  temperature— 
and  a  knowledge  of  the  future  variability  of  climate.  It  is  not  just 
how  the  mean  temperature  is  going  to  change.  We  do  not  respond 
to  the  mean.  We  respond  to  variability.  You  can  imagine  building 
for  70  degree  temperatures  and  it  would  matter  if  the  day  is  110 

11 Leadership  Under  Challenge:  Information  Technology  R&D  in  a  Competitive  World,  Presi-
dent’s  Council  of  Advisors  on  Science  and  Technology  (PCAST),  August  2007,  http:// 
www.ostp.gov/pdf/nitrdlreview.pdf. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00030 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

27 

and the evening is 30 or if it is going to be 70 degrees all the time. 
We respond to variability not to the mean. 

So  the  question  is  can  we  do  these  two  problems.  Can  we  make 
skillful  predictions  a  season  in  advance,  and  can  we  figure  out 
what  the  future  variability  of  climate  is  going  to  be  as  the  climate 
changes? 

And  here  the  answer  is  yes  and  no.  We  can  make  predictions  a 
season in advance. The reason it has been so cold this winter is be-
cause  it  has  been  a  La  Nin˜ a  year,  and  that  was  predicted  about  6 
months ago. But we cannot do the variability correctly. Despite the 
fact  that  we  are  spending  a  fair  amount  of  money  building  these 
models  for  the  IPCC,  the  IPCC  cannot  do  regional  climate.  It  can 
only  do  climate  on  continental  scales.  Continental  scales  are  not 
the scale at which applications are made. 

So  what  do  we  have  to  do?  The  basic  reason  we  cannot  do  the 
variability  correctly  is  that  the  known  modes  of  variability,  El 
Nin˜ o,  Pacific  Decadal  Oscillation,  and  the  North  Atlantic  oscilla-
tion,  are  simply  not  done  correctly  and  in  the  right  place  by  these 
models. In order to get them to do the right thing in the right place 
by  these  models,  there  certainly  are  modeling  issues.  As  has  been 
mentioned  so  far,  resolution  is  one  of  those  modeling  issues,  and 
for  that  resolution  we  need  bigger  computers,  to  be  sure.  But  we 
also need access to these computers. At this moment, none of these 
big  climate  models  are  being  run  at  universities  because  univer-
sities  simply  do  not  have  the  wherewithal  to  do  the  running  of  it. 
So access to the various places that would have interest in improv-
ing the variability of these models is absolutely crucial. 

The  second  leg  of  the  stool,  as  I  believe  has  been  mentioned  al-
ready, is observations. We do not have a climate observing system. 
If  we  do  not  have  a  climate  observing  system,  we  cannot  know 
what the climate is in all of its specificity around the globe. In par-
ticular,  we  make  observations,  but  these  observations  are  not  nec-
essarily connected dynamically. 

And  there  is  a  certain  amount  of  research  that  absolutely  needs 
to  be  done  on  El  Nin˜ o  southern  oscillation,  on  the  Pacific  Decadal 
Oscillation,  on  the  North  Atlantic  Oscillation,  and  the  effects  of 
CO2 and various other constituents on the atmosphere. 

A  lot  of  these  things—modeling  observations  and  research  needs 
to be done in an integrated manner. If we do not have the observa-
tions,  we  cannot  really  do  the  modeling.  If  we  do  not  have  the  re-
search,  we  cannot  really  do  the  modeling.  If  we  do  not  have  the 
modeling, we cannot really integrate the observations. These things 
really do need integration and some method of putting them all to-
gether and going ahead in a consistent manner. 

There has been a lot of talk about a national climate service. We 
have  a  National  Weather  Service.  The  National  Weather  Service 
takes  weather  observations,  integrates  them,  and  puts  out  maps 
twice or four times a day. We have nothing similar for climate, and 
having a national climate service would go a long way toward solv-
ing  some  of  the  problems  necessary  for  doing  good  regional  infor-
mation. 

Thank you. 
[The prepared statement of Dr. Sarachik follows:] 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00031 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

28 

PREPARED STATEMENT

OF EDWARD SARACHIK,  EMERITUS PROFESSOR

OF 
ATMOSPHERIC SCIENCE,  ADJUNCT PROFESSOR OF OCEANOGRAPHY,  AND ADJUNCT 
PROFESSOR OF APPLIED MATHEMATICS AT THE UNIVERSITY OF WASHINGTON AND 
CO-DIRECTOR, CENTER FOR SCIENCE IN THE EARTH SYSTEM 
My  name  is  Edward  Sarachik  and  I  am  Emeritus  Professor  of  Atmospheric 
Science, Adjunct Professor of Oceanography, and Adjunct Professor of Applied Math-
ematics  at  the  University  of  Washington.  I  am  also  Co-Director  of  the  Center  for 
Science in the Earth System (supported by NOAA) which contains two groups: a Cli-
mate Dynamics Group and a Climate Impacts Group. The Climate Dynamics Group 
studies  the  physical  climate  system  relevant  to  the  Pacific  Northwest  and  the  Cli-
mate Impacts Group examines the impacts of climate variability and change on the 
Pacific  Northwest,  and  produces  climate  information  products  and  derived  pre-
dictions  (e.g.,  streamflow  forecasts)  for  a  set  of  local  stakeholders.  The  combined 
Center  studies  the  general  problem  of  making  climate  information  useful  to  stake-
holders in the Pacific Northwest. The range of our activities and a list of our stake-
holders can be seen on our website: http://cses.washington.edu/. 

I have also chaired two National Research Council committees: one that produced 
an National Academy Press report Learning to Predict the Climate Variations Char-
acteristic of El Nin˜ o and the other, Improving the Effectiveness of U.S. Climate Mod-
eling,  both  highly  relevant  to  this  hearing.  I  also  chair  the  advisory  group  for  the 
International  Research  Institute  for  Climate  and  Society  at  Columbia  University 
which  deals  with  the  same  problem  as  that  of  this  hearing  but  in  an  international 
context. 
What do stakeholders want? 

They ask questions they would have asked in the absence of climate change: basi-
cally, some knowledge about the variability in the near future. Some examples from 
the Pacific Northwest: 

• All stakeholders want to know next season’s temperature and rainfall. 
• Power  companies,  city  water  utilities,  and  ski  area  operators  want  to  know 
whether next winter’s snowpack will be thick and long lasting or thin and early 
melting. 

• Fishers want to know if next season’s coastal mixed layer will be deep or shal-

low, warm or cold. 

• The tourist industry wants to know if next summer will be clear or cloudy. 
• Insurance  companies  and  state  flood  control  agencies  want  to  know  if  there  be 
an  unusual  number  of  storms  next  winter,  and  the  probability  that  there  will 
be destructive windstorms. 

Then they ask questions about the very long term, say 50 years from now: 
• Individuals  and  developers  want  to  know  if  they  should  build  near  the  ocean 

in the presence of rising sea level. Do they need a sea wall? 

• Foresters want to know what species of tree should be planted in what climate 
regime. In particular, what will be the future range of temperature and precipi-
tation? 

• Wineries  want  to  know  if  it  will  be  too  warm  for  specific  grape  varieties  and 

whether or not irrigation will be needed. 

• Everybody wants to know if it will get too warm for salmon survival. 
The progression of climate in a given small region is not what we are used to from 
global  warming  simulations.  For  temperature,  the  global  average  smoothes  the 
record  and  the  year  to  year  variability  is  about  half  a  degree  F.  Local  temperature 
record  has  a  year  to  year  variability  about  5  °F.  Since  the  year  to  year  variability 
in a limited region is of order of the 50 year warming trend, constantly dealing with 
next year’s climate over a long period of time gives practice about dealing with long 
term climate change since many (but not all) of the climate manifestations are simi-
lar. 

The  problem  of  producing  climate  information  relevant  to  decisionmakers’  needs 

then becomes 

gion. 

limited region. 

• Skillfully  predicting  next  year’s  temperature  and  precipitation  in  a  limited  re-

• Accurately  simulating  future  variability  of  temperature  and  precipitation  in  a 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00032 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

29 

Can existing climate models do this? 

The answer is both yes and no. 
Yes.  Next  years  climate  can  be  predicted  using  current  climate  conditions,  espe-
cially in the tropical oceans, as a starting point—this can only be done two or three 
seasons  in  advance.  There  are  a  number  of  groups  in  the  world  that  produce  such 
predictions  and  there  exists  a  ocean  observing  system  in  the  tropical  Pacific  that 
produce the current climate conditions. Estimates of the predictable part of seasonal 
temperature  variability  is  about  30  percent  for  the  Pacific  Northwest  and  about  40 
percent for the extreme Southeast part of the U.S. so that even if the prediction sys-
tems were perfect, only these percentages of future variations can be predicted. This 
makes predictions of next year’s climate intrinsically probabilistic. 

No.  Existing  climate  models  used  for  the  Intergovernmental  Panel  on  Climate 
Change  (IPCC)  process  are  comprehensive  global  models  and  are  designed  for  miti-
gation,  on  large  space  and  time  scales.  The  variability  known  to  be  important  re-
gionally (El Nin˜ o, Pacific Decadal Oscillation, North Atlantic Oscillation) in the cur-
rent  crop  of  models  used  in  the  IPCC  has  been  neglected  and  is  done  poorly.  The 
IPCC concentrates on global averages and freely admits that the smallest region for 
which  the  models  are  useful  is  the  continental  scale,  about  3000  mile.  On  scales 
smaller than continental scale, the models are not useful and downscaling to small-
er space scales by higher resolution models using the large global models as bound-
ary  conditions  can  not  be  expected  to  improve  the  situation.  The  output  of  existing 
models  can  be  corrected  to  agree  with  past  climate  conditions  and  the  correction 
used for future climates but there is no agreed upon methods for doing this. 
What is the best path to producing useful regional climate information? 

Ideally  we  want  a  comprehensive  climate  model  (similar  to  the  ones  currently 
used for the IPCC process) but which does the known patterns of climate variability 
(El Nin˜ o, Pacific Decadal Oscillation, North American Oscillation, etc.) correctly and 
which  is  run  globally  at  high  resolution  (20  miles  rather  than  the  current  100 
miles). 

This requires: 

1.  A  set  of  model  building  institutions  well  resourced  and  interacting  with  the 
entire public and private research sectors. 
2.  Far  more  capable  supercomputers.  And,  equally  important,  making  these 
supercomputers  and  advanced  models  available  to  the  entire  research  commu-
nity. 

Supercomputing  is  necessary,  but  it  is  not,  by  itself,  sufficient.  Also  required  is: 
3. A research program to investigate the nature of climate variability (especially 
decadal  variability)  and  assure  the  global  climate  models  are  capable  of  doing 
variability correctly and in the correct locations. 

All  research  ultimately  depends  on  having  good  observations—since  we  do  not 
have a climate observing system, all future progress in climate research will depend 
on implementing one. So also required is: 

4.  A  climate  observing  system  producing  regular  and  systematic  climate  obser-
vations. 

Since  the  output  of  the  climate  observing  system  will  never  cover  every  point  in 
the  atmosphere,  ocean  and  ice  over  the  entire  earth,  the  models  themselves  can  be 
used for interpolation, just as current weather models are used to assimilate weath-
er  observations  into  consistent  global  fields.  Therefore  the  last  component  required 
is 

5. A monthly analysis of the climate system using the observations produced by 
the climate observing systems in 4. and the models developed in 1. and 2. 

Because this hearing assumes it, it is hardly necessary to add: 

6.  A  distribution  network  for  regional  climate  and  resource  information  inter-
acting directly with local stakeholders. 

At least a major portion of 4, 5, and 6 could be accomplished by the establishment 

of a National Climate Service. 

It may seem strange that starting with models for simulating local climate infor-
mation  we  wound  up  with  far  more  comprehensive  requirements,  but  the  ability  to 
produce  useful  regional  climate  information  to  meet  stakeholder  needs  depends  on 
a  healthy  climate  infrastructure.  This  is  precisely  the  situation  in  that  the  ability 
to produce weather information for public and private use would be impossible with-
out  the  weather  infrastructure  contained  within  the  National  Weather  Service 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00033 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

30 

(NWS) and the research that is enabled by the observations and analyses emerging 
from the NWS. The ability to provide climate information to address end-user needs 
depends  generally  on  the  health  of  the  climate  infrastructure  and  the  climate  com-
munity. 

Senator KERRY. Thank you very much, Doctor. Appreciate it. 
Mr. Carlisle? 
STATEMENT OF BRUCE K. CARLISLE, ASSISTANT DIRECTOR, 

OFFICE OF COASTAL ZONE MANAGEMENT, EXECUTIVE 
OFFICE OF ENERGY, AND ENVIRONMENTAL AFFAIRS, 

COMMONWEALTH OF MASSACHUSETTS 

Mr.  CARLISLE.  Senator  Kerry  and  Senator  Stevens,  my  name  is 
Bruce  Carlisle  and  I  am  the  Assistant  Director  for  the  Massachu-
setts  Office  of  Coastal  Zone  Management.  I  want  to  thank  you  for 
the  opportunity  to  offer  testimony  on  the  importance  of  predicting 
the  effects  of  climate  change  through  a  national  modeling  strategy 
and ensuring that such a strategy meets the needs of state coastal 
managers and local officials. 

Our presence today is also on behalf of the Coastal States Orga-
nization  which  represents  the  interests  of  the  Governors  from  the 
35 coastal states, commonwealths, and territories on issues relating 
to sound management of our coasts, Great Lakes, and oceans. 

This  testimony  will  cover  climate  change  issues  in  the  coastal 
zone,  focusing  on  the  priority  modeling  and  information  needs  of 
coastal  zone  managers  around  the  country  and  highlighting  the 
work  being  done  in  Massachusetts  to  build  effective  coastal  flood 
plain  management  strategies  from  the  ground  up.  Your  continuing 
support  for  climate  change  modeling,  along  with  the  necessary  re-
search, monitoring, and computing infrastructure, is of critical and 
growing  importance  to  coastal  states  and  communities.  One  of  the 
points I will emphasize is that while a national strategy for climate 
change modeling and assessment is necessary, to be truly effective, 
it  must  be  connected  to  and  coordinated  with  state,  regional,  and 
local partners. 

Throughout  the  Nation,  our  coastlines  and  extensive  coastal 
floodplains play a significant role in protecting our homes, personal 
safety,  providing  recreational  opportunities  for  all  incomes,  pre-
serving  our  natural  resources  and  quality  of  life,  and  maintaining 
our  viable  economies.  Coastal  counties  host  more  than  half  of  the 
Nation’s  population,  support  nearly  half  of  the  Nation’s  jobs,  and 
generate more than half of its gross domestic product. With acceler-
ated sea level rise, more frequent intense storms and shifts in pre-
cipitation  and  temperatures,  the  coastal  zone  will  also  feel  the 
brunt  of  global  climate  change,  and  these  areas  will  be  subject  to 
increased flooding, shoreline erosion, saltwater intrusion into fresh 
water  aquifers,  harmful  algal  blooms,  and  the  loss  of  coastal  habi-
tats. 

For more than 30 years, state coastal managers like those at the 
Massachusetts Office of Coastal Zone Management have been lead-
ers  in  integrating  coastal  hazard  response  and  proactive  planning 
into  coastal  zone  management.  As  a  key  sector  and  end-user,  we 
have  identified  the  following  priorities  and  urge  Congress  to  pro-
vide support in addressing these needs. 

The  first  is  high  resolution  data  models  and  diagnostics  to  gen-
erate  regional  and  local  sea  level  rise  scenarios.  In  addition,  modi-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00034 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

31 

fication of wind speed and storm surge height models to assimilate 
changing  storm  intensity  and  frequencies  and  incorporate  the 
unique configurations and characteristics of local embayments. Ad-
ditionally,  more  information  to  better  understand  the  effects  of 
changing  sediment  transport,  erosion,  and  accretion  regimes  on 
habitats  and  the  important  ecosystem  services  they  provide.  Addi-
tional  modeling  on  climate  change  impacts  to  local  or  regional 
hydrological  processes  and  the  rate  of  saltwater  intrusion  into 
coastal aquifers. 

In Massachusetts and many other coastal states, coastal land use 
decisions  are  being  made  at  the  town  and  municipal  level  by  local 
officials who are working with shrinking budgets and resources and 
often  lack  technical  and  scientific  expertise.  Communities  are  in 
need  of  current  information  and  predictions,  packaged  and  deliv-
ered through specific tailored guidance on how to put that informa-
tion to use. 

To  start  to  address  such  needs,  the  Massachusetts  Office  of 
Coastal  Zone  Management  just  launched  its  new  StormSmart 
Coasts  program.  StormSmart  Coasts  is  designed  to  give  local  deci-
sion-makers  and  ultimately  businesses  and  homeowners  the  infor-
mation  and  tools  they  need  to  protect  themselves  from  coastal 
storm damage and flooding and to prepare for sea level rise and cli-
mate change. We deliver StormSmart Coasts tools via an extensive 
website  which  translates  complex  technical  information  into  user- 
friendly  guidance  and  planning  frameworks  with  links  to  the  best 
information  and  data  from  around  the  Nation.  Complicated  con-
cepts are reinforced through a series of short fact sheets explaining 
the tools and providing success stories. 

One of the basic building blocks of StormSmart Coasts is hazard 
identification  mapping.  The  StormSmart  Coasts  website  explains 
the  limitation  of  current  flood  maps,  which  for  most  communities 
in  Massachusetts  are  more  than  20  years  old  and  do  not  include 
the effects of erosion or sea level rise. StormSmart Coasts strongly 
advises  planners  to  seek  and  use  additional  sources  of  data  such 
as  storm  surge,  shoreline  change,  and  inundation  maps  to  assess 
their true vulnerability to coastal storm damage. 

There  are  two  pending  bills  that  would  assist  in  developing  key 
Federal-state  partnerships  to  support  these  needs.  Massachusetts 
and  the  Coastal  States  Organization  appreciate  the  work  of  Sen-
ator  Kerry  and  strongly  support  the  climate  change  research  and 
monitoring  activities  proposed  in  the  Global  Change  Research  Im-
provement  Act  of  2007.  Under  the  bill,  particular  attention  will  be 
focused on regional and state vulnerabilities to climate change. 

Massachusetts  and  the  Coastal  States  Organization  also  support 
the  climate  adaptation  provisions  in  America’s  Climate  Security 
Act  of  2007,  particularly  the  specific  allocation  of  5  percent  of  the 
emission  allowance  account  to  states  which  could  be  used  for  af-
fected coastal communities to adapt to climate change. These provi-
sions  recognize  that  coastal  states  and  communities  are  on  the 
front lines of climate change and will need Federal support that is 
proportionate to this risk. 

As  you  work  on  a  results-oriented  national  modeling  strategy, 
you  must  specifically  answer  the  kind  of  questions  asked  by  all 
coastal  communities  looking  to  implement  effective  coastal  flood-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00035 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

32 

plain  management.  What  are  the  current  risks  to  my  community, 
and how will those risks change in the future? 

Thank you again for the opportunity to testify. I would be happy 

to respond to any questions that you may have. 

[The prepared statement of Mr. Carlisle follows:] 

ZONE MANAGEMENT,  EXECUTIVE OFFICE

PREPARED STATEMENT OF BRUCE K.  CARLISLE,  ASSISTANT DIRECTOR,  OFFICE OF 
OF ENERGY,  AND 

COASTAL
ENVIRONMENTAL AFFAIRS, COMMONWEALTH OF MASSACHUSETTS 
Mr.  Chairman  and  Members  of  the  Committee,  my  name  is  Bruce  Carlisle  and 
I  am  the  Assistant  Director  for  the  Massachusetts  Office  of  Coastal  Zone  Manage-
ment. I want to thank you for the opportunity to offer testimony on the importance 
of predicting the effects of climate change through a national modeling strategy, and 
ensuring  that  such  a  strategy  meets  the  needs  of  state  coastal  managers  and  local 
officials, who will be the ultimate decision-makers and end-users of this information. 
Through  my  fourteen  years  of  working  on  coastal  policy,  planning,  and  manage-
ment,  I  am  keenly  aware  of  the  coastal  climate  change  information  needs  in  the 
Commonwealth. 

My  presence  today  is  also  on  behalf  of  the  Coastal  States  Organization  (CSO), 
which since 1970, has represented the interests of the Governors from the 35 coastal 
States,  Commonwealths,  and  Territories  on  Federal  legislative,  administrative,  and 
policy  issues  relating  to  sound  coastal,  Great  Lakes,  and  ocean  management.  CSO 
and its members have been actively engaged in this issue, and in November of last 
year, Dr. Braxton Davis, Chair of the CSO Climate Change Work Group and Direc-
tor of the Science and Policy Division at South Carolina’s Office of Ocean and Coast-
al  Resource  Management,  gave  testimony  to  your  committee  on  the  importance  of 
climate change research to state and local resource managers. 

This  testimony  will  cover  climate  change  issues  in  the  coastal  zone,  focusing  on 
the  priority  modeling  and  information  needs  as  conveyed  by  coastal  zone  managers 
around the country and highlighting the work being done in Massachusetts to build 
effective  coastal  floodplain  management  strategies  from  the  ground  up.  Your  con-
tinuing  support  for  climate  change  modeling,  along  with  the  necessary  research, 
monitoring,  and  computing  infrastructure,  is  of  critical  and  growing  importance  to 
coastal  states  and  communities.  One  of  the  points  I  will  emphasize  is  that  while  a 
national  strategy  for  climate  change  modeling  and  assessments  is  necessary,  to  be 
truly  effective,  it  must  be  connected  to  and  coordinated  with  state,  regional,  and 
local partners. 
Background 

Throughout the Nation, our coastlines and extensive coastal floodplains play a sig-
nificant  role  in  protecting  our  homes  and  personal  safety,  providing  recreation  op-
portunities for all incomes, preserving our natural resources and quality of life, pro-
viding spawning grounds critical to our fishing industry, and maintaining our viable 
local, regional, and state economies. The coastal zone will also feel the brunt of glob-
al  climate  change.  More  than  half  of  the  Nation’s  population  lives  in  coastal  coun-
ties,  and  key  economic  sectors  are  directly  linked  to  the  coasts  and  oceans.  Coastal 
counties host nearly half of the Nation’s jobs and generate more than half its gross 
domestic  product.  Through  the  combined  effects  of  climate  change—accelerated  sea 
level  rise,  more  frequent  and  intense  storms,  and  shifts  in  precipitation  and  tem-
peratures—these areas will see increased flooding and shoreline erosion, changes in 
sediment  transport,  saltwater  intrusion  into  groundwater  aquifers  and  coastal  riv-
ers, increased harmful algal blooms, the loss of coastal wetland and coral reef habi-
tats, and changes in population dynamics among marine and coastal species. Unless 
coastal decision-makers and officials start to plan for and implement effective meas-
ures  to  ensure  coastal  community  resiliency,  current  and  future  development  and 
activities—when  poorly  sited  and/or  designed—will  aggravate  these  impacts  over 
time. 

For more than 30 years, coastal managers—like those at the Massachusetts Office 
of  Coastal  Zone  Management—have  been  leaders  in  integrating  coastal  hazard  re-
sponse  and  proactive  planning  into  coastal  zone  management.  We  work  in  close  co-
ordination with both Federal agencies and local communities. Our efforts on coastal 
shoreline and floodplain management are extensive and include such actions as: de-
veloping critical information (e.g., high-resolution shoreline change data and coastal 
high-hazard zone delineation), coordinating the state’s Rapid Response Storm Dam-
age Survey Team to help spur recovery efforts, and providing hands-on technical as-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00036 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

33 

sistance to communities as they review development projects or develop beach man-
agement plans. 
Think Globally, Act Locally 

Large-scale  research,  observation,  and  modeling  are  critical  to  improving  our  un-
derstanding  of,  and  predictive  capabilities  for,  global  climate  change.  The  2003  Na-
tional  Strategic  Plan  for  the  U.S.  Climate  Change  Science  Program  explains  that 
while research focused on key and emerging climate change science areas is a high 
priority,  directly  supporting  regional  resource  management  efforts  is  also  a  critical 
component of the national strategy. The plan points to the development of scenarios 
and  comparisons,  the  implementation  and  application  of  models,  and  the  advance-
ment  of  information  supporting  adaptation  strategies  as  means  of  supporting  deci-
sion-making  at  all  levels.  Addressing  the  limitations  of  regional-  and  local-scale 
analyses of potential climate change impacts and improving the availability of such 
diagnostics  will  greatly  enhance  their  effectiveness  in  regional  and  local  decision- 
making  contexts.  As  a  key  ‘‘sector’’  and  ‘‘end-user,’’  the  CSO  has  identified  the  fol-
lowing  priority  information  and  products  to  address  future  impacts  of  climate 
change  in  the  coastal  zone,  and  we  urge  Congress  to  provide  support  in  addressing 
these needs: 

• Localized  Sea  Level  Rise  Scenarios—High-resolution  coastal  topographic  and 
bathymetric elevation data should be coupled with region-specific tide data, sea 
level  rise  projections,  and  other  key  input  parameters  to  develop  basic  inunda-
tion models for the assessment of lands and resources most vulnerable to accel-
erated  sea  level  rise.  These  regional  models  are  an  important  first  step,  but 
coastal  states  will  need  more  detailed  and  complex  models  that  incorporate 
local,  embayment-scale  changes  in  coastal  geomorphology,  hydrological  condi-
tions,  and  human  alterations  and  responses  (e.g.,  seawalls  and  beach  nourish-
ment)  to  more  adequately  assess  social,  environmental,  and  economic 
vulnerabilities of climate change. Coastal states and communities would benefit 
from  the  development  of  uniform  methods  for  modeling  local-scale  shoreline 
changes associated with varying sea level rise projections. 
• Storm  Surge  Models—Existing  models  that  estimate  wind  speeds  and  storm 
surge heights resulting from predicted storm events need to be broadened to in-
corporate changing storm intensities and frequencies as the result of global cli-
mate  change.  Again,  models  that  incorporate  the  unique  configurations  of  local 
embayments  or  coastline  morphologies,  water  depths,  and  physical  features 
such  as  bridges  and  roads  are  required  to  develop  accurate  storm  surge  pre-
dictions  and  serve  as  effective  planning  tools  for  decisions  being  made  today 
about the siting of new development and public infrastructure. 
• Impacts  on  Coastal  Habitats  and  Ecosystem  Services—The  integrity  of  many 
coastal habitats, such as estuarine marshes and beaches, are dependent on ade-
quate  sources  of  sediment  supply  and  the  accretion  of  sediments  at  certain 
rates.  To  predict  changes  to  the  these  habitats  and  the  important  ecosystem 
services  they  provide—such  as  flood  protection,  wildlife  habitat,  and  recre-
ation—more  information  is  needed  to  better  understand  erosion  and  deposition 
cycles  and  to  improve  our  ability  to  predict  the  effects  of  accelerated  rates  of 
sea  level  rise  on  sediment  transport,  and  accretion  and  erosion.  Without  suffi-
cient  vertical  accretion,  estuarine  marshes,  in  particular,  are  extremely  vulner-
able to being drowned by accelerated sea level rise. 
• Ground  Water  and  Salt  Water  Intrusion—Climate  change  will  have  significant 
effects  on  local  hydrologic  cycles  through  altered  precipitation,  evapotranspira-
tion, and soil moisture patterns. These changes will lead to altered groundwater 
recharge in watershed areas, which will change the groundwater flow to coastal 
regions  and  thus  the  rate  of  saltwater  intrusion  in  coastal  aquifers.  Additional 
modeling  on  the  climate  change  impacts  to  local  or  regional  hydrological  proc-
esses  and  coastal  water  resources  is  also  needed  to  manage  coastal  water  sup-
plies and estuarine biodiversity. 

In  Massachusetts  and  many  other  coastal  states,  coastal  land  use  decisions  are 
all  too  often  being  made  at  the  town  and  municipal  level  by  local  officials  who  are 
working  with  shrinking  budgets  and  resources,  and  often  lack  technical  and  sci-
entific  expertise.  Communities  are  in  critical  need  of  current  information  and  pre-
dictions,  packaged  and  delivered  through  specific,  tailored  guidance  on  how  to  put 
that information to use to make storm resilient communities a reality. Because state 
coastal programs provide high-quality products, services, and hands-on assistance to 
these  constituents,  they  are  uniquely  positioned  for  the  implementation  of  coastal 
climate change adaptation strategies. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00037 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

34 

StormSmart Coasts 

Created  by  the  Massachusetts  Office  of  Coastal  Zone  Management,  StormSmart 
Coasts  is  designed  to  give  local  decision-makers,  and  ultimately  businesses  and 
homeowners, the information and tools they need to protect themselves from coastal 
storm  damage  and  flooding,  and  to  prepare  for  sea  level  rise  and  climate  change. 
The strategy for initially delivering the StormSmart Coasts tools includes an exten-
sive  website  (www.mass.gov/czm/stormsmart)  and  a  series  of  regional  workshops. 
The  website  translates  complex  technical  information  into  user-friendly  guidance 
and regulatory models with links to the best information and data from around the 
Nation. Complicated concepts are reinforced through a series of short fact sheets ex-
plaining  the  tools  and  providing  success  stories  (see  attached  examples).  The  next 
phase  of  delivery  will  be  to  provide  targeted  technical  assistance  for  StormSmart 
tool  implementation  to  a  select  handful  of  coastal  communities,  and  then  take  the 
lessons learned from these efforts and translate and package them for use by other 
coastal communities within Massachusetts and nationwide. 
A Partnership at All Levels 

Led  by  a  Coastal  Management  Fellow  provided  by  the  National  Oceanic  and  At-
mospheric  Administration’s  (NOAA)  Coastal  Services  Center,  the  StormSmart 
Coasts program is very much a team approach. StormSmart Coasts would not have 
been  possible  without  support  and  contributions  from  individuals  and  groups  at  all 
levels.  The  StormSmart  Coasts  program  was  strongly  influenced  by  guidance  and 
advice from an attorney specializing in floodplain and wetlands law, representatives 
from the national Association of State Floodplain Managers, hazard mitigation staff 
from  our  state  Department  of  Conservation  and  Recreation,  Federal  Emergency 
Management Agency (FEMA) personnel, and local officials. Recognizing the value of 
StormSmart  Coasts  as  a  national  model,  the  Coastal  Services  Center  has  selected 
Massachusetts to receive another Coastal Management Fellow starting this summer 
to  implement  StormSmart  Coast  strategies  in  specific  Massachusetts  coastal  com-
munities. 
StormSmart Coasts and the Local Connection 

Throughout  its  development,  StormSmart  Coasts  has  benefited  from  extensive 
input  and  review  from  local  officials—the  key  target  audience  for  the  program.  By 
involving local officials at the earliest stages of program development, we have cre-
ated  tools  that  directly  meet  their  needs,  and  packaged  them  in  a  format  that  they 
can easily understand, access, and successfully implement. Empowering local action 
is critical, because in the end, it is the decisions that are made locally that will de-
termine  if  we  can  successfully  adapt  to  climate  change  and  be  resilient  to  natural 
hazards so as to avoid such tragedies as experienced in the aftermath of Hurricane 
Katrina. 
No Adverse Impact 

The  StormSmart  Coasts  program  is  based  around  the  concept  of  No  Adverse  Im-
pact.  No  Adverse  Impact  is  a  set  of  ‘‘do  no  harm’’  principles  for  local  communities 
to  follow  when  planning,  designing,  or  evaluating  public  and  private  development 
activities and storm-damage prevention measures. This approach clarifies that com-
munity  leaders  not  only  have  the  legal  right  to  consider  the  cumulative  impacts  of 
their  permitting  decisions,  they  have  the  legal  responsibility.  No  Adverse  Impact 
tools  and  techniques  ensure  that  private  development,  public  infrastructure,  and 
planning  activities  do  not  have  direct  or  indirect  negative  consequences  on  the  sur-
rounding natural resource areas, private property, or other communities. 
Applying Model Outputs to Coastal Land Use Decisions 

One  of  the  basic  building  blocks  of  StormSmart  Coasts  is  hazard  identification 
and mapping. The StormSmart Coasts website explains the limitation of the current 
FEMA  Flood  Insurance  Rate  Maps,  which  are  engineering  estimates  of  the  extent 
of  the  floodplain  at  the  time  of  the  mapping.  For  most  communities  in  Massachu-
setts,  those  maps  are  more  than  20  years  old  and  do  not  include  the  effects  of  ero-
sion  or  sea  level  rise.  StormSmart  Coasts  strongly  advises  hazard  mitigation  plan-
ners  to  seek  and  use  additional  sources  of  data,  such  as  storm  surge,  shoreline 
change,  and  inundation  maps,  to  assess  their  true  vulnerability  to  coastal  storm 
damage.  They  need  current  and  specific  information,  synthesized  and  adapted  to 
suit  their  requirements  to  best  plan  for  and  strategically  address  coastal  floodplain 
management  issues,  adapt  to  climate  change  issues,  and  reduce  impacts  for  future 
generations. 

The  Massachusetts  Office  of  Coastal  Zone  Management  has  extensive  experience 
packaging  technical  information  for  use  by  local  decision-makers.  One  example  is 
our shoreline change maps, which measure and estimate the changes in the Massa-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00038 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

35 

chusetts  coastline  as  a  result  of  natural  erosion  and  accretion,  as  well  as  relative 
sea  level  rise.  These  maps  and  all  accompanying  data  are  available  on  our  website 
(www.mass.gov/czm/hazards/shorelinelchange/shorelinechangeproject.htm) with a 
fact sheet explaining how to use the maps. These resources receive thousands of hits 
per year and are used locally to supplement information provided by outdated flood 
maps. 
The Time to Act Is Now 

It  is  very  important  to  emphasize  that  this  is  not  a  problem  only  for  the  future. 
In an increasing number of communities along the Massachusetts coast, erosion and 
flooding impacts are increasingly causing damage even during today’s minor storms. 
And  with  climate  change,  these  impacts  will  only  grow  as  storms  increase  in  fre-
quency and intensity. 
Successful Strategies through Federal-State Partnerships 

Through  the  Coastal  Zone  Management  Act  amendment  process,  provisions 
should be developed to allow states and territories to develop specific coastal climate 
change  adaptation  plans  and  strategies.  States  also  support  increased  funding  for 
climate  change  activities  and  support  legislation  that  would  encourage  NOAA  and 
other agencies to assist the states via technical assistance, mapping, modeling, data, 
and forecasting products, and intergovernmental coordination. Federal activities re-
lated  to  coastal  adaptation  should  be  coordinated  closely  with  states  by  involving 
coastal zone management programs early in the planning process. 

There are several emerging areas where state, Federal, and other partners are ac-
tively  working  on  improved  coordination  and  cooperation  for  more  effective  coastal 
and  ocean  management.  One  of  these  is  the  new  Integrated  Ocean  Observing  Sys-
tem  (IOOS)  initiative.  Led  by  NOAA,  the  IOOS  program  seeks  to  integrate  coastal 
and  ocean  observing  capabilities,  in  collaboration  with  Federal  and  non-Federal 
partners, to maximize access to data and generation of information products and in-
form  decisionmaking.  Massachusetts  has  been  participating  in  both  the  Northeast 
and  Mid-Atlantic  Regional  Coastal  Ocean  Observing  Systems,  which  are  comprised 
of diverse partners including state and Federal agencies, academic institutions, and 
coastal  and  maritime  interests.  In  both  of  these  regions,  remote  observation  tech-
nologies (e.g., instruments on buoys and high frequency radar) and the development 
of prototype products have been prioritized to address the issue area of coastal inun-
dation. When fully operational, real-time observations on meteorological and oceano-
graphic  measurements  will  be  integrated  into  interactive  products  such  as  a  Gulf 
of Maine Storm Simulation and Prediction System. 

Another  example  of  emerging  synchronization  is  the  Northeast  Regional  Oceans 
Council  (NROC).  Consisting  of  delegates  from  the  six  New  England  states  and  ex- 
officio members from Federal agencies, NROC was established in 2005 by resolution 
of  the  New  England  Governor’s  Association.  The  primary  function  of  the  council  is 
to  engage  in  efforts  that  require  or  benefit  from  regional  actions  to  address  issue 
areas  of  ocean  and  coastal  ecosystem  health,  coastal  hazards  resiliency,  ocean  en-
ergy  planning  and  management,  and  maritime  security.  By  increasing  communica-
tion  and  cooperation  among  regional  interests,  the  council  provides  new  forums  for 
information  exchange  and  strategic  state-Federal  collaboration  on  such  actions  as 
regional climate change activities and initiatives. 

Finally,  the  Joint  Subcommittee  on  Ocean  Science  and  Technology  created  the 
Interagency  Working  Group  on  Ocean  and  Coastal  Mapping  in  response  to  rec-
ommendations of the U.S. Ocean Action Plan and the 2004 National Research Coun-
cil report, A Geospatial Framework for the Coastal Zone: National Needs for Coastal 
Mapping  and  Charting.  The  Interagency  Working  Group  on  Ocean  and  Coastal 
Mapping brings together Federal, state, industrial, academic, and nongovernmental 
organizations to coordinate the best use of mapping resources and to avoid duplica-
tion of effort. One of the first tasks for this group is to develop an inventory of ocean 
and  coastal  mapping  data  and  activities.  At  a  recent  strategic  planning  workshop 
in  February  2008,  highlights  of  Federal  ocean  and  coastal  mapping  activities  were 
presented,  and  representatives  from  Massachusetts,  Florida,  and  California  pro-
vided updates of their current data collection and mapping activities, best practices, 
and  challenges.  All  participants  identified  coordination,  collaboration,  and  partner-
ships as keys to successful past and future efforts. 
Legislative Opportunities 

There are two pending bills that could assist in developing these key Federal-state 
partnerships.  Massachusetts  and  CSO  appreciate  the  work  of  Senator  Kerry  and 
strongly  support  the  climate  change  research  and  monitoring  activities  proposed  in 
the Global Change Research Improvement Act of 2007 (S. 2307). The proposed legis-
lation would establish a national climate service through NOAA to address weather, 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00039 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

36 

climate  change,  and  climate  variability  affecting  public  safety,  advancing  the  na-
tional  interest  in  understanding,  forecasting,  responding,  adapting  to,  and  miti-
gating  the  impacts  of  both  natural  and  human-induced  climate  change  and  climate 
variability.  National  level  research,  infrastructure,  and  coordinated  outreach  and 
communication  mechanisms  would  directly  support  state  and  local  policymakers  by 
providing  comprehensive  national  research  to  assist  with  regional  adaptation  and 
mitigation planning. Under the bill, existing Federal climate change research would 
be  coordinated  and  particular  attention  would  be  focused  on  regional  and  state 
vulnerabilities  to  climate  change,  allowing  communities  to  utilize  national  data  to 
help address adaptation and mitigation on a localized level. 

Massachusetts  and  CSO  also  support  the  climate  adaptation  provisions  in  Amer-
ica’s  Climate  Security  Act  of  2007  (S.  2191),  particularly  the  specific  allocation  of 
5  percent  of  the  Emission  Allowance  Account  to  states,  which  can  be  used  for  spe-
cific purposes, one of which is to collect, evaluate, disseminate, and use information 
necessary  for  affected  coastal  communities  to  adapt  to  climate  change.  We  are  in 
favor  of  the  expansion  of  the  Adaptation  Fund,  funded  through  the  emissions  cap 
and  trade  program,  to  include  coastal  adaptation.  These  provisions  recognize  that 
coastal  states  and  communities  are  on  the  front  lines  of  climate  change  and  will 
need Federal support that is proportionate to this risk. 
The Future of a Successful Climate Modeling Partnership 

As  state-level  coastal  managers,  we  can  develop  new  tools  and  package  available 
tools  through  programs  like  StormSmart  Coasts.  While  we  will  always  do  the  best 
we can with the information we have available, the current scarcity of regional- and 
local-scale, high-priority data and information is alarming. For example, to improve 
our  understanding  of  current  and  future  coastal  floodplains  and  high-hazard  zones, 
we need topographical information in finer resolution than the coarse 10- to 20-foot 
contour  intervals  available  today.  Similarly,  while  there  are  hydrodynamic  models 
that  encompass  regional  systems  (e.g.,  Gulf  of  Maine,  Massachusetts  Bay),  these 
have  not  been  tailored  to  the  region’s  complex  coastline  and  bathymetry,  which  in-
cludes  numerous  islands  and  shoals,  and  they  lack  the  necessary  field  measure-
ments  for  model  verification  and  refinement.  Without  adequate  data  or  resources, 
state  and  local  decision-makers  cannot  accurately  map  the  existing  extent  of  the 
coastal  floodplain,  let  alone  project  what  that  floodplain  will  look  like  in  the  next 
30  years.  Given  the  scientific  complexity  and  levels  of  funding  involved,  state  and 
local  governments  cannot  possibly  hope  to  fill  this  data  gap  alone.  We  are  very 
pleased  to  know  that  the  Federal  Government  is  looking  to  fulfill  this  role,  and  we 
guarantee  that  if  you  get  us  the  information  we  need,  we  are  prepared  to  use  it 
wisely.  Our  personal  safety,  ecosystems,  and  local  and  regional  economies  depend 
on it. 

But  data  alone  cannot  solve  the  problem—this  information  must  get  into  the 
hands of the people who can use it to make better choices about development, rede-
velopment,  and  storm-damage  protection,  including  municipal  officials,  business 
owners, and current and future homeowners in coastal floodplain areas. 

Through  StormSmart  Coasts,  we  have  built  the  framework  and  have  begun  to 
work  with  coastal  communities  to  implement  results-oriented  strategies.  But  ulti-
mately,  the  effectiveness  of  those  strategies  is  limited  by  the  data,  models,  and 
diagnostics  available—and  the  information  generated  through  a  strategic  climate 
modeling  approach  that  provides  such  decision-support  resources  as  reliable  esti-
mates  of  sea  level  rise  in  the  next  few  decades  will  be  the  key  to  future  success. 
With  this  critical  gap  filled,  local  and  state  officials  will  be  able  to  successfully  im-
plement  real-world  strategies  to  address  this  very  real  problem—creating  a  true 
partnership that maximizes the best of what all levels of government have to offer. 
Conclusion 

As  you  move  forward,  we  strongly  encourage  you  to  look  at  how  state  programs 
like  StormSmart  Coasts  serve  as  successful  examples—demonstrating  not  only  how 
states  can  fine-tune  and  package  the  data  and  information  developed  through  the 
Federal climate change programs for the local decision-makers to use in a real-world 
context—but  also  how  all  levels  of  government  can  work  together  successfully.  To 
ensure that you continue to build a results-oriented national climate modeling strat-
egy, we strongly encourage you to work with state coastal managers, as well as local 
officials, to understand our specific needs. To be effective, such a strategy must spe-
cifically  answer  the  kind  of  questions  asked  by  all  coastal  communities  looking  to 
implement  effective  coastal  floodplain  management—what  are  the  current  risks  to 
my  community  and  how  will  those  risks  change  in  the  future.  Please  help  us  put 
all of the pieces together so we can respond quickly and effectively to future coastal 
hazards. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00040 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

37 

Thank  you  again  for  the  opportunity  to  testify  on  the  importance  of  national  ef-
forts  for  climate  change  modeling.  I  would  be  happy  to  respond  to  any  questions 
that you may have. 

FACT SHEET 1 

Introduction to No Adverse Impact (NAI) Land Management in the Coastal 

Zone 

A legally sound way for municipalities to protect people and property 
What Is NAI? 

No  Adverse  Impact  (NAI)  is  a  forward-thinking,  fair,  and  legally  defensible  ap-
proach to coastal land management. In its broadest sense, it is a set of ‘‘do no harm’’ 
principles to follow when your community is planning, designing, or evaluating pub-
lic and private development activities and storm-damage prevention measures. 

While  seawalls  and  other  structures  can  sometimes  provide  storm  protection,  they  generally 
require  regular  expensive  upkeep  and  often  lead  to  other  problems  (including  beach  erosion). 
Marshfield, Massachusetts. 

NAI  protects  the  rights  of  residents,  businesses,  and  visitors  in  your  community 
by  requiring  that  public  and  private  projects  be  designed  and  completed  in  such  a 
way  that  they  do  not:  (1)  pose  a  threat  to  public  safety,  (2)  increase  flood  or  storm 
damage to public or private property, and/or (3) strain municipal budgets by raising 
community  expenditures  for  storm-damage  mitigation,  stormwater  management, 
emergency services, and disaster recovery efforts. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00041 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
1
L
R
A
C
8
0
5

38 

NAI: Local and Comprehensive 

Careful  management  of  coastal  floodplains  is  critical  to  protect  people  and  prop-
erty, and to reduce the financial strain on businesses, private property owners, and 
municipal  budgets.  While  the  Commonwealth  of  Massachusetts  has  passed  regula-
tions  to  help  prevent  storm  damage,  ultimately  most  of  the  authority  and  tremen-
dous responsibility to manage floodplains is entrusted to local governments. 

Accurately  evaluating  the  potential  effects  of  proposed  activities  can  be  chal-
lenging,  and  requires  looking  both  on  and  offsite,  since  damage  often  isn’t  confined 
to  the  parcel(s)  under  review.  For  example,  the  construction  of  a  home  may  change 
stormwater flow and increase erosion (removal of sediment by water or wind) to sur-
rounding  properties.  Similarly,  new  parking  lots,  roads,  and  buildings  may  redirect 
stormwater  onto  other  properties  instead  of  allowing  it  to  be  reabsorbed  into  the 
ground. 

In addition to being costly to repair, roads damaged by storms can become hazards for rescue 
personnel and others. This road in Rockport, Massachusetts, was destroyed by a 2007 nor’easter. 
Since  each  permit  might  be  considered  to  set  a  precedent,  it  is  critical  that  com-
munities  consider  the  potential  cumulative  effects  of  their  decisions—a  number  of 
seemingly insignificant projects can collectively cause substantial damage. The NAI 
approach  clarifies  that  community  leaders  not  only  have  the  legal  right  to  consider 
the  cumulative  impacts  of  their  permitting  decisions,  they  have  the  legal  responsi-
bility.  Increasingly,  communities  that  permit  projects  that  result  in  flooding  or 
storm damage to other properties end up in land court. (See the StormSmart Coasts 
Fact Sheet 2, No Adverse Impact and the Legal Framework of Coastal Management). 
Adopting the NAI approach also gives your community the chance to clearly articu-
late a ‘‘do no harm’’ goal for all future land use. 
The NAI Approach 

The  Association  of  State  Floodplain  Managers  (ASFPM),  a  national  organization 
of  professional  flood  hazard  specialists  from  all  levels  of  government,  the  research 
community,  the  insurance  industry,  and  technical  fields,  identifies  three  different 
levels of floodplain management strategies: Basic, Better, and NAI. 

• Basic:  Approaches  typically  used  to  meet  minimum  Federal  or  state  require-

ments for managing floodplains and coastal areas to minimize flood losses. 

• Better:  Activities  that  are  more  effective  than  the  basic  level  because  they:  (1) 
are  tailored  to  specific  situations,  (2)  provide  protection  from  larger  floods,  (3) 
allow for uncertainty in storm magnitude prediction, and (4) serve multiple pur-
poses. 

• NAI:  Tools  and  techniques  that  go  further  than  the  measures  defined  as  ‘‘bet-
ter’’  by  ensuring  that  private  development,  public  infrastructure,  and  planning 
activities  do  not  have  direct  or  indirect  negative  consequences  on  the  sur-
rounding natural resource areas, private property, or other communities. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00042 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
2
L
R
A
C
8
0
5

39 

ASFPM  has  created  seven  NAI  Building  Blocks,  which  can  help  communities  to 
maintain and enhance flood protection. These building blocks—hazard identification 
and  mapping;  planning;  regulations  and  development  standards;  mitigation;  infra-
structure  siting  and  design;  emergency  services;  and  public  outreach  and  edu-
cation—are  briefly  introduced  in  the  table  on  the  next  page.  For  more  information, 
see  ASFPM’s  Coastal  NAI  Handbook  at  www.floods.org,  or  the  StormSmart  Coasts 
website at www.mass.gov/czm/stormsmart. 

NAI Building Block 

Basic 

Better 

NAI 

NAI Building Blocks 

Hazard 
Identification 
and Mapping 

Use FEMA Flood In-
surance Rate Maps for 
land use decisions. 

Planning 

Use land use planning 
and zoning through a 
community master 
plan. 

Regulations 
and Development 
Standards 

Follow Federal Emer-
gency Management 
Agency National Flood 
Insurance Program 
regulations. 

Gather and use detailed 
coastal hazard data (e.g., 
historic erosion rates, ac-
tual observed extents of 
floodwaters) for land use 
decisions. 

Incorporate coastal hazard data 
(e.g., erosion rates, vulnerability of 
environmentally sensitive areas, 
and sea-level rise rates and im-
pacts) into community-wide plan-
ning maps and regulations. 

Develop floodplain man-
agement plans that include 
stormwater management 
and hazard mitigation 
measures. Promulgate de-
tailed guidance focusing on 
reducing flood damage. 

Design special area management 
plans to: protect storm damage 
and flood control functions of nat-
ural resources, promote reasonable 
coastal-dependent economic 
growth, and improve protection of 
life and property in hazard-prone 
areas. 

Adopt conditions for siting 
new development. Regu-
late cumulative, substan-
tial improvements. Revise 
regulatory tools for ad-
dressing erosion along 
shorelines including: relo-
cation of threatened build-
ings, building setbacks, 
beach nourishment and 
bio-engineering, and sta-
bilization of eroded areas. 

Preserve sensitive areas through 
bylaws and regulations that may: 
establish maximum densities for 
development, restrict structures 
between the shoreline and the set-
back line, mandate vegetative 
coastal buffers rather than man-
made structures (bulkheads, sea-
walls, or groins), minimize imper-
vious cover, and preserve stream 
corridor and wetland buffers. Reg-
ulate placement of fill. 

Mitigation 

Use common practices, 
such as flood proofing 
existing structures. 

Elevate or relocate build-
ings. Acquire land. Encour-
age nonstructural methods 
for shoreline protection. 

Stabilize shorelines with vegeta-
tion. Prohibit construction in espe-
cially damage-prone areas. Prevent 
filling of wetlands and other low-
lands. Nourish beaches where ap-
propriate. Protect watersheds. 
Monitor corrective efforts. Regu-
late construction of shore-protec-
tion structures. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00043 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
3
L
R
A
C
8
0
5

40 

NAI Building Blocks—Continued 

NAI Building Block 

Basic 

Better 

NAI 

Infrastructure 
Siting and Design 

Respond to storm 
events as they occur. 
After a storm, rebuild/ 
repair to previous con-
dition. 

Upgrade damaged facilities 
to more hazard-resistant 
standards. Inventory haz-
ard risks of all public 
buildings. Insure buildings 
for all hazards (as appro-
priate). Identify, and if 
possible, relocate or protect 
‘‘critical facilities.’’ 

Prohibit major public infrastruc-
ture investments in special flood 
hazard areas. Ensure that roads, 
sewer lines, and utility upgrades 
don’t encourage development in 
hazard-prone areas. Zone to pro-
hibit construction in high-hazard 
areas. Locate new critical facilities 
above 500-year flood-plain. 

Emergency 
Services 

Create and use generic 
hazard response plan. 

Create and test commu-
nity-wide hazard plans 
that involve all local 
boards and departments. 

Public Outreach 
and Education 

Answer questions and 
provide information as 
requested by public. 

Periodically inform resi-
dents of coastal hazards, 
vulnerability, and mitiga-
tion techniques through 
public workshops, and in 
forums after storm recov-
ery. 

Create plans to ensure that all 
people who want or need to be 
evacuated can be moved to safe 
shelters, and post-disaster plans 
that improve community flood re-
sistance through: willing land ac-
quisition, determining which struc-
tures are ‘‘substantially damaged,’’ 
and ensuring that appropriate re-
construction meets code require-
ments. Establish mutual aid 
agreements with neighboring com-
munities. 

Create comprehensive education 
and out-reach programs using ex-
pertise of state and Federal agen-
cies (when needed) to encourage 
community-wide proactive storm 
preparation. Establish coastal haz-
ard disclosure requirements for 
property sales. 

The Benefits of NAI 

While  NAI  strategies  require  investment  in  planning  and  implementation,  they 

offer real benefits for your community. NAI can . . . 

• Save  money:  Less  damage  means  lower  post-storm  community  cleanup  costs, 
fewer demands on public officials’ limited time, and reduced strain on public re-
sources. 

• Decrease  litigation:  NAI  principles  have  been  judicially  tested  and  courts  have 
shown  immense  deference  to  regulations  that  seek  to  prevent  harm  (for  an  ex-
ample,  see  the  StormSmart  Coasts  Fact  Sheet  3,  A  Cape  Cod  Community  Pre-
vents New Residences in Floodplains). NAI can also help your community avoid 
potential  litigation  over  ineffectual  flood  management  practices  that  result  in 
future  damage  or  loss  of  life.  (See  Fact  Sheet  2,  No  Adverse  Impact  and  the 
Legal Framework of Coastal Management.) 

• Reduce conflicts with property owners: NAI doesn’t say ‘‘no.’’ It says ‘‘yes, if . . .’’ 
It is a common-sense approach that seeks to protect everyone’s property by only 
allowing projects that eliminate or mitigate their impacts. 

• Reduce  risk  to  people  and  public  and  private  property:  Better  planned  and  de-
signed  development  and  public  infrastructure  is  less  likely  to  cause  and  suffer 
damage.  An  NAI  approach  can  help  protect  the  beaches  that  are  critical  to 
many communities’ economies. 

• Lower flood insurance rates: The Community Rating System (CRS) is a Federal 
Emergency  Management  Agency  (FEMA)  program  that  decreases  flood  insur-
ance  rates  for  communities  with  effective  hazard  mitigation  strategies.  Many 
NAI  strategies  qualify  for  CRS  credits.  For  more  information  see  the  CRS  Re-
source Center at training.fema.gov/EMIWeb/CRS/. 

• Increase  your  capacity  to  bounce  back  after  a  storm:  Reduced  storm  damage 
means  less  downtime  and  less  costly  clean  up  for  local  businesses,  which  is  es-
pecially important for small, locally owned businesses that may otherwise strug-
gle to stay solvent during frequent or prolonged closures. 

• Clarify  your  land  use  objectives:  By  adopting  NAI  principles,  your  community 
can  articulate  the  overarching  goals  that  help  bring  consistency  and  predict-
ability to permitting. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00044 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

41 

• Preserve  quality  of  life:  With  NAI  you  can  help  make  your  community  safer 
while preserving quality of life for your citizens now and in the future. An NAI 
approach  can  help  ensure  that  your  community  resources,  including  beaches, 
public  parks,  and  other  open  spaces,  are  there  to  be  enjoyed  by  future  genera-
tions. 

For More Information .

.

. 

• For more on the theory of NAI and its application in coastal areas, see the Asso-
ciation  of  State  Floodplain  Managers  website  (www.floods.org),  especially  their 
Coastal  NAI  Handbook.  Also  see  the  StormSmart  Coasts  website  at 
www.mass.gov/czm/stormsmart. 

• For  more  on  the  legal  issues  surrounding  coastal  management,  see  the 
StormSmart Coasts Fact Sheet 2, No Adverse Impact and the Legal Framework 
of Coastal Management. 

• For  an  example  of  NAI-type  regulations  at  work,  see  the  StormSmart  Coasts 
Fact Sheet 3, A Cape Cod Community Prevents New Residences in Floodplains. 
• For  a  more  detailed  look  at  the  legal  theory  behind  this  and  similar  cases  in-
volving  land  management  in  hazardous  areas,  see  the  Association  of  State 
Floodplain  Managers’  No  Adverse  Impact  Floodplain  Management  and  the 
Courts by attorneys Jon Kusler and Ed Thomas, at www.floods.org. 

No Adverse Impact and the Legal Framework of Coastal Management 
How communities can protect people and property while minimizing lawsuits 

FACT SHEET 2 

Managing  coastal  floodplains  is  a  challenging  endeavor  that  sometimes  is  incor-
rectly  thought  to  put  local  government’s  duty  to  protect  people  and  property  in  di-
rect  conflict  with  property  rights.  Most  local  officials  want  to  reduce  the  harm  and 
costs  associated  with  coastal  storms,  and  recognize  that  unwise  development  can 
worsen the situation. Unfortunately, as our society has grown more litigious, it may 
seem  harder  for  municipal  governments  to  stay  out  of  land  court  when  preventing 
or  conditioning  development  projects,  even  when  there  is  good  evidence  that  these 
projects may create problems for others. However, the No Adverse Impact (NAI) ap-
proach  to  land  use  management  is  an  appropriate  way  to  protect  people,  property, 
and  property  rights.  (To  learn  more  about  NAI,  see  the  StormSmart  Coasts  Fact 
Sheet 1, Introduction to No Adverse Impact (NAI) Land Management in the Coastal 
Zone.) 

While  nothing  can  prevent  all  legal  challenges,  following  the  NAI  approach  can 
help  to:  (1)  reduce  the  number  of  lawsuits  filed  against  local  governments,  and  (2) 
greatly increase the chances that local governments will win legal challenges to their 
floodplain  management  practices.  The  legal  system  has  long  recognized  that  when 
a  community  acts  to  prevent  harm,  it  is  fulfilling  a  critical  duty.  The  rights  of  gov-
ernments to protect people and property have been well recognized by the legal sys-
tem  since  ancient  times.  Courts  from  the  Commonwealth  of  Massachusetts  to  the 
U.S. Supreme Court have consistently shown great deference to governments acting 
to  prevent  loss  of  life  or  property,  even  when  protective  measures  restrict  the  use 
of private property. This ‘‘prevention of harm’’ principle is the foundation of the NAI 
approach.  The  goal  of  this  fact  sheet  is  to  provide  local  officials  with  information 
on how to use the NAI tools to confidently protect people and property in a fair and 
effective way, while avoiding lawsuits (even those alleging takings). 

Two key points: 

1. Communities have the legal power to manage coastal and inland floodplains. 
2. Courts may (and often do) find that communities have the legal responsibility 
to do so. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00045 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

42 

These  Sandwich  homeowners  proactively  protected  their  property  by  planting  beach  grass. 
Vegetating dunes and banks can reduce erosion and slow floodwaters without adversely impact-
ing other properties. 
How NAI Can Help Your Community Avoid Lawsuits 

The best way to avoid losing in court is to stay out of court. One of the strengths 
of  the  NAI  approach  is  that  its  clear  goal  (the  prevention  of  harm)  fosters  and  en-
courages  cooperation  between  landowners  and  regulators  as  they  work  together  to 
try to find solutions to the problems associated with proposed projects. Such collabo-
ration is a great way to stay out of land court. 

When  avoiding  court  isn’t  possible,  following  the  NAI  approach  can  greatly  in-
crease  the  chances  that  local  governments  will  win  in  lawsuits  arising  from  their 
floodplain  management  practices.  The  most  common  and  historically  problematical 
challenges  that  local  officials  face  while  trying  to  regulate  use  of  private  property 
are allegations of ‘‘constitutional takings.’’ 
Not all the uses an owner may make of his property are legitimate. When regulation 

prohibits wrongful uses, no compensation is required.’’—The Cato Institute 

Takings background: This fact sheet summarizes a complex body of law under the 
so-called  ‘‘Takings  Clause’’  of  the  Fifth  Amendment  to  the  U.S.  Constitution.  This 
summary  is  not  intended  to  be  legal  advice  for  any  particular  situation,  and  may 
not  be  relied  upon  as  such.  To  determine  whether  a  particular  regulation  would 
cause  a  taking,  communities  should  consult  with  an  attorney.  Property  owners  file 
takings  cases  when  they  believe  regulations  violate  their  constitutional  property 
rights.  The  legal  basis  for  these  arguments  can  be  found  in  the  Fifth  Amendment 
of the U.S. Constitution, which prohibits the Government from taking private prop-
erty  for  public  use  without  compensation.  The  interpretation  of  the  courts  through 
the  years  has  clarified  that  the  Fifth  Amendment  encompasses  more  than  an  out-
right  physical  appropriation  of  land.  In  certain  situations,  the  courts  have  found 
that  regulations  may  be  so  onerous  that  they  effectively  make  the  land  useless  to 
the  property  owner,  and  that  this  total  deprivation  of  all  beneficial  uses  is  equiva-
lent  to  physically  taking  the  land.  In  such  a  situation,  courts  may  require  the  gov-
erning  body  that  has  imposed  the  regulation  to  either  compensate  the  landowner 
or repeal the regulation. 

Needless to say, with local budgets strapped and coastal land values skyrocketing, 
it  is  rarely  economically  feasible  for  local  governments  to  compensate  landowners 
when,  for  example,  prohibiting  a  house  on  a  solid  foundation  in  an  area  known  to 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00046 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
4
L
R
A
C
8
0
5

43 

flood,  or  preventing  the  construction  of  a  seawall  to  protect  a  home  on  an  eroding 
bluff. 

NAI to the Rescue: It is critical that management decisions respect property rights 
and  follow  general  legal  guidelines  (see  the  ‘‘Legal  Dos  and  Don’ts  of  Floodplain 
Management’’  text  box).  The  courts  have  made  it  very  clear  that  property  rights 
have  limits.  For  example,  both  Commonwealth  of  Massachusetts  and  Federal  laws 
acknowledge  that  property  owners  do  not  have  the  right  to:  be  a  nuisance,  violate 
the property rights of others (for example, by increasing flooding or erosion on other 
properties), trespass, be negligent, violate reasonable surface water use and riparian 
laws, or violate the public trust. 
The Four Types of Regulatory Takings 

The best way to understand how the NAI approach helps to prevent takings chal-
lenges is to look specifically at what the courts have decided may constitute a regu-
latory  taking.  In  2005,  the  U.S.  Supreme  Court  ruled  on  a  precedent-setting  case 
(Lingle v. Chevron), which clearly established regulatory taking guidelines. In their 
unanimous decision, the Court determined that there are four ways for a regulation 
to be a taking. Each way is briefly discussed below, with a non-technical explanation 
of how they are relevant to an NAI approach. (For a more detailed legal explanation 
of  these  cases,  see  the  latest  edition  of  No  Adverse  Impact  Floodplain  Management 
and  the  Courts,  published  by  the  Association  of  State  Floodplain  Managers  at 
www.floods.org.) 

1.  A  physical  intrusion.  Governments  may  not,  without  compensation,  place  any-
thing  on  private  property  against  the  wishes  of  the  owner.  The  case  discussed 
(Loretto  v.  Teleprompter  Manhattan)  involved  a  New  York  City  requirement  that 
building owners allow the cable company to install a small cable box and cables on 
all residential buildings. Because the NAI approach doesn’t generally promote struc-
tural solutions, this type of regulatory taking is unlikely to apply. However, if a com-
munity’s NAI plan involves the placement of structures (culverts, for example) on pri-
vate  property,  this  ruling  makes  it  clear  that  the  community  may  be  required  to  ob-
tain the permission of the landowner or pay compensation. 

2.  A  total  or  near-total  regulatory  taking.  If  a  regulation  restricts  property  rights 
to  such  a  degree  that  it  eliminates  all  or  essentially  all  economically  viable  uses  of 
a piece of property, this may constitute a taking. The case reviewed (Lucas v. South 
Carolina Coastal Council) was filed by a landowner who was prohibited from build-
ing a home on a barrier beach. In their opinion, the Court clearly states that regula-
tions  aimed  at  preventing  nuisance  don’t  constitute  takings.  It  warns,  though,  that 
governing  bodies  arguing  that  specific  regulations  are  designed  to  prevent  nuisances 
will  need  to  demonstrate  how  they  are  addressing  similarly  situated  nuisances  (i.e., 
regulations  may  not  be  applied  arbitrarily).  The  NAI  approach  can  help  your  com-
munity  to  consistently  articulate  how  potentially  harmful  projects  are  nuisances. 
When  designing  land  use  regulations,  your  community  should  always  try  to  ensure 
that  the  owner  retains  at  least  some  economically  beneficial  uses.  This  is  both  fair 
and  helps  establish  the  legal  reasonableness  of  your  regulations.  Note  that  land 
uses that harm others are not legal or beneficial, and that beneficial uses don’t nec-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00047 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
5
L
R
A
C
8
0
5

44 

essarily  include  building  residences  or  other  structures,  especially  in  hazardous 
areas.  Where  new  regulations,  even  hazard-based  regulations,  could  sharply  de-
crease  the  market  price  of  property,  consider  allowing  the  transfer  of  development 
rights  to  areas  where  your  community  would  like  growth  to  occur.  To  learn  about 
transferable  development  rights,  see  www.mass.gov/envir/smartlgrowthltoolkit/ 
pages/mod-tdr.html. 

3. A significant, but not near-total regulatory taking. Courts hearing takings argu-
ments should consider three factors that have ‘‘particular significance’’—(a) the mag-
nitude  of  the  economic  impact,  (b)  how  severely  the  regulation  affects  ‘‘investment- 
backed expectations,’’ and (c) the character of the government in action. The central 
case  discussed  (Penn  Central  v.  City  of  New  York)  concerned  a  denied  expansion  of 
Grand  Central  Station  in  New  York  City.  The  historic  preservation  regulation  re-
viewed  in  this  case  seeks  to  protect  neighborhood  character—not  to  prevent  physical 
harm. These are two very different things in the eyes of the law. The U.S. legal sys-
tem  sometimes  requires  governments  to  compensate  landowners  when  property 
rights are compromised for community improvement, but less frequently when they 
prevent  potential  harm.  There  is  no  property  right  to  use  or  develop  land  in  a  way 
that harms others, even if that use maximizes the particular site’s economic potential. 
There  is  no  constitutional  or  legal  right  to  a  good  return  on  investments.  Unfortu-
nately, some people invest in land with erroneous ideas about what they are legally 
allowed  do  with  it,  and  when  forbidden  to  do  as  they  wish,  may  argue  that  regula-
tions have devalued their property. The courts have made it clear that while regula-
tions designed to prevent harm may reduce the market value of a piece of property, 
they do not decrease its true value, and hence NAI-based regulations cannot trigger 
this aspect of a taking test. A 2005 Massachusetts Supreme Judicial Court decision 
upheld  a  coastal  town’s  regulation  prohibiting  new  residences  in  its  coastal  flood-
plain  because  the  town  successfully  established  that  this  regulation  was  designed 
to prevent harm and did not render the land valueless. 

For more information, see the StormSmart Coasts Fact Sheet 3, A Cape Cod Community Pre-

vents New Residences in Floodplains. 

4.  Insufficient  relationship  between  the  requirement  and  the  articulated  govern-
ment  interest.  If  a  community  conditions  a  permit,  the  requirements  it  exacts  from 
the  landowner  must  be  related  to  the  goals  of  the  regulation  and  must  be  ‘‘roughly 
proportional’’  to  the  predicted  impacts  of  the  proposed  development.  In  the  two 
cases,  Nollan  v.  the  California  Coastal  Commission  and  Dolan  v.  City  of  Tigard, 
landowners  were  required  to  provide  a  public  right  of  way  as  a  permit  condition, 
even  though  the  proposed  developments  did  not  reduce  public  access.  The  NAI  ap-
proach  avoids  this  type  of  taking  by  tightly  binding  regulations  to  the  specific  goal 
of preventing harm. 

With  these  and  other  decisions,  the  courts  have  made  it  clear  that  governments 
may regulate land without compensation if they do so with the intent of preventing 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00048 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
6
L
R
A
C
8
0
5

45 

harm. Fairly applied No Adverse Impact regulations make the ‘‘takings issue’’ a non- 
issue. 

From  the  property  rights  perspective,  it’s  worth  noting  that  the  Cato  Institute, 
which advocates for limited government, individual liberty, and free markets, agrees 
that  preventing  landowners  from  causing  harm  to  others  does  not  constitute  a  tak-
ing: 

‘‘Owners  may  not  use  their  property  in  ways  that  will  injure  their  neighbors. 
Here the Court has gotten it right when it has carved out the so-called nuisance 
exception  to  the  Constitution’s  compensation  requirement.  Thus,  even  in  those 
cases  in  which  regulation  removes  all  value  from  the  property,  the  owner  will 
not  receive  compensation  if  the  regulation  prohibits  an  injurious  use.’’—Roger 
Pilon,  Senior  Fellow  and  Director—Cato  Institute  (to  the  U.S.  House  of  Rep-
resentatives, 2/10/95) 

‘‘The  takings  clause  was  never  intended  to  compensate  property  owners  for  property 

rights they never had.’’—Massachusetts Supreme Judicial Court 

Why You Should Manage Your Floodplains 

Protecting people and property is a fundamental duty of all levels of government. 
One  of  the  most  effective  ways  that  local  governments  protect  people  and  property 
is through the permitting process. Here, local officials can and should do what they 
can  to  reduce  the  likelihood  that  the  development  or  use  of  property  will  cause 
harm. 

Communities should also be aware that in a growing number of states, courts are 
favoring  plaintiffs  that  sue  local  governments  for  permitting  projects  that  later 
cause  damage  to  property  (for  example,  permitting  the  construction  of  roads  that 
back-up streams and increase flooding in the community). For more information on 
this trend, see No Adverse Impact Floodplain Management and the Courts (available 
at www.floods.org), where the authors found that a community is vastly more likely 
to  be  successfully  sued  for  allowing  improper  development  that  causes  harm  than 
for prohibiting it. 

The  take-home  lesson:  As  a  local  official,  you  have  been  given  the  responsibility 
and the legal rights to manage coastal and inland floodplains. If you do so in a way 
that expressly seeks to prevent harm, the courts will support you. 
For More Information .

. 

.

This  is  not  and  cannot  be  legal  advice.  To  answer  specific  legal  questions  please 
see an attorney licensed in your jurisdiction. To learn more about the general legal 
framework of NAI-based floodplain management see: 

• No  Adverse  Impact  Floodplain  Management  and  the  Courts  for  an  excellent 
overview  of  the  case  history  of  NAI  at  www.floods.org.  While  this  document  is 
designed  for  attorneys,  it  is  useful  for  anyone  working  in  floodplain  manage-
ment. 
• The  StormSmart  Coasts  Fact  Sheet  3,  A  Cape  Cod  Community  Prevents  New 
Residences in Floodplains, which examines a community’s successfully defended 
NAI-type bylaw. 

• The Coastal NAI Handbook at www.floods.org. 
• The  NAI  section  of  the  Association  of  State  Floodplain  Managers  website  at 
• The Institute for Local Government’s one-page publication, 10 Tips for Avoiding 
Takings  Claims,  at  cacities.org/index.jsp?displaytype=11&zone=ilsg&section= 
land&sublsec=landlproperty&tert=&story=20219. 
• The  American  Planning  Association’s  1995  Policy  Guide  on  Takings  at 
• The StormSmart Coasts website at www.mass.gov/czm/stormsmart. 

www.planning.org/policyguides/takings.html. 

www.floods.org. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00049 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

46 

FACT SHEET 3 

Case  Study—A  Cape  Cod  Community  Prevents  New  Residences  in 

Floodplains 

Lessons learned from Chatham’s legally successful conservancy districts 

In a landmark 2005 ruling, the highest court in Massachusetts decisively affirmed 
the authority of municipalities to regulate or even prevent residential or other high- 
risk  development  in  flood-prone  areas  without  financial  compensation  to  the  prop-
erty  owners,  so  long  as  the  regulation  does  not  render  the  land  entirely  valueless. 
The  case  arose  from  the  town  of  Chatham’s  refusal  to  permit  the  construction  of 
a new home in a flood zone because the local zoning bylaw prohibited new residen-
tial  units  in  the  town’s  mapped  floodplains.  After  multiple  appeals  by  the  land-
owner,  the  Massachusetts  Supreme  Judicial  Court  ruled  on  July  26,  2005,  that  the 
zoning  bylaw  was  based  on  reasonable  public  interest,  and  did  not  render  the  lot 
economically  worthless.  Therefore,  no  compensation  was  due.  The  decision  was  not 
appealed. 
The Zoning Bylaw 

Chatham’s zoning bylaw designates ‘‘conservancy districts’’ encompassing all land 
in the town’s 100-year floodplain as mapped in its most recent town-approved Flood 
Insurance  Rate  Maps.  The  goal  of  the  bylaw  is  to  protect  people,  property,  and  re-
sources  (see  ‘‘Chatham  Conservancy  District  Purposes’’  sidebar).  The  bylaw  clearly 
delineates  three  types  of  activities  in  designated  conservancy  districts—permitted 
uses,  special  permit  uses,  and  prohibited  uses—examples  are  shown  in  the  table 
below. 

Permitted uses 

Special permit uses 

Prohibited uses 

Examples from Chatham’s Zoning Bylaw 

Fishing, cultivation, and harvesting 
of  shellfish  (including  excavation  of 
areas for cultivation and harvesting 
of  marine 
foods);  various  horti-
culture activities 

Outdoor  recreation  activities,  pro-
vided that related structures do not 
destroy  beneficial  character  of  dis-
trict 

Floats 

Maintenance  of  existing 
roadways Installation of utilities 

raised 

Agriculture 

Government  dredging  of  navigation 
channels 

Construction  and  maintenance  of 
town 
landings  and  public  boat 
launching  ramps;  nourishment  of 
town beaches 

Mosquito control by Cape Cod Mos-
quito Control Project 

Maintenance  of  existing  channels 
and marine facilities 

Construction of certain structures, 
including catwalks, piers, ramps, 
stairs, boat shelters, tennis courts. 

Filling of land 

Draining of land 

Construction of structures or 
buildings used in conjunction with 
a marina or boatyard. 

Construction and maintenance of 
driveways or roadways of min-
imum legal length and width. 

Construction and maintenance of 
private boat launches and beach-
es. 

Installation of submerged pipes or 
cables used for swimming pools or 
commercial fishing operations. 

Discharging of hazardous sub-
stances, treated sewage, or ther-
mal effluent 

Construction of residential units 
or use of houseboats or barges as 
dwellings 

Building of any structure in V and 
V1–30 Zones 

Construction of pipelines to carry 
crude oil or unprocessed natural 
gas 

Actions that destroy natural vege-
tation, alter existing tidal flow, or 
otherwise alter the character of 
the land 

Destruction of natural growth that 
prevents erosion or storm damage 

Draining, damming, or relocating 
water courses except for aqua-
culture, agriculture, or flood or 
mosquito control 

‘‘The  takings  clause  was  never  intended  to  compensate  property  owners  for  property 

rights they never had.’’—Massachusetts Supreme Judicial Court 

The Case 

The  lawsuit  concerned  a  1.8-acre  parcel  located  in  Chatham’s  mapped  floodplain 
(and  therefore,  in  a  conservancy  district).  In  1998,  the  owner  of  the  lot  received  an 
offer  of  $192,000  for  the  parcel,  contingent  upon  the  ability  of  the  purchaser  to  ob-
tain  the  permits  necessary  to  build  a  home.  The  proposed  home  was  to  be  elevated 
on open piles above the mapped 100-year flood elevation. 

Because  the  lot  is  located  within  a  conservancy  district,  the  town’s  Zoning  Board 
(the district permitting authority) denied the building permit application. The owner 
of the lot responded by filing one suit against the Selectmen and Zoning Board and 
another  against  the  town’s  Conservation  Commission  (the  construction  would  have 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00050 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

47 

also violated a local wetlands bylaw), each suit alleging that the bylaws violated the 
owner’s  constitutional  property  rights,  and  that  the  town  had  thereby  effectively 
‘‘taken’’  her  property  (for  more  on  constitutional  takings,  see  StormSmart  Coasts 
Fact Sheet 2, No Adverse Impact and the Legal Framework of Coastal Management). 
A  Superior  Court  judge  combined  the  two  suits.  After  a  two-day  trial,  which  in-
cluded  testimony  on  the  flood  history  of  the  property,  the  risks  and  impacts  of  its 
potential development, and the difficulty in safely evacuating the area, the Superior 
Court  found  insufficient  evidence  to  support  the  plaintiff’s  claims  that  the  bylaws 
had resulted in a regulatory land taking, and upheld the town’s decision. 

When  the  plaintiff  appealed  the  decision,  the  Massachusetts  Appeals  Court  af-
firmed  the  Superior  Court’s  decision.  While  acknowledging  that  the  bylaw  did  se-
verely  constrict  the  possible  uses  of  the  lot,  the  Appeals  Court  noted  that  ‘‘a  land- 
use  regulation  may  deprive  an  owner  of  a  beneficial  property  use—even  the  most 
beneficial  such  use—without  rendering  the  regulation  an  unconstitutional  taking.’’ 
The Appeals Court further noted that: 

‘‘As  a  matter  of  Massachusetts  law,  restricting  residential  development  within 
the path of floodwater, the flood plain, is a direct, logical, and reasonable means 
of  safeguarding  persons  and  property  from  those  hazards  occasioned  by  a  flood 
and  advances  a  substantial  state  interest,  that  is,  the  health,  safety,  and  wel-
fare of the general public as well as that of its individual members.’’ 

The  arrow  indicates  the  approximate  location  of  the  proposed  home  site.  This  satellite  photo-
graph  also  shows  the  breach  in  the  barrier  beach  from  1987.  The  breach  greatly  increased  the 
exposure of the lot and surrounding properties to wave and storm surge. 

The plaintiff then appealed to the Massachusetts Supreme Judicial Court, which, 
after  reviewing  the  case,  upheld  the  lower  courts’  rulings,  citing  a  recent  U.S.  Su-
preme  Court  decision  that  had  rendered  zoning  bylaws  and  ordinances  valid  under 
the U.S. Constitution so long as their application bears a ‘‘reasonable relation to the 
State’s legitimate purpose’’ (such as protecting people and property). 

The  decision  also  noted  that  while  the  regulation  may  have  indeed  reduced  the 
market value of the property, the prevention of one potential use for a piece of prop-
erty did not constitute a total taking. A witness for the plaintiff estimated that with 
the bylaw, the lot was worth at least $23,000—a substantial reduction but still more 
than  a  ‘‘token’’  interest,  according  to  the  decision  which  cited  a  (2001)  case  where 
the U.S. Supreme Court ruled that no compensation was due when a regulation re-
duced the appraised value of a parcel from $3,150,000 to $200,000. 

Finally,  the  decision  noted  that  there  was  ample  evidence  showing  that  the  con-
struction of a home on the lot could have severe adverse impacts on the surrounding 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00051 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
7
L
R
A
C
8
0
5

48 

community.  The  plaintiff’s  expert  testified  that  the  proposed  house  could  be  picked 
up  off  its  foundation  and  floated  away  by  a  severe  storm,  potentially  damaging 
neighboring  homes.  The  defendant  offered  testimony  that  efforts  to  evacuate  the 
home during a flood would pose risks to rescue workers, as well as the home’s occu-
pants. 

A  Nauset  Beach  home  destroyed  by  a  2007  storm.  As  was  noted  in  the  Massachusetts  Su-
preme  Judicial  Court’s  ruling,  damaged  structures  like  the  one  in  this  photo  can  create  debris 
that may threaten other structures. 

The  Massachusetts  Supreme  Judicial  Court  concluded  that  no  compensation  was 
due to the property owner, because: ‘‘The taking clause was never intended to com-
pensate property owners for property rights they never had.’’ 

The decision was not appealed. 

Why Chatham Won the Case 

1.  The zoning bylaw had the clear goals of protecting people and property. 
2.  While the bylaw prevents construction of new homes, it leaves property owners with 

many alternative uses. The land retains more than a ‘‘token’’ value. 

3.  The law was fair, and applied to identifiable, mapped areas (i.e., wasn’t ‘‘spot zoning,’’ 

which unfairly prevents one individual property owner from using property in a certain 
way). 

4.  The town’s emergency management experts testified that evacuation of the areas would 

put rescue workers at risk. 

5.  The town was willing to legally defend its position. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00052 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
8
L
R
A
C
8
0
5

49 

Top: The erosional beach near the proposed home site is prone to flooding and storm damage. 
Bottom:  An  area  ofChatham  in  the  floodplain  where  flooding  can  make  evacuation  difficult. 

For More Information .

.

. 

• For  an  overview  of  the  legal  framework  of  coastal  management  in  Massachu-
setts,  see  the  StormSmart  Coasts  Fact  Sheet  2,  No  Adverse  Impact  and  the 
Legal Framework of Coastal Management. 

• For the text of the decision, see www.socialaw.com/slip.htm?cid=15382. 
• For  a  copy  of  the  bylaw  see  www.chatham-ma.gov/Publicldocuments/chat 

hammalCommDev/Zbylaw2005.pdf. 

• For  a  more  detailed  look  at  the  legal  theory  behind  this  and  similar  cases  in-
volving  management  of  land  in  hazardous  areas,  see  the  Association  of  State 
Floodplain  Managers’  No  Adverse  Impact  Floodplain  Management  and  the 
Courts, by attorneys Jon Kusler and Ed Thomas at www.floods.org. 

• The  Massachusetts  StormSmart  Coasts  webpage:  www.mass.gov/czm/ 

stormsmart. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00053 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
9
L
R
A
C
8
0
5

50 

As  coastal  areas  of  Massachusetts  continue  to  change  in  response  to  erosion  and  storms,  the 
relative risks to properties do too. While the risk to these homes near a new breach is obvious, 
homes  on  the  mainland  that  were  once  protected  by  the  shifting  barrier  island  also  face  in-
creased exposure. (Photo: Nauset Beach, Chatham.) 

Senator KERRY. Thank you very much, Mr. Carlisle. 
Dr. Walsh? 
STATEMENT OF JOHN E. WALSH, DIRECTOR, COOPERATIVE 

INSTITUTE FOR ARCTIC RESEARCH, INTERNATIONAL 
ARCTIC RESEARCH CENTER, UNIVERSITY OF ALASKA 

Dr.  WALSH.  Senator  Kerry,  Senator  Stevens,  thank  you  for  the 
chance  to  speak  today.  I  am  John  Walsh  from  the  International 
Arctic  Research  Center.  I  am  also  Director  of  NOAA’s  Cooperative 
Institute for Arctic Research at the University of Alaska. 

In  many  respects,  Alaska  is  ground  zero  for  recent  climate 
change.  We  are  seeing  a  dramatic  loss  of  ice,  melting  glaciers, 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00054 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

s
p
e

.

1
1
L
R
A
C
8
0
5

.

s
p
e
0
1
L
R
A
C
8
0
5

51 

warming permafrost. Senator Stevens is well aware of the changes 
that are ongoing in Alaska. 

I  would  like  to  focus  on  the  gap  between  what  the  stakeholders 
need,  what  they  are  requesting  of  the  climate  community,  and 
what climate models are actually delivering. 

Alaska has a diverse population. It ranges from small indigenous 
communities  that  are  reliant  on  subsistence  activities  to  growing 
urban  areas  and  to  an  energy  sector  on  which  the  rest  of  the  U.S. 
depends. Much of the infrastructure is built on permafrost, and one 
of  the  main  characteristics  of  Alaska’s  climate  is  its  wide  seasonal 
swings.  In  addition  to  that,  there  are  tremendous  spatial  vari-
ations.  The  graphics  in  the  written  testimony  provide  an  example 
of the extreme contrasts spatially in the variables such as tempera-
ture. 

I will cite one example for what that means for the weather and 
climate  of  Alaska  relative  to  modeling.  The  interior  valleys,  the 
Yukon  River  Valley,  the  Tanana  River  Valley,  are  precipitation 
shadows  in  reality.  The  present  climate  models  with  their  resolu-
tions of 100 to 200 kilometers present these areas as maximum ele-
vation  regions  with  precipitation  greater  than  the  surrounding 
areas.  So  we  are  completely  losing  the  precipitation  signal  over  a 
large portion of the state because of inadequate resolution. The res-
olution that we need in order to capture fields such as precipitation 
is  between  one  and  two  orders  of  magnitude  finer  than  what  we 
now have in the latest generation of global models. 

A  high  priority  for  Alaskans  is  the  tailoring  of  model  output  to 
include the information that is most relevant to the needs of plan-
ners  and  the  public,  as  well  as  other  stakeholders.  The  variables 
that  are  carried  in  climate  models  are  often  not  the  ones  that  cor-
respond most to the user needs. In Alaska, some examples of these 
user  needs  are  information  about  the  firmness  of  the  ground  for 
overland transportation, snow cover characteristics, vegetative dry-
ness during the fire season, and wind chill temperatures in exposed 
areas. 

A  recent  illustration  of  the  needs  and  the  gap  relative  to  the 
modeling  capabilities  is  the  attempt  by  Peter  Larsen  and  his  col-
leagues in Anchorage to estimate the economic risks to public infra-
structure in Alaska as a result of climate change in the coming dec-
ades.  The  global  models  on  which  he  based  his  scenarios  have  un-
certainties  in  themselves,  and  they  produced  a  range  of  a  factor  of 
two to three in the estimates of infrastructure costs to be expected 
from climate change over the next 50 years. 

But  more  importantly  are  the  limitations  on  the  availability  of 
variables beyond temperature and precipitation which were used in 
the  Larsen  study.  Infrastructure  such  as  buildings  and  roads  will 
clearly be affected by freeze-thaw cycles, by changes in snow loads, 
by the temperature extremes, the peak-wind events, and the occur-
rences of flooding. We are a long way from being able to obtain that 
type of information in a credible manner from today’s climate mod-
els, and there has actually been little effort to translate model out-
put into these quantities that the users and the stakeholders need. 
So  the  bridging  of  the  models  and  the  user  needs  is  an  emerging 
area of activity. This need is intertwined with this need for higher 
resolution and for more credible model simulations. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00055 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

52 

So the people of Alaska are already calling for more detailed and 
robust  information  than  they  are  receiving  from  climate  models, 
and  I  would  argue  that  the  challenge  of  integrating  user  needs 
such  as  those  in  Alaska  with  advances  in  climate  modeling  gives 
us an opportunity to respond to our taxpayers at the regional scale 
and to serve as a prototype for some globally integrated climate de-
livery services. 

Thank you. 
[The prepared statement of Dr. Walsh follows:] 

PREPARED STATEMENT OF JOHN E.  WALSH,  DIRECTOR,  COOPERATIVE INSTITUTE FOR 
ARCTIC RESEARCH,  INTERNATIONAL ARCTIC RESEARCH CENTER,  UNIVERSITY OF 
ALASKA 

Climate Modeling for Decision-Makers and Stakeholders in Alaska 
Alaska’s statewide annual average temperature has increased by 3.4 °F since the 
mid-20th  century,  and  the  increase  is  much  greater  (6.3  °F)  in  winter.  The  higher 
temperatures  of  the  recent  decades  have  been  associated  with  an  earlier  snowmelt 
in  spring,  a  reduction  of  summer  sea  ice  coverage,  a  retreat  of  many  glaciers,  and 
a warming of permafrost. These surface changes, as well as their associated climate 
drivers, have two characteristics that require advances in modeling if projections of 
change are to meet the needs of decision-makers and planners. First, feedbacks be-
tween  ice,  snow  and  the  atmosphere  exert  potentially  strong  leverage  on  high-lati-
tude climate change, and these feedbacks introduce large uncertainties into simula-
tions by existing climate models. For example, the recent retreat of summer sea ice 
is  occurring  at  a  faster  rate  than  projected  by  any  of  the  models  in  the  recent 
Fourth  Assessment  (2007)  of  the  Intergovernmental  Panel  on  Climate  Change 
(Stroeve  et  al.,  2007).  There  are  also  indications  that  feedbacks  may  already  be  oc-
curring between the earlier spring snowmelt and the surface energy budget, result-
ing in an increase of vegetative greenness (photosynthetic activity) in parts of Alas-
ka  (Euskirchen  et  al.,  2007).  Second,  the  surface  changes  are  highly  variable  over 
small spatial scales, largely as a result of complex topography and coastal configura-
tions  around  the  region.  The  figure  below  illustrates  the  fine  resolution  required  to 
capture the spatial variations in Alaskan climate. 

Figure  1.  Average  July  daily  high  temperatures  in  Alaska  for  1961–1990.  Color  ranges  are 
40–45  °F  (blue),  45–50  °F  (green),  50–55  °F  (yellow),  55–65  °F  (orange),  and  65–75  °F  (darker 
red). Image is from the PRISM database (Daly et al., 2008). 

In  contrast  to  the  2  km  resolution  in  figure  above,  the  grid  cell  dimensions  (spa-
tial  resolution)  of  global  climate  models  are  typically  100–200  km.  Figure  2  below 
shows  the  smoothness  of  projected  temperature  changes  obtained  from  the  global 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00056 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
1
H
S
L
A
W
8
0
5

53 

models  for  Alaska.  The  mis-match  of  scales  is  even  greater  for  precipitation,  which 
is  a  variable  that  is  of  great  interest  to  users  of  climate  information  pertaining  to 
water supplies, inland transportation, forestry, and terrestrial ecology. 

Figure  2.  Projected  changes  of  annual  mean  temperature  (°F)  over  Alaska  for  the  late  21st 
century  (2090),  based  on  the  B1  simulations  by  the  models  used  in  the  IPCC’s  (2007)  Fourth 
Assessment. Yellow denotes a warming of 3–5 °F, deep red a warming of 8–10 °F. 

How can the utility of climate projections be made more useful to decision-makers 
and  stakeholders  in  Alaska?  Based  on  the  experience  of  the  Alaska  Center  for  Cli-
mate Assessment and Prediction (a NOAA Regional Integrated Sciences and Assess-
ment Center), the greatest needs are: (1) downscaling of the coarse-resolution model 
output,  (2)  reduction  of  the  uncertainty  inherent  in  the  model-derived  projections, 
and (3) tailoring of model output to include variables and information more directly 
relevant  to  the  needs  of  planners  and  stakeholders.  In  the  remainder  of  this  testi-
mony, we address these needs and approaches to meeting these needs. 

The  mis-match  of  scales  between  Figures  1  and  2  can  be  addressed  by  two  types 
of  downscaling:  dynamical  and  statistical.  Dynamical  downscaling  consists  of  the 
nesting of a high-resolution regional model inside a coarser-resolution global model. 
This approach has been tested in various regions of the world, and its effectiveness 
is  highly  dependent  on  the  validity  of  the  input  supplied  at  the  lateral  boundaries 
by the global model. For Alaska, the approach is being applied to simulations of the 
mass  balance  of  glaciers  in  southeastern  Alaska.  The  nesting  of  finer  grids  inside 
coarse grids achieves 1 km resolution over the glaciers. Applications to other surface 
features  (e.g.,  permafrost,  ecosystem  changes)  are  being  developed.  The  second  ap-
proach to downscaling is statistical in its nature. In this case, statistical algorithms 
(e.g.,  multiple  regression  equations)  are  developed  to  relate  model-computed  quan-
tities and observational data for which sufficiently long records exist. The predictors 
can  be  either  pre-selected  or  screened.  This  approach,  which  generally  requires  a 
priori  knowledge  of  a  system’s  behavior  in  order  to  select  candidate  predictors,  has 
been used successfully in weather prediction, where the term ‘‘Model Output Statis-
tics  (MOS)’’  describes  the  products.  The  predictor  fields  can  be  model  counterparts 
of  the  desired  quantity  (i.e.,  a  model’s  grid-cell  temperature  can  be  used  as  a  pre-
dictor of temperature at a specific location, e.g., a weather station), or the predictors 
can include other model variables such as wind, humidity and cloud cover from the 
target location’s grid cell and/or from upstream grid cells. This approach has signifi-
cant  potential  to  meet  user  needs  for  site-specific  scenario  information,  but  it  has 
not been applied extensively in Alaska. 

The  reduction  of  the  uncertainty  in  climate  projections  from  global  models  is  es-
sential  for  the  validity  of  applications  such  as  downscaling,  whether  dynamical  or 
statistical.  While  global  models  are  improving  over  time  (Reichler  and  Kim,  2008), 
a promising area for advancement is the selection of subsets of models that are most 
credible for the application at hand. In the case of Alaskan climate simulations, sev-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00057 Fmt 6633 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

.

s
p
e
2
H
S
L
A
W
8
0
5

54 

eral global climate models used in the IPCC Fourth Assessment capture the present 
climate  (including  its  seasonal  cycle)  more  successfully  than  other  models.  Prelimi-
nary  studies  indicate  that  a  composite  over  a  subset  of  the  best  5–7  models  (out  of 
the  total  of  20–25  available  models)  provides  the  greatest  skill  in  simulations  of 
Alaska, the Arctic and the Northern Hemisphere. These models tend to project larg-
er  changes  of  temperature  and  precipitation  over  Alaska  for  the  remainder  of  the 
21st  century.  In  this  respect,  selection  of  models  based  on  quantitative  metrics  of 
performance  can  reduce  the  uncertainty  of  future  climate  projections.  Such  activity 
should be a high priority for user services provided by the climate modeling commu-
nity. 

A high priority in climate research is the tailoring of model output to include vari-
ables and information most relevant to the needs of planners and stakeholders. The 
variables carried by climate models are not always the ones that correspond to user 
needs,  which  can  include  (for  example)  the  firmness  of  the  ground  for  overland 
transportation;  snow  cover  characteristics;  vegetative  dryness  during  fire  season, 
etc. A recent illustration of such needs is the attempt by P. Larsen (Nature Conser-
vancy) to estimate the economic risks to public infrastructure in Alaska as a result 
of climate change in the coming decades. While global model uncertainties limit the 
robustness  of  such  estimates,  an  even  greater  limitation  is  the  availability  of  vari-
ables beyond temperature and precipitation. Infrastructure such as roads and build-
ings  will  clearly  be  affected  by  changes  in  the  freeze-thaw  cycles,  snow  loads,  tem-
perature  extremes,  peak-wind  events  and  occurrences  of  flooding.  There  has  been 
little effort to translate model output for Alaska into these quantities that are most 
relevant  to  infrastructure  risks  as  well  as  to  other  concerns  of  users.  The  bridging 
of models and user needs is an emerging area of activity, and it is intertwined with 
the  need  for  site-specific  (downscaled)  climate  projections  and  for  reduced  uncer-
tainty in climate model output. 
References 

Daly, C., M. Halbleib, J.I. Smith, W.P. Gibson, M.K. Doggett, G.H. Taylor, J. Curtis, and P.A. 
Pasteris, 2008. Physiographically-sensitive mapping of temperature and precipitation across the 
conterminous United States. International Journal of Climatology, DOI: 10.1002/joc.1688. 

Euskirchen,  E.S.,  A.D.  McGuire,  and  F.S.  Chapin  III.  2007.  Energy  feedbacks  of  northern 
high-latitude  ecosystems  to  the  climate  system  due  to  reduced  snow  cover  during  20th  century 
warming. Global Change Biology 13, 2425–2438. 

IPCC, 2007. The Physical Basis of Climate Change. Working Group I, Fourth Assessment Re-
port  of  the  Intergovernmental  Panel  on  Climate  Change,  Cambridge  University  Press,  Cam-
bridge, U.K., 996 pp. 

Reichler,  T.,  and  J.  Kim,  2008.  How  well  do  coupled  models  simulate  climate?  Bulletin  of  the 

American Meteorological Society, 89, 303–311. 

Stroeve, J., M.M. Holland, W. Meier, T. Scambos and M. Serreze, 2007. Arctic sea ice decline: 

Faster than forecast. Geophysical Research Letters, 34, L09501, doi:10.1029/2007GL029703. 

Senator KERRY. Thank you very much, Dr. Walsh. 
It  was  very  helpful,  all  of  you,  and  I  might  add  here  and  there 
a  little  baffling  and  confusing  as  to  how  we  make  layman’s  sense 
out of what we need versus what your hopes are and requests are. 
I  gather,  in  listening  to  you,  that  the  principal  obstacle  that  we 
have  got  is  a  human  resource  pressure.  We  have  this  whole  issue 
of  high-end  computing,  adequate  access  to  it,  which  we  just  heard 
about.  The  appropriateness  of  software,  software  standards,  proto-
cols,  et  cetera,  and  what  they  are  going  to  be.  More  observational 
data,  which  you  have  all  said  we  have  got  to  have,  and  obviously, 
adequate research funding. 

What  I  am  trying  to  figure  out,  as  I  listen  to  this,  and  perhaps 
you  can  help  us  understand.  Give  it  to  us  in  relative  terms.  We 
have now had several rounds of the IPCC. We are gearing up glob-
ally to make certain decisions. Now, obviously, Dr. MacDonald, you 
would like resolution, and Dr. Walsh you are talking about that in 
terms of the ability to be able to really predict something for a com-
munity. 

To  what  degree  are  we  able  now  to  adequately  predict  in  a  way 
that allows public policymakers to make a smart decision? Is there 
an  element  of  guesswork  in  this  now?  Is  there  a  sufficient  level  of 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00058 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

55 

accuracy that you can make some predictions, and then we need to 
go  further  to  make  the  others?  Who  wants  to  tackle  that?  Dr. 
Sarachik? 

Dr. SARACHIK. I think right now we are able to make certain pre-
dictions. We are able to say certain things. For example, there was 
a recent paper by a group out of Scripps talking about, as time goes 
on, the snowpack will decrease. Those are firm results because they 
are  based  on  science.  As  it  gets  warmer,  you  have  less  ice,  and 
therefore  you  have  less  available  water  for  irrigation  and  various 
other things because the snowpack serves as reservoirs. 

On the other hand, we cannot make very fine-scale decisions be-
cause  we  do  not  have  very  fine-scale  information.  And  a  lot  of  our 
people  want  to  do  things  at  the  watershed  level.  Puget  Sound,  for 
example,  an  estuary,  is  a  climate  regime  in  itself  that  has  a  lot  of 
very  unique  problems  that  simply  cannot  be  dealt  with  until  we 
can make predictions of variability and actual climate for the com-
ing year on that scale, and we cannot do that right now. 

Senator KERRY. And to do that, what will it take? 
Dr.  SARACHIK.  I  think  it  takes  a  balanced  program  of—sorry  to 
say  this—observations,  modeling,  and  research  because  I  do  not 
think  one  of  them  can  be  allowed  to  get  ahead  of  the  others.  They 
are  all  necessary  and  you  cannot  make  progress  without  doing 
them all. 

Senator KERRY. Who will define the balance? 
Dr.  SARACHIK.  I  think  you  can  define  the  balance  by  asking  the 
National Academy to define that balance. So far, nobody has really 
thought  about  what  the  progression  is  going  to  be,  and  at  the  mo-
ment  everybody  is  arguing  for  his  own  little  specialty,  but  nobody 
is arguing for everything going together. And that is what is really 
necessary. 

Senator  KERRY.  The  answer  to  that  means  you  have  got  to  have 

resources in every sector. That is a resource-based request. 

Dr. SARACHIK. I think it is a resource-based request and it is an 
organizational  problem  for  Government  because  government  is  not 
presently  organized  to  do  this  very  well.  There  have  been  con-
tinuing  complaints  about  the  United  States  Global  Change  Re-
search  Program,  for  example,  which  is  not  able  to  focus  money  on 
problems  because  agencies  have  different  needs  and  requirements. 
Therefore, you cannot really solve some major problems. 

One  of  the  outstanding  problems  has  been  decadal  variability. 
We  do  not  have  a  program  on  decadal  variability  despite  its  being 
recommended  for  a  very  long  time.  It  is  absolutely  crucial  for  both 
understanding ENSO and for understanding global warming. 

Senator KERRY. For understanding what first? 
Dr. SARACHIK. Global warming and El Nin˜ o Southern Oscillation. 
Senator  KERRY.  Can  you  give  us  the  kind  of  dollar-to-result  con-

nection, or is that completely speculative? 

Dr. SARACHIK. We have actually done this in various committees 
of  the  National  Academy  that  I  have  served  on,  and  the  answer  is 
if we want the national climate service which will give us the larg-
est scale of information correctly, the observations, modeling on the 
large scale that will interact with offices on a small scale, it would 
be on the order of $1 billion a year. 

Senator KERRY. $1 billion a year. Where are we now? 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00059 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

56 

Dr.  SARACHIK.  It  is  a  little  hard  to  say  because  each  agency  de-
fines  climate  in  different  ways  and,  therefore,  cannot  bring  re-
sources to bear on problems. 

Senator KERRY. With respect to that, anybody else jump in when 

you want to here. I am not just targeting one person. 

Does that mean that we need to pull the effort of all these agen-
cies  under  one  roof?  Would  that  serve  the  agency  interests  ade-
quately if we did that? 

Dr.  SARACHIK.  Or  within  a  single  agency.  I  do  not  know  of  any 
weather service which is not a single agency throughout the world. 
There are hundreds of weather services throughout the world, and 
they  are  all  separate  agencies  I  would  think.  A  climate  service 
should  be  either  a  separate  agency  or  within  a  single  agency  in 
order to be able to accomplish a single goal. 

Senator KERRY. Dr. Reed, do you want to weigh in? 
Dr. REED. Just to echo that comment. That challenge is mirrored 
on  the  side  of  computing  research  and  infrastructure  as  well.  It  is 
scattered  across  many  agencies.  There  is  a  loose  confederation  of 
programs,  and  as  I  alluded  in  my  testimony,  one  of  the  perpetual 
recommendations of the community is tighter coordination of those 
activities  to  focus  on  the  underlying  R&D  that  enables  the  com-
putational  science,  of  which  climate  change  is  one  example,  but 
also  the  procurement  process  for  open  scientific  research  facilities 
that support problems like this. 

Senator  KERRY.  How  far  are  we,  in  your  judgment,  on  the  cur-
rent  scale  of  what  we  are  putting  in  and  the  current  rate  of  pro-
gression  in  modeling?  The  modeling,  you  would  all  agree,  is  better 
today  than  it  was  5  years  ago,  and  that  is  better  than  it  was  10 
years ago. So we are making some progress. We are not making all 
the  progress  you  would  like  to  make.  Is  the  progress  we  are  mak-
ing  sufficient  to  responsively  address  the  concerns  of  people  like 
Mr.  Carlisle,  Dr.  Walsh,  and  others  who  are  trying  to  make  deci-
sions  at  a  local  level?  Or  is  there  a  correlation  here  between  the 
amount  of  money,  energy,  effort,  leadership  that  ought  to  go  in  to 
accelerating  the  supercomputing  capacity  and  the  other  things  so 
that we are getting better real-time results? 

Does  anybody  want  to  take  it?  Yes,  Dr.  MacDonald.  You  can  all 

have a shot at it. 

Dr. MACDONALD. Senator Kerry, our scientists at GFDL feel that 
we  do  need  a  balanced  program,  but  we  are  at  a  point  where  sig-
nificantly  higher  resolution  models  really  would  give  us  much  bet-
ter regional information, and they made that point that it was not 
just  that  they  thought  that,  that  they  had  been  able  to  test  that 
concept,  instead  of  running  at  130  miles,  running  at  like  30-  and 
40-mile resolution. So they tested that out, and that is where they 
got this figure that you see. 

Senator KERRY. What does it take us to get there? 
Dr.  MACDONALD.  The  increase  by  a  factor  of  four  in  resolution 
with  better  physics  takes  something  like  100  times  as  much  proc-
essing as today. 

I want to add one more point. 
Senator  KERRY.  What  is  the  limitation  on  that?  Is  it  just  the 

commitment to the funding or is there a technical limitation? 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00060 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

57 

Dr.  MACDONALD.  I  do  not  think  there  is  a  large  technical  prob-
lem  since  they  have  proved  that  they  are  able  to  do  it.  So  I  think 
it is just access to the computing. 

And one more point. 
Senator  KERRY.  Do  you  agree  with  the  figure  that  Dr.  Sarachik 

gave us, $1 billion a year? 

Dr. MACDONALD. I guess to understand that figure, I would have 
to know what all do we include, whether we include all of our sat-
ellite systems and so on. So I cannot comment on it. 

But I did want to make an additional point that part of the rea-
son I think we can advance rapidly in climate modeling is that we 
did  the  same  thing  in  weather  models.  When  I  started  my  career, 
they were really poor on precipitation and we got higher and high-
er resolution. They got better and better. I guess some would argue 
with that, but we think they are a lot better. 

Senator KERRY. I saw another hand. Yes, Dr. Walsh. 
Dr. WALSH. I would like to pick up on this analogy to the weath-
er  models.  I  realize  weather  and  climate  prediction  are  certainly 
different  beasts,  but  I  think  that  there  is  a  lesson  we  can  learn 
from  the  weather  prediction  community  that  may  help  us  accel-
erate  progress  toward  meeting  the  user  needs.  That  is  the  statis-
tical  adjustment  of  model  output.  The  adjustments  are  made  for 
specific  locations  based  on  algorithms  that  have  been  developed 
using  observational  data.  Now,  that  type  of  approach  shortcuts  to 
some extent the slow progress in model resolution and in model ca-
pabilities. 

But  there  are  two  requirements  there.  One  is  for  an  integrated 
observational  system  that  is  truly  integrated  with  what  the  mod-
eling community is doing, and the second is it requires some coordi-
nation, some organization in the national-level program, whether it 
is through multi-agency or not. It is not going to happen piecemeal. 

Senator KERRY. Senator Stevens? 
Senator  STEVENS.  Well,  thank  you  very  much.  I  woke  up  this 
morning  and  thought  about  this  problem  that  just  hit  Myanmar, 
whatever we call it now. 

Senator KERRY. Burma. 
Senator  STEVENS.  Do  you  remember,  Dr.  Walsh,  we  had  a  ty-
phoon  off  of  the  northern  coast  of  Alaska?  We  had  cyclones,  hurri-
canes.  We  seem  to  be  unable  to  predict  these  things,  and  I  think 
the  total  damage  that  comes  from  not  being  able  to  predict  them 
are fairly obvious to everyone. 

Have we used space enough in terms of getting that data for you 
all  to  use  in  your  computers?  Goldwater  used  to  believe  we  could 
get  a  lot  of  information  by  just  observing  what  is  going  on  on  the 
globe as a whole rather than just one spot. Have we ever proceeded 
on any of this? 

Dr.  WALSH.  The  satellite  information  seems  to  have  been  most 
useful  in  the  weather  prediction  arena.  In  the  sense  that  climate 
includes  the  statistics  of  weather  over  time,  I  think  we  may  have 
more  of  a  challenge  in  incorporating  the  satellite  information  into 
an enhancement of the climate models. I see the payoff more in the 
weather prediction arena. 

Senator STEVENS. Dr. MacDonald, my staff thinks I ought to ask 
you  the  question  of  whether  the  budget  this  year  will  keep  NOAA 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00061 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

58 

on  track  in  obtaining  supercomputing  resources  for  improved  cli-
mate modeling. Do you have enough money? 

Dr.  MACDONALD.  Senator  Stevens,  we  have  been  able  to  get  the 
support  that  we  need,  and  I  think  our  climate  modeling  is  going 
quite well. And what we are talking about here is kind of the next 
big jump up in the 5 to 10 year timeframe. 

Senator STEVENS. This is not the place to get into it. I have been 
worried  about  the  predictions  that  many  people  are  relying  on  in 
terms  of  some  of  these,  for  instance,  the  IPCC  because  of  what 
went into their computers. Do we have enough reliable information 
about the past to really feed the stuff into the computers as we are 
doing now? 

Dr. MACDONALD. I think within NOAA and elsewhere, there are 
a lot of programs where we do try and look at not only the last 100 
years  but  the  last  1,000  years.  It  is  tough  work.  You  are  drilling 
into  glaciers  and  trying  to  see  what  it  was.  And  that  has  taught 
us a lot about our models. I think if we look objectively, our models 
have  improved  greatly  from  the  1990s  and  I  think  it  is  partly  be-
cause  we  were  able  to  look  at  the  past  and  see  what  it  was.  So  I 
am kind of an optimist to think we are making great progress, but 
the  need  for  understanding  climate  is  so  great,  that  we  are  really 
looking for trying to get much better. 

Senator KERRY. Can I just extrapolate on that? What is the level 
of  accuracy  about  the  longer-term  predictions  of  consequences  of 
climate  change?  I  think  that  is  part  of  what  Senator  Stevens  was 
asking.  Has  enough  data  gone  in  here  that  is  good  data  to  be  able 
to  say  the  sea  level  rise  is  accurate,  that  the  vegetation  migration 
is accurate, these expectations that we are now factoring in? 

Dr.  SARACHIK.  I  think  you  can  say  that  on  a  large  enough  space 
scale.  The  problem  is  that  that  is  very  good  for  the  IPCC,  which 
is interested in mitigation. To simplify, mitigation is global, but ad-
aptation is local. We do not have that sort of information on a local 
scale. 

Senator KERRY. You just do not know where that is going to hap-

pen, but you know it is going to happen. 

Dr. SARACHIK. It is not clear to me it is going to happen because 
I  think  it  depends  on  the  health  of  the  climate  community.  I  work 
in  the  trenches,  or  I  used  to.  I  have  had  10  students  getting 
Ph.D.’s, five of whom are no longer in the field because there were 
no opportunities for them in the field. This is a field which is sim-
ply not providing enough opportunities because there is not enough 
money. I do not know what it is like for government organizations, 
but  the  money  is  not  working  its  way  down  to  the  universities 
where  a  lot  of  this  work  is  done  and  a  lot  of  people  are  trained. 
Senator  STEVENS.  Dr.  Sarachik,  again  a  question.  I  am  told  the 
NOAA  research  program  has  been  active  for  more  than  a  decade. 
That  program  in  Alaska  was  started  in  2006.  What  are  the  chal-
lenges in translating scientific research and complex modeling, par-
ticularly  the  data,  information  that  can  be  used  by  people,  by  just 
ordinary citizens? 

Dr.  SARACHIK.  We  deal  with  that  pretty  much  every  day  in  my 
center,  and  they  need  information  that  is  translated  into  resource 
predictions. We, for example, use whatever climate information we 
can  get  from  the  models.  We  correct  the  models  as  best  we  can.  It 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00062 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

59 

is  not  a  very  well  defined  procedure.  Then  we  make  hydrological 
predictions  for  things  like  stream  flow,  and  from  stream  flow,  we 
can make energy predictions, because most of our energy is hydro-
power,  and  salmon  predictions  and  water  availability  predictions, 
irrigation, and agricultural predictions. That is the sort of thing we 
need  to  do.  It  is  not  being  done  well  enough  because  the  global 
models  are  not  good  enough,  and  it  is  not  being  done  in  enough 
places in the United States. 

Senator  STEVENS.  Well,  I  guess  I  could  not  get  too  specific.  And 
Dr. Walsh might correct me on this if I am wrong, but it is my un-
derstanding that few of the computers that are producing informa-
tion that looks at the Arctic for the future have taken into account 
the  vast  amount  of  Atlantic  water  that  has  gone  into  the  Arctic 
Ocean  at  a  fairly  deep  level  and  that  that  water  has  been  very 
warm  and  that  the  thawing  in  our  area  has  been  from  the  bottom 
up, not from the top down, not from warming on top, but from the 
warm water that is coming from the Atlantic Oscillation and bring-
ing  up  more  warm  water  than  ever  before.  And  theoretically,  it 
may  stop  at  any  time.  It  may  reverse  and  go  back  to  its  normal 
pattern. 

How  do  computers  figure  that  in?  The  net  result  of  the  com-
puters  today  say  ice  is  going  to  disappear  in  our  Arctic  Ocean  by 
2020, 2030. Our people dispute that. As I understand it, we believe 
it is thinning and we are going to lose summer ice, but we are not 
going  to  be  ice-free  as  these  computers  predict.  How  do  we  really 
get any balance in terms of the public information as to the results 
of  something  like  these  computers  that  are  fed  one  basic  group  of 
statistics and other statistics that are not made available to them? 
Dr.  SARACHIK.  I  think  all  of  the  models  currently  being  done  for 
the  IPCC  process  do  include  deep  water  coming  up,  thermohaline 
circulation,  if  you  will.  The  fact  that  the  models  do  not  agree 
among  themselves  is  an  indication  that  we  are  a  long  way  from 
making reasonable predictions. 

The policy issue is, how much do you need in order to make deci-
sions?  How  much  do  you  need  to  know  about  the  future?  And  the 
future  is  always  murky.  The  more  we  know  about  the  future,  the 
better those decisions will be. 

My attitude toward models is it tells us the range of things that 
could  happen.  It  does  not  necessarily  tell  us  what  would  happen. 
And that is the best we can do at the moment, but I think we can 
do better. 

Senator STEVENS. I thank you. 
John,  what  coordination  now  exists  between  our  International 
Arctic Research Center and those entities that are producing these 
global climate models that we are hearing so much about? Are they 
really  feeding  in  some  of  the  information  that  you  have  gathered 
through  the  International  Arctic  Research  Center  now  for  over  20 
years? 

Dr.  WALSH.  Well,  I  think  you  touched  on  a  good  example  with 
the inflow of Atlantic water into the Arctic Ocean. As Dr. Sarachik 
mentioned, there is a wide range among models and how they sim-
ulate that inflow. What we need is a good observational assessment 
to pin down which models are doing things right for the right rea-
son. So I think what we are pointing to here is the need for, again, 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00063 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

60 

a  coordination  between  the  observations  and  the  models.  In  this 
case, it is the model assessment side of the modeling enterprise. 

Senator  STEVENS.  I  do  not  want  to  offend  my  friend  here.  But 
around  here  if  I  criticize  IPCC,  I  am  criticizing  motherhood.  And 
yet, I think that their models are deficient in terms of the informa-
tion  base  that  has  been  made  available  to  us  in  the  Arctic.  Am  I 
wrong? 

Dr.  WALSH.  There  was  a  polar  chapter  in  the  IPCC  assessment. 
But you are right that it contained very little use of the model out-
put and very little critical assessment of the models. 

Senator  STEVENS.  And  now  we  face  the  problem  of  having  the 
polar  bear  declared  endangered  because  its  habitat  may  be  af-
fected,  and  that  question  of  whether  the  habitat  is  going  to  be  af-
fected comes from this IPCC model that was deficient to start with, 
as far as I am concerned. 

Now,  I  do  not  want  to  put  this  on  my  friend  from  Alaska.  But 
what do you do about this, Dr. Sarachik? How do we find some way 
where we can obtain models that the public as a whole can rely on 
without the hype that comes from something like IPCC? We do not 
have  hype  with  the  Alaska  models,  but  they  have  been  financed, 
by the way, by the Federal Government for 20 years. 

Dr.  SARACHIK.  We  will  never  have  perfect  information  about  the 
future.  There  will  always  be  an  uncertainty  in  the  policy  decisions 
that need to be made. 

What  we  now  know  is  that  there  is  a  possibility  of  large  ice  de-
pletion  in  the  Arctic.  We  do  not  know  how  much  it  is  going  to  be. 
We  have  seen  one  example  of  complete  melting  of  summer  ice  in 
the opening of the Northern Passage. Nobody would have expected 
that  25  years  ago,  but  some  of  the  models,  in  fact,  have  predicted 
that, but some of them have not. 

Senator  STEVENS.  Would  it  surprise  you  to  know  that  we  know 
in  history  that  it  has  been  open  before  for  substantial  periods  of 
time? 

Dr.  SARACHIK.  If  you  go  back  in  the  geological  record,  yes,  of 

course. 

800 years. 

of fact. 

Senator  STEVENS.  I  am  talking  about  in  recent  history,  the  last 

Dr.  SARACHIK.  I  do  not  believe  that  there  is  firm  evidence  that 

the Northern Passage has been open during that time. 

Senator  STEVENS.  Well,  it  was  open  several  times,  as  a  matter 

Senator KERRY. Where is that documented? 
Dr.  SARACHIK.  The  ability  to  go  across  the  Arctic  Ocean  in  the 
summer  which  would  shorten  the  distance  between  Asia  and  Eu-
rope considerably and the fact that it was open—— 

Senator  STEVENS.  The  question  is  what  is  open  and  how  long  it 
has  to  be  open  in  order  to  be  classified  as  being  open.  But  very 
clearly, there have been periods of time when people could go from 
the Atlantic to the Pacific across the top of this continent. 

And  now  it  is  being  predicted  that  it  will  be  open  for  a  period 
of time, a substantial period of time. Many people believe that will 
be year-round. I am told it will not be year-round. It might be open 
for a period of time in the summertime, but winter ice is not going 
to be gone. Would you disagree with that? 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00064 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

61 

Dr. SARACHIK. I would plead ignorance because some models say 

that it will and some models say that it will not. 

I  think  one  of  the  things  we  should  recognize  is  that  a  lot  of— 
I  know  this  is  on  the  one  hand  and  on  the  other  hand,  but  pre-
dictions are not made by models. Predictions are made by the emis-
sions  that  go  into  the  models.  So  the  models  simply  give  you  the 
response  to  those  predictions.  If  in  fact  we  emit  more  greenhouse 
gases, more CO2, then the climate will be warmer and a lot of these 
things  will  happen  more.  The  models  simply  describe  the  response 
to the emissions of the various greenhouse gases. 

Senator  STEVENS.  Mr.  Chairman,  we  mentioned  this  question  of 
earmarks for the last 4 years. Through earmarks we have kept four 
vessels taking statistics on the Arctic Ocean for a period of time in 
the  summertime.  I  do  not  know  how  long.  We  thought  that  would 
disappear  because  of  the  inability  to  get  the  earmark,  but  thank-
fully  NOAA  has  agreed  now  to  finance  the  same  concept  and  keep 
it going so we get reliable predictions over a period of time of what 
the  actual  changes  are  in  the  Arctic  Ocean.  So  I  look  forward  to 
having  accurate  observational  statistics  coming  at  us  now  in  this 
period ahead of us, and I hope we can do that in places where there 
are  areas  of  real  controversy  like  what  is  going  to  happen  in  the 
Arctic  because  those  vessels,  being  from  various  nations,  are  col-
lecting  the  same  type  of  data  throughout  the  Arctic  Ocean,  which 
is not just a little pond. It is an enormous place. If we can get that 
information  and  feed  it  into  the  computers,  we  are  liable  to  start 
getting  some  accurate  predictions,  Doctor.  So  I  agree  with  you. 
Stuff in and stuff out. So we want to put the right stuff in. 

Thank you. 
Senator KERRY. Can I say to my friend from Alaska—I just want 
to  pick  up  on  this—I  think  the  key  of  what  Dr.  Sarachik  just  said 
is  you  have  to  look  at  what  the  input  is  to  whatever  the  model  is 
that  you  are  looking  at.  And  he  said  that  if  emissions  continue  to 
go up, it will get warmer. 

Now,  on  the  current  track  that  we  are  on,  emissions  are  abso-
lutely  guaranteed  to  go  up  at  an  alarming  rate.  Is  that  correct? 
Does anybody disagree? Good. And if they go up—— 

Senator STEVENS. You are talking about CO2. 
Senator  KERRY.  I  am  talking  about  CO2.  I  am  talking  about  all 
greenhouse gases. Greenhouse gases are going to go up. If we reach 
600 to 900 parts per million, which the current rate of China’s and 
India’s  and  our  own  pulverized  coal  powerplant  production  levels 
are,  the  ice  is  going  to  continue  to  melt.  And  then  the  polar  bear 
is going to be threatened. 

So it is a question of your input. You and others have to look at 
the  input  and  make  a  public  policy  judgment  as  a  person  whether 
you find it accurate and concerning or not. 

I  do  not  disagree.  I  have  always  said  this,  that  there  is  a  level 
of  inexactitude  in  the  modeling.  We  cannot  tell  you  exactly  what 
is  going  to  happen  in  a  lot  of  different  places,  but  we  get  a  big 
enough  picture,  do  we  not,  gentlemen,  that  gives  you  pretty  good 
indications  of  trend  lines,  which  as  a  matter  of  public  policy  indi-
cates you better take notice or not take notice? 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00065 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

62 

I mean, you have seen these transitions in Alaska. We are spend-
ing, what is it? $100 million and some to move a village. Your per-
mafrost is melting, is it not? 

Senator  STEVENS.  We  would  like  to  have  that  $100  million, 

though. We need $100 million. 

Committee. 

Senator KERRY. And you, sir, have about as much ability as any-

body here in the Senate to make sure it will happen. 

[Laughter.] 
Senator  KERRY.  That  is  why  I  like  sitting  by  you  here  in  this 

Anyway, the point is made that I think we know we want to try 
to bring this down to a greater level of exactitude, and the question 
I  am  trying  to  get  at  is  how  rapidly  can  we  do  that,  at  what  kind 
of  expense.  I  think  it  is  important  that  you  have  said  we  have  to 
do  this  with  much  greater  coordination.  We  have  got  to  coordinate 
more effectively, and we have got to look in this committee at how 
you do that. We have to look at the question of whether or not you 
bring  this  under  one  roof.  Correct?  We  need  to  get  the  National 
Academy perhaps involved in how we can best do this is what I am 
trying to glean out of this. 

What  else  do  we  need  to  do  as  a  committee  and  as  a  Congress 

to try to get us on the right track here as fast as we can? 

Dr.  SARACHIK.  I  have  served  on  a  lot  of  Academy  committees 
which  have  talked  precisely  about  this  problem,  and  this  has  been 
over a course of the last 15 years I would say. 

Senator KERRY. So are the studies already there? 
Dr.  SARACHIK.  A  lot  of  the  studies  are  there,  yes.  In  particular, 
there  was  a  study  called  Pathways  which  was  done  in  the  late 
1990s.  I  served  on  that  committee.  And  it  described  the  balanced 
approach  to  things,  and  it  also  objected  to  the  way  that  research 
was  currently  being  carried  out  in  the  United  States  by  the 
USGCRP, the U.S. Global Change Research Program. 

I think climate science has made some tremendous advances. We 
now know that there only seem to be three major phenomena that 
we have to explain, El Nin˜ o, Pacific Decadal Oscillation, and North 
Atlantic  Oscillation.  If  we  can  do  that,  we  can  get  a  large  amount 
of the predictable part of climate in the future. 

I do not know of any programs in the United States which actu-
ally  concentrate  on  that.  I  have  been  working  on  El  Nin˜ o  for  25 
years, and at this moment, I do not know where I would apply for 
money in order to study that problem. 

Senator STEVENS. You have come to the right place because I will 
sure help you if you could find some way to get a program that we 
could finance that would make some sense. 

Dr.  SARACHIK.  I  have  made  recommendations  and  the  Academy 
has made recommendations. For example, I worked on a committee 
about decadal variability. The idea is that the basic problem in pre-
dicting  El  Nin˜ o  was  our  inability  to  understand  its  decadal  vari-
ations,  and  one  of  the  big  problems  of  global  warming  is  the  fact 
that  it  is  being  modulated  by  decadal  variability.  So  we  rec-
ommended  only  one  thing,  a  program  in  decadal  variability.  When 
we  presented  this  to  the  various  agencies,  they  said  we  cannot  do 
it.  So  they  did  not  get  together  and  form  an  initiative,  which  I 
would have hoped and expected that they would do. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00066 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

63 

fell off. 

Senator STEVENS. I am serious, Mr. Chairman. I wish you would 
really  give  Congress  some  recommendations  along  that  line.  In 
spite  of  the  earmarks,  I  think  it  is  high  time  we  understood  both 
the  Atlantic  Oscillation  and  the  Pacific  El  Nin˜ o  concepts  and  try 
to  understand  why  they  apparently  are  not  there  in  the  southern 
hemisphere.  At  least,  I  have  not  seen  any  sign  of  them  having  a 
reciprocal effect on, I think, the South Pole. 
Dr. SARACHIK. Oh, you mean why the—— 
Senator STEVENS. Why was not the southern hemisphere affected 
the  same  way?  If  you  go  to  the  South  Pole,  you  will  find  the  ice 
has been piling up there for 40 years and not melting at all. 

Dr.  SARACHIK.  —the  basic  reason  for  that  is  that  there  is  a  cir-
cumpolar current in the South Pole which goes all the way around, 
which  allows  water  to  come  up  from  the  deep.  That  water  is  ex-
tremely cold and will stay cold for a very, very long time. 

Senator  KERRY.  But  I  understand  there  was  a  very  significant 

breach in the ice in the Antarctic just recently. 

Senator  STEVENS.  That  was  because  of  the  weight  of  the  ice.  It 

Dr. SARACHIK. There is melting of the Antarctic continent, but in 
general, the predictions are that the Arctic will melt far more than 
the Antarctic basically because cold water will come up in the Ant-
arctic which does not necessarily come up in the Arctic. 

Senator  STEVENS.  Well,  why  do  you  not  present  us  your  rec-
ommendations?  Maybe  we  can  find  some  bipartisan  way  to  get 
around  this  problem  of  no  earmarks.  I  think  that  is  the  most  sig-
nificant thing that has come out of this. There really is not enough 
information  to  know  about  these  oscillations  and  what  it  does  to 
the North American continent. 

Dr. SARACHIK. Correct. 
Senator STEVENS. And I would like to join in demanding that the 

money be made available to do so. 

Senator  KERRY.  Would  it  have  just  fallen  off  if  it  stayed  colder? 

That is OK. 

Dr. Hack, a quick question. Is your center over-subscribed? 
Dr. HACK. We are fully subscribed. 
Senator  KERRY.  Fully  subscribed.  And  how  do  you  prioritize  and 

allocate the time for the computers? 

Dr.  HACK.  Right  now,  all  of  the  time  at  the  center  is  allocated 
through  a  program  called  INCITE.  It  is  a  program  that  is  open  to 
all comers. 

Senator KERRY. How much is allocated toward climate use? 
Dr.  MACDONALD.  Fourteen  percent  of  the  total  cycles  are  allo-

cated to—— 

Senator  KERRY.  If  I  could  interrupt,  let  me  just  say,  before  Sen-
ator Stevens goes, if you could get the Committee in the next days 
your specific thoughts about how we address Senator Stevens’ con-
cern, we will go to work and see what we can do here and we will 
leave the record open. Fair enough? 

Senator  STEVENS.  Thank  you.  I  am  sorry  I  have  to  leave.  Thank 

you very much. 

Senator KERRY. I have to leave in about 5 minutes, folks. I have 

a foreign guest coming in. So I need to get up to that meeting. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00067 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

64 

Dr.  HACK.  The  center  has  had  a  very  long  history  with  the  cli-
mate  community  back  in  the  IPCC  days  when  the  AR4  computa-
tions were being done. 

Senator  KERRY.  But  it  is  competing.  It  is  competing  with  these 

What I am getting at is, do we need a climate-specific supercom-

other interests. Right? 

puter center? 

Dr. HACK. It would be a tremendous asset to the climate commu-

nity to have something like that I think. 
Senator KERRY. What would that cost? 
Dr.  HACK.  If  you  are  talking  something  on  the  order  of  a 
petascale  type  of  center,  we  are  probably  talking  between  $50  mil-
lion and $100 million. 

Senator KERRY. Where would be the preferred place of siting that 

other than Massachusetts? 

[Laughter.] 
Dr. HACK. Well, Oak Ridge would make a nice place. 
[Laughter.] 
Dr. HACK. But I think the main thing is to see—I think the hard-
er thing is to try and coordinate this as an interagency question so 
that  the  agencies  are  all  on  board  and  one  could  tailor  the  needs 
of a center like that to meet all the disparate needs of the different 
agencies that would be running on the computer system. 

I  just  wanted  to  follow  up  on  a  couple  of  things  that  have  come 
up.  And  that  is  that  I  think  a  lot  of  the  questions,  when  we  are 
asking about prediction and what is going to happen in the future, 
really come down to uncertainty in the modeling frameworks. How 
certain are the forecasts? And there is no one single thing you can 
put your finger on that is going to tell you why they are uncertain. 
Certainly  resolution  plays  a  very  large  role,  as  Dr.  MacDonald 
showed.  We  have  done  our  own  experiments  with  resolution  to  il-
lustrate  the  same  sort  of  thing.  You  cannot  capture,  for  example, 
orographic  precipitation  accurately  with  a  very  course  model.  That 
is a very simple thing. 

For  things  like  ice,  say,  sea  ice  in  the  Arctic,  the  processes  that 
are  embodied  in  these  models,  the  mathematical  representations 
are approximations, and we improve those approximations through 
the  observations.  And  as  the  observations  get  better,  the  approxi-
mations get better and the models get better and the uncertainties 
are reduced. 

So  this  is  why  I  think  putting  one’s  finger  just  to  say  that  this 
one  magic  pill  that  will  solve  all  these  things—I  do  not  think  that 
is  the  right  way  to  look  at  it.  I  think  that  all  these  factors  are 
interrelated.  They  all  rely  on  one  another.  Computing  is  certainly 
as  important  as  the  investments  in  modeling  and  the  investments 
in the observational systems to help improve the models. 

Senator  KERRY.  I  guess  with  any  of  these  models  at  some  point 
you  have  to  be  willing  to  just  draw  a  line  and  dismiss  the 
imponderables, I assume, like the sunspot argument or dust storms 
or  the  Gulf  Stream  shuts  down  and  all  of  a  sudden  that  are 
unpredicted.  There  is  a  point,  is  there  not,  where  you  are  able  to 
take  all  of  the  potential  variables  that  people  can  conjure  up  and 
adequately address them? Or is there just ultimately a level of im-
ponderability here? 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00068 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

65 

sense—— 

Dr.  HACK.  I  think  there  are  gaps  in  what  we  understand  about 
the  climate  system.  I  think  it  is  encouraging  that  the  models  are 
as  good  as  they  are  at  their  ability  at  least  on  global  scales  to  re-
produce the observed record. 

Senator  KERRY.  And  the  key  is  just  really  to  look  at  what  is 
going into it, is it not? You then decide, hey, what is the probability 
of this in a sense—— 

Dr.  HACK.  As  far  as  we  are  going  to  be  able  to  do  is  to  give  a 

Senator  KERRY.—and  take  the  data  on  that.  We  know  there  is 
going to be X amount of powerplants in China, X amount. We know 
there  is  going  to  be  X  amount  of  greenhouse  gas.  We  are  getting 
pretty good, I assume, at correlating the degrees, the Centigrade or 
Fahrenheit  degrees  of  the  warming  level  according  to  the  green-
house  gases.  As  for  forest  migration  or  CO2 in  the  ocean,  how  do 
you bring all those together? 

Dr. HACK. I am optimistic that these kinds of problems can—the 
noise, let us say, and the uncertainty can be driven down with—— 
Senator  KERRY.  How  long  will  it  take  us  to  get  there?  Because 

time is ticking on us. We have got some skeptics around still. 

Dr.  HACK.  I  think  with  a  focused  effort,  goals  are  achievable 

within the decade. 

Senator KERRY. Within the decade. 
Dr. HACK. And the other aspect of this is the stakeholder commu-
nity,  the  people  I  have  interacted  with.  For  example,  a  year  ago, 
I was in a meeting with western water judges who were looking at 
a  rule  on  water  rights  matters.  The  message  was  that  they  would 
much  rather  have  data  with  uncertainty  in  it  than  no  data  at  all. 
And  the  stakeholder  community  is  a  very  sophisticated,  intelligent 
community. They know how to use data that is not perfect, and as 
long  as  we  make  an  attempt  to  try  and  establish  what  the  error 
bounds  are  and  start  to  be  able  to  address  some  of  the  issues  Dr. 
Sarachik  talked  about  with  regard  to  low  frequency  variability  in 
the system, the answers that they are getting are going to be tight-
er  and  they  will  be  of  more  use  to  their  planning  with  regard  to 
infrastructure and resources, resource management. 

Mr. CARLISLE. Senator, I would like to echo that point, if I could. 
A decade is a long time for coastal communities who are faced with 
siting decisions every single day. So we are willing to accept a cer-
tain  level  of  uncertainty.  As  long  as  we  can  frame  it  and  base  it 
back  to  sound  science,  we  can  at  least  start  to  have  informed  con-
versations.  And  even  a  little  bit  of  information  helps.  So  the  sea 
level  rise,  with  all  the  uncertainties  around  it—at  least  we  can 
track  that  trajectory,  and  that  is  important.  We  can  start  to  build 
in freeboard. 

So  one  of  the  things  I  will  make  a  call  for  is  while  the  modeling 
at  the  global  scale  and  regionals  is  really  important,  we  still  can 
use  things  like  high  resolution  topographical  and  bathymetry  data 
and  we  can  get  a  lot  from  that  type  of  information.  So  these  are 
very,  very  important,  but  we  can  also  make  progress  while  we  are 
going  through  these  long-term  decadal  research  improvements  to 
make some progress on the ground. 

Dr.  HACK.  I  would  like  just  to  make  one  more  statement 

about—— 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00069 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

66 

Senator KERRY. You will have to do it quickly because I have got 

to wrap it up. 

Dr.  HACK.  That  is  that  the  IPCC  showed  very  clearly  that  the 
models  show  predictive  skill  on  subcontinental  scales,  certainly  on 
continental  scales.  So  the  issue  of  resolution  I  think  does  provide 
an  opportunity  for  a  rather  substantial  improvement  in  predictive 
skill  in  the  models  if  we  can  explore  that  part  of  parameter  space. 
It  is  just  too  expensive  and  this  is  where  the  whole  computational 
infrastructure  issue  comes  into  play.  With  dedicated  facilities, 
these  models  can  be  configured  to  at  least  explore  what  the  pre-
dictive  skill  of  the  models  would  be  if  you  were  able  to  run  them 
at  resolutions  that  are  more  typical  of  weather  prediction  models. 
Senator KERRY. Well, we will give you the chance in the next few 
days to get in to us what that best practice is going to be over the 
course  of  these  next  few  years,  as  I  will  leave  the  record  open.  We 
really welcome that. 

Last question, Dr. MacDonald, just quickly. You talked about the 
three-legged  stool,  the  increased  computation  measurement,  et 
cetera. Are each of those legs equal today? 

Dr. MACDONALD. I think that they are equal, and we are invest-
ing  in  them.  We  are  trying  to  get  the  climate  sensors  onto 
NPOESS.  We  are  trying  to  get  the  really  big  increases  in  com-
puting,  and  we  have  expeditions  up  to  the  Arctic  to  try  to  under-
stand  what  is  happening  with  the  ice.  So  they  are  equally  impor-
tant. 

Senator KERRY. And do we need to make an equal amount of ad-
vancement  in  each  of  them  in  order  to  get  this  level  of  predict-
ability we want, or is there one more than the other that we ought 
to be focused on? 

Dr.  MACDONALD.  No.  I  think  of  it  as  equal.  That  is  why  we  like 
using  the  example  of  the  stool.  You  cannot  have  one  leg  that  is  a 
lot longer. It is not a very good stool if it is. 

Senator KERRY. So we are back to our balance. Good enough. 
Folks, we could spend more time. I unfortunately cannot, not be-
cause  I  do  not  want  to.  Is  that  a  vote  we  have  on?  Well,  that  is 
another reason we cannot. 

I am greatly appreciative. It has been very, very interesting cer-
tainly,  and  we  will  leave  the  record  open  for  a  week  to  allow  any 
other colleague who wants to submit a question and to get your re-
sponse  back.  And  we  thank  you  again.  I  know  you  have  traveled 
a  distance.  Enjoy  Washington  for  a  day  or  so.  And  thank  you  all 
very, very much. I appreciate it. 

We stand adjourned. 
[Whereupon, at 4 p.m., the hearing was adjourned.] 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00070 Fmt 6633 Sfmt 6601 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

A P P E N D I X 

RESPONSE TO WRITTEN QUESTIONS SUBMITTED BY HON. JOHN F. KERRY TO 

BRUCE K. CARLISLE 

Question  1.  You  emphasize  the  need  for  a  national  strategy  on  climate  modeling 
to be coordinated with state, regional and local partners. How should states and cit-
ies  inform  the  development  of  these  models  and  the  products  generated  based  on 
the models? 

Answer.  State  coastal  management  programs  are  primarily  using  models  and 
products  from  the  Intergovernmental  Panel  on  Climate  Change,  academia,  and  the 
National Oceanic and Atmospheric Administration. As these and other entities look 
to  fine-tune  and  expand  their  models  or  begin  working  on  next  generation,  they 
should engage with ‘‘end-users’’ at the regional, state, and local levels to assess their 
needs and identify opportunities for pilot applications. State governments are a very 
effective level to start at as many state programs have existing mechanisms for com-
municating,  coordinating,  and  working  directly  with  counties,  cities,  and  towns. 
State  coastal  programs,  in  particular,  have  a  demonstrated  track  record  of  working 
in  close  coordination  with  both  Federal  agencies  and  local  communities  to  success-
fully  provide  high-quality  products,  services,  and  hands-on  assistance  to  constitu-
ents in and beyond the coastal zone. The Coastal States Organization (CSO) serves 
as  a  central  mechanism  to  coordinate  input  from  and  collaboration  with  state  pro-
grams.  In  addition,  ICLEI—an  international  association  of  local  governments—rep-
resents  another  venue  for  communicating  local  needs.  If  modelers  and  product  de-
velopers need to distribute and/or translate data into local tools and strategies, CSO 
can  also  work  with  the  state  programs  and  ICLEI  to  increase  local  awareness  and 
implementation for real world change. 

Question 2. What information does the state need to advise cities, towns and citi-

zens about the impacts of climate change and how they can prepare and adapt? 

Answer. For coastal communities, municipalities and citizens need to be aware of 
increased  vulnerability  to  storms  and  sea-level  rise.  Therefore,  information  on  the 
potential  magnitude  of  impacts—including  increased  flooding,  shoreline  erosion, 
saltwater intrusion into fresh water aquifers, invasive species, harmful algal blooms, 
and  the  loss  of  coastal  habitats  such  as  beaches  and  marshes—within  the  next  20 
to  50  years  is  paramount  for  states  to  provide  technical  assistance  to  communities 
for  effective  climate  change  adaptation  planning  and  implementation.  Within  these 
issue  areas,  high-resolution  topographic  and  bathymetric  elevation  data  are  re-
quired,  to  be  coupled  with  region-specific  tide  data,  sea  level  rise  projections,  and 
other  key  parameters  in  order  to  identify  the  areas  and  resources  most  vulnerable 
to accelerated sea level rise 

Question 3. How should that information be delivered to end-users? 
Answer.  State  coastal  programs  have  the  ability  to  work  with  the  scientific  com-
munity  hand-in-hand  to  tailor  high-quality  products,  services,  and  hands-on  assist-
ance  to  best  suit  the  needs  of  both  state  and  local  decision-makers  and  resource 
managers.  Massachusetts  has  found  that  concurrent,  targeted  outreach  and  tech-
nical  assistance  is  essential  to  successful  implementation.  To  that  end,  the  Massa-
chusetts Office of Coastal Zone Management has developed the StormSmart Coasts 
program, which is designed to give local decision-makers, and ultimately businesses 
and homeowners, information and tools on coastal resiliency through a user-friendly 
website,  fact  sheets,  workshops,  and  direct  technical  assistance.  We  have  received 
extensive  positive  feedback  from  municipalities,  acclaim  from  national  organiza-
tions,  and  interest  from  a  multitude  of  state  programs.  A  national  version  of 
StormSmart  Coasts  could  be  used  to  communicate  current  information  on  climate 
modeling. 

Question  4.  In  your  written  testimony,  you  discuss  the  state’s  role  in  developing 
high-resolution  shoreline  change  data  and  coastal  high-hazard  zone  delineations. 
Are  you  capable  of  projecting  that  information  into  the  future,  given  the  likely  im-
pacts of global climate change? 

(67) 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00071 Fmt 6601 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

68 

Answer. Shoreline change data and identified flood- and erosion-hazard areas are 
extremely  critical  for  coastal  managers.  However,  identification  of  current  and  fu-
ture risk zones is limited by the state of the science as well as our lack of resources 
to apply current scientific understanding. Erosion rates in Massachusetts and across 
much of the Nation have been increasing as a result of human alterations, changes 
in  sediment  supply,  increasing  frequency  of  storms,  and  sea-level  rise,  therefore, 
funding for updates of shoreline change data every five to 10 years is critical. More 
accurate and up-to-date flood-hazard maps are also critical. 

Question 5. What additional information do you need in order to make those pro-

Answer.  Additional  topographic  and  bathymetric  data  are  needed  by  all  coastal 
states.  These  data  are  often  limited  to  sparse  coverage  over  oceanfront  shorelines 
and  do  not  extend  into  bays  or  estuaries,  where  impacts  will  be  experienced.  In-
creased resolution of the following models is also essential: 

jections? 

local,  embayment-scale  changes 

• Sea-level rise—Coastal states will need more detailed and complex models that 
incorporate 
in  coastal  geomorphology, 
hydrological  conditions,  and  human  alterations  and  responses  (e.g.,  seawalls 
and beach nourishment). 
• Storm  surge—Models  that  incorporate  the  unique  configurations  of  local 
embayments  or  coastline  morphologies,  water  depths,  and  physical  features 
such as bridges and roads are required. 
• Sediment transport, wetland changes, and river hydrology—More information is 
needed  to  better  understand  erosion  and  deposition  cycles,  improve  our  ability 
to predict changing sediment transport, accretion and erosion regimes. 
• Ground water and salt water—More information is required on climate induced 
precipitation, 

through 

altered 

changes 
evapotranspiration, and soil moisture patterns. 

local  hydrologic 

cycles 

to 

Atmospheric models would provide some input to the above, but they are at scales 

not directly useful to state coastal managers. 

RESPONSE TO WRITTEN QUESTIONS SUBMITTED BY HON. DANIEL K. INOUYE TO 

DR. ALEXANDER (SANDY) MACDONALD 

Question.  Are  regional  climate  models  ready  to  be  run  today?  If  not  what  needs 
to  be  done  to  get  them  to  the  point  where  they  can  be  used  and  deliver  adequate 
information? 

Answer.  Regional  climate  models  with  resolutions  of  50  kilometer  (km)  and  finer 
have  been  developed  within  NOAA  and  other  U.S.  Government  agencies,  and  are 
ready to be used for regional projections. In the recent Intergovernmental Panel on 
Climate  Change  (IPCC)  Fourth  Assessment  (AR4),  NOAA  and  other  U.S.  Govern-
ment  agencies  used  a  climate  model  with  ocean  resolution  of  100  km  and  atmos-
pheric resolution of 200 km. Since then, U.S. Government scientists have developed 
and  validated  models  with  much  finer  resolution  (e.g.,  50  km  resolution  in  the  at-
mosphere  and  10–25  km  resolution  in  the  ocean).  Implementing  these  new,  finer 
resolution  models  to  produce  comprehensive  climate  projections  for  reports  such  as 
the  IPCC  Fifth  Assessment  Report  (due  out  in  2013)  would  require  a  100-fold  in-
crease in computer capacity, an estimate compatible with that reported in the 2004 
Federal  Plan  for  High-End  Computing:  Report  of  the  High-End  Computing  Revital-
(http://www.nitrd.gov/pubs/2004lhecrtf/20040702lhecrtf 
ization  Task  Force 
.pdf). NOAA is exploring a Memorandum of Agreement (MOA) with DOE to address 
HPC  requirements  collaboratively.  This  MOA  would  apply  to  NOAA  use  of  DOE 
computing for prototyping models for climate research. 

RESPONSE TO WRITTEN QUESTIONS SUBMITTED BY HON. JOHN KERRY TO 

DR. ALEXANDER (SANDY) MACDONALD 

Question  1.  In  your  written  testimony,  you  discuss  the  smaller-resolution  proto-
type  models  NOAA  has  developed.  However,  you  note  that  NOAA’s  computer  re-
sources  are  inadequate  to  run  comprehensive  simulations  of  climate  change  using 
these models. What level of computing resources do you need? How much does that 
cost?  Should  these  resources  be  centralized  at  NOAA,  or  is  it  appropriate  to  have 
several computing centers that can run these advanced models? 

Answer.  The  Nation’s  climate  mission  requires  dedicated  support  for  large  scale 
High Performance Computing (HPC). The Administration’s FY 2009 Budget Request 
for  NOAA  provides  a  set  of  priorities  to  sustain  core  mission  services  and  address 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00072 Fmt 6601 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

69 

some  of  our  highest  priority  program  needs.  Roughly  $19  million  of  the  FY  2009 
President’s  budget  request  for  NOAA  is  for  climate  research  HPC.  Currently,  these 
funds are allocated to the following high-priority activities: 

Science Program Synthesis and Assessment products. 

• The  development  and  application  of  the  next  generation  of  climate  change  and 
Earth  System  Models,  in  preparation  for  the  IPCC  Fifth  Assessment  Report 
(AR5; due out in 2013), including new atmospheric and global ocean component 
models. 
• Support  for  climate  modeling  requirements  in  developing  the  Climate  Change 
• Computational support for the World Meteorology Organization/United Nations 
• Computational support for developing a modeling capability for monitoring and 
• Continued limited integrations of high-resolution atmospheric models to support 
• Support  for  reanalysis  and  reforecast  of  1979–2008  using  the  coupled  Climate 

making predictions of Atlantic Meridional Overturning Circulation changes. 

Environment Programme Stratospheric Ozone Assessments. 

the North American Climate Change Assessment Program. 

Forecast System. 

NOAA’s  global  climate  models,  as  well  as  other  U.S.  models,  are  among  the  best 
in the world. Currently, the HPC available to the Nation’s climate scientists allows 
global climate models to resolve climate research questions down to the scale of con-
tinents.  Additional  research  HPC  capacity  for  climate  would  be  targeted  toward 
using  currently  available  higher  resolution  models  to  meet  stakeholder  demand  for 
regional  to  local  scale  climate  information.  The  additional  HPC  would  also  be  used 
to produce more comprehensive climate outlooks with advanced models that improve 
treatments  of  processes  critical  to  our  understanding  of  climate  change,  such  as 
aerosols  and  clouds.  These  advanced  models  would  also  include  processes  that  are 
missing  in  today’s  models,  such  as  ice  sheet  melting  that  is  crucial  to  address  sea- 
level rise. Another example of what advanced models would include are complex bio-
geochemical  cycles  that  can  be  applied  to  answer  questions  about  the  carbon  cycle 
and interaction of climate and ecosystems, such as the effects of ocean acidification. 
In July 2003, the Climate Change Science Program specifically identified two cen-
ters, NOAA’s Geophysical Fluid Dynamics Laboratory and the National Centers for 
Atmospheric  Research  (NCAR),  to  produce  sophisticated  simulations,  such  as  those 
required  for  assessment  by  the  IPCC.  Scientific  uncertainty,  numerical  algorithm 
variations,  non-unique  parameterizations  of  sub-grid  size  phenomena,  and  gaps  in 
knowledge  make  it  essential  that  multiple  models  be  used  to  explore  different  ap-
proaches to improve understanding of the climate of the global integrated Earth sys-
tem.  At  this  time,  NOAA  is  exploring  partnerships  with  the  Department  of  Energy 
and NCAR to identify the most cost-effective solution for facilities to house the Na-
tion’s climate computing. Should these activities be successful, leveraging these na-
tional  partnerships  and  adopting  a  phased  approach  to  implementing  the  required 
level of computing represents an executable strategy for meeting the Nation’s grow-
ing climate information needs. 

Question 2. In your written testimony, you discuss NOAA’s Modular Ocean Model. 
Are  we  capable  of  modeling  the  oceans  with  the  same  level  of  confidence  that  we 
model the atmosphere? Do we need more ocean observations to feed into those mod-
els? 

Answer. At the global scale, the ocean’s role in climate change is governed by well 
understood  scientific  principles  which  are  suitably  represented  by  the  present  class 
of  climate  models.  The  NOAA  Geophysical  Fluid  Dynamics  Laboratory’s  Modular 
Ocean Model (MOM) is the world’s most widely used numerical model for simulating 
the ocean circulation at the global scale and for understanding and predicting ocean 
climate phenomena. MOM is used for operational seasonal (including El Nin˜ o) fore-
casting at NOAA’s National Weather Service, and was prominently used by several 
groups  in  the  U.S.  and  worldwide  in  the  recent  IPCC  Fourth  Assessment  Report. 
Uncertainty remains, however, when asking questions about regional spatial pat-
terns  and  precise  time  scales  of  the  ocean’s  response  to  climate  change  (e.g.,  how 
fast  and  how  much  will  the  Massachusetts  coastal  waters  warm  and  the  sea  levels 
rise?).  Such  regional  questions  represent  a  grand  challenge  to  be  addressed  by  the 
next generation of global climate models. 

The  geography  of  the  world’s  ocean  basins  is  extremely  complex,  with  many  rel-
atively  small  scale  features  (e.g.,  continental  shelves,  narrow  straits,  and  marginal 
seas) playing an important role affecting key features of large scale ocean properties 
(e.g., heat, salinity, nutrients). In addition, the spatial scales for ocean ‘‘weather ed-
dies’’ is roughly 10 times smaller than the atmospheric weather eddies, thus making 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00073 Fmt 6601 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

70 

it roughly 10 x 10 times (factor of 10 for each of the two horizontal directions) more 
computationally  expensive  to  represent  ocean  eddies  in  a  numerical  simulation. 
These two characteristics of the ocean underscore the benefits of model grid resolu-
tion  finer  than  10  km  resolution,  to  address  questions  of  regional  climate  impacts, 
including those most pertinent to the U.S. coastal zones. 

An  ocean  model  is  evaluated  by  confronting  simulations  with  ocean  observations. 
This  evaluation  in  turn  provides  feedback  to  observing  system  design  (i.e.,  do  we 
need  more  observations,  and  if  so,  where?).  The  scientific  reliability  of  global  ocean 
climate simulations will match the level of atmospheric simulations through: the de-
velopment  of  refined  resolution  global  ocean  climate  simulations;  targeted  ocean 
field  studies,  observations,  long-term  monitoring;  and  theoretical  studies,  which  en-
able a rigorous assessment of the models based on the real ocean system. 

Question 3. In your written testimony, you note that NOAA makes large amounts 
of  your  climate  model  output  freely  available.  Is  this  information  accessible  only  to 
advanced  researchers,  or  are  end-users  able  to  access  and  utilize  this  data?  Is  the 
information available in a format that is useful for end-users? 

Answer. NOAA is committed to making our climate model output available to the 
public.  With  respect  to  access,  the  NOAA  Operational  Model  Archive  and  Distribu-
tion System (NOMADS), provides open access to climate model output. NOAA’s Geo-
physical  Fluid  Dynamics  Laboratory  modeling  center  provides  climate  data  on  the 
NOMADS publicly accessible data portal, and works directly with researchers to fa-
cilitate  use  of  the  data.  Because  information  portals  and  access  systems  work  best 
when we also invest in partnerships with decisionmakers, NOAA also works directly 
with end-users to help them interpret model projections in a manner useful to their 
needs.  There  are  different  types  of  users  of  our  climate  model  output:  climate  re-
searchers;  researchers  who  study  the  impact  of  climate  change  on  various  sectors 
(e.g.,  agriculture,  public  health,  air  quality,  water  resources,  migration,  inter-
national  security,  travel,  trade);  and  the  engaged  public  (e.g.,  policymakers,  urban 
planners, state and regional resource managers, or even curious students). Some ex-
amples of NOAA working successfully with different users include: 

• Department  of  Energy’s  Program  for  Climate  Model  Diagnosis  and  Inter-
comparison.  Through  this  program,  NOAA  climate  model  output  from  simula-
tions of past, present and future climate was used to prepare the IPCC’s Fourth 
Assessment Report on Climate Change. 

• NOAA’s  Earth  System  Research  Laboratory  (ESRL),  and  the  Climate  Program 
Office’s  Regional  Integrated  Sciences  and  Assessments  (RISA)  Program  gen-
erate  regionally  downscaled  projections  of  future  climate  change.  Through  sus-
tained  interaction  with  stakeholders,  ESRL  and  RISAs  also  provide  regionally 
tailored analyses that transform the global climate projections into value-added 
decision-relevant information. 

• The National Integrated Drought Information System Drought Portal, an inter-
agency  effort  coordinated  by  NOAA,  provides  valuable  information  to  stake-
holders  such  as:  early  warning  about  emerging  and  anticipated  droughts;  as-
similated and quality controlled data about droughts; model-based drought out-
looks  and  forecasts;  information  about  risk  and  impact  of  droughts  to  different 
agencies and stakeholders; information about past droughts for comparison and 
to  understand  current  conditions;  and  explain  how  to  plan  for  and  manage  the 
impacts of droughts. 

NOAA,  with  its  mission  to  act  as  a  research  and  information  service  on  environ-
mental issues, is uniquely poised to serve the range of climate data needs, from re-
searchers to end-users. 

Question  4.  Given  likely  investment  and  innovation  in  computing  infrastructure, 
when  would  data  from  the  next  generation  of  climate  models  be  available  to  end- 
users and researchers? 

Answer.  NOAA  is  committed  to  sharing  climate  model  data  with  end-users  and 
researchers  as  rapidly  as  possible.  However,  before  data  can  be  shared,  the  data 
must  be  verified  and  validated  for  scientific  credibility  by  peer  review,  and  pack-
aging  and  quality  assurance  tasks  must  be  completed.  As  in  the  past,  NOAA 
prioritizes  computing  infrastructure  acquisitions  for  the  following:  simulations  and 
analysis to understand and project climate change; archival storage of large volumes 
of  data  generated  by  these  simulations;  and  for  networking,  to  deliver  the  data  to 
our  partners  and  stakeholders.  NOAA  modeling  centers  have  long-term  experience 
in acquiring and maintaining a balanced infrastructure with available resources. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00074 Fmt 6601 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

71 

RESPONSE TO WRITTEN QUESTIONS SUBMITTED BY HON. JOHN F. KERRY TO 

EDWARD SARACHIK 

Question  1.  The  National  Research  Council  (NRC)  released  reports  in  1998  and 
2001  stating  that  the  United  States  lagged  behind  other  nations  in  our  ability  to 
model climate change. As chair of the 2001 NRC Panel, do you believe that our ca-
pacity  has  improved  since  these  reports  were  released?  What  improvements  have 
been  made?  What  challenges  does  the  U.S.  climate  modeling  community  still  face? 
Answer.  Our  capacity  for  the  kind  of  climate  modeling  that  the  IPCC  does  has 
indeed  increased—we  now  have  higher  resolution  coupled  climate-biogeochemistry 
models. But the kind of models the IPCC does is in support of the Framework Con-
vention  on  Climate  Change,  and  this  involves  getting  global  averages  correct  in 
order  to  avoid  dangerous  interference  with  the  climate  system.  On  the  other  hand, 
the  kind  of  modeling  that  this  hearing  was  about,  namely  climate  modeling  for  the 
use  of  decisionmakers  and  end-users,  involves  getting  the  regional  scales  right  and 
this has not improved. The IPCC itself recognizes that the results of its models are 
valid on space scales of 3,000 miles (continental scale) and this is not useful for any 
decisionmaking other than mitigation of greenhouse gases. The failure of our major 
large  scale  modeling  institutions  (we  only  have  two—NCAR  and  GFDL)  to  address 
regional  problems  is  the  failure  to  get  climate  variability  correct—annual  cycle,  El 
Nin˜ o,  Pacific  Decadal  Oscillation  and  North  Atlantic  Oscillation.  The  failure  to  get 
climate  variability  correct  is  due  to  the  general  inadequacy  of  our  (sub-critical)  cli-
mate  community  to  address  new  problems—a  combination  of  lack  of  sustained  ob-
servations,  absence  of  the  development  of  a  model-based  climate  analysis  (which 
would serve as the primary material for analysis of climate variability), and general 
inability of the CCSP to concentrate resources on research problems outside the di-
rect interest of the participating government agencies. 

Question 2. In your written testimony, you emphasize that the university research 
community needs access to the supercomputers themselves, as well as access to the 
information generated by the models. What is the best way to facilitate that? 

Answer.  It  really  wouldn’t  help  to  simply  make  supercomputer  time  available 
since the enormity of coupled climate models is generally beyond the capacity of in-
dividuals  or  small  groups  of  individuals  to  deal  with.  The  NCAR  Community  Cli-
mate  Systems  Model  is  an  excellent  template,  one  that  has  the  broad  community 
interacting with a core NCAR group—this is the best synergy between model build-
ers  at  NCAR  and  model  users  in  the  distributed  community  and  is  far  more  than 
the sum of the parts. Supercomputer time needs to be made available to enable this 
synergy as well as funding. What the U.S. needs is many of these core model build-
ing  institutions  interacting  with  anyone  that  has  something  to  contribute.  At  the 
present  time,  the  Hadely  Centre  in  England  is  funded  at  more  than  GFDL  and 
NCAR  CCSM  combined,  this  in  a  country  with  10  percent  of  the  GDP  of  the  U.S. 
By  this  standard,  the  U.S.  should  have  ten  or  twenty  modeling  centers  each  with 
its  own  supercomputer,  most  interacting  with  the  external  community.  Some  of 
these  centers  should  be  regional  centers  concentrating  on  the  climate  problems  of 
the regions in which they are sited. 

Question  3.  Several  witnesses  have  emphasized  the  need  to  integrate  observa-
tional  data  into  climate  models.  In  your  opinion,  in  addition  to  incorporating  this 
data, do we need additional observational data? 

Answer.  What  we  need  is  sustained  and  accurate  data  in  the  atmosphere,  ocean, 
land  and  cryosphere  adequate  to  define  the  long  term  climate  patterns:  annual 
cycle,  El  Nin˜ o-Southern  Oscillation,  Pacific  Decadal  Oscillation,  and  North  Atlantic 
Oscillation,  and  physical  climate  impacts:  soil  moisture,  stream  flow,  and  glacier 
and  ice  sheet  evolution.  This  is  a  finite  (but  expensive)  aim.  It  would  also  be  desir-
able to have data on ecological impacts, especially fisheries, forests, and disease vec-
tors.  We  also  need  to  be  able  to  deal  with  opportunities  as  they  arise—at  the  mo-
ment,  the  melting  of  the  Greenland  Ice  sheet  is  so  much  on  scientists’  and  the 
public’s  mind  that  one  would  expect  our  science  agencies  to  undertake  a  large  and 
concerted  effort  to  measure  the  melting  of  Greenland—so  far  this  hasn’t  happened. 

RESPONSE TO WRITTEN QUESTIONS SUBMITTED BY HON. JOHN F. KERRY TO 

DR. DANIEL A. REED 

Question 1. In your written testimony, you discuss the need to integrate observa-
tional  data  into  our  climate  models.  What  are  the  limitations  in  our  ability  to  do 
that today? 

Answer.  Data  assimilation,  the  integration  of  observational  data  with  computa-
tional models at each cycle, remains a technically challenging problem, both mathe-

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00075 Fmt 6601 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

72 

matically and technically. We have made progress, but much work remains. My cli-
mate modeling colleagues can best speak to the modeling issues, but I can comment 
on  the  computational  and  observational  aspects.  The  diversity  and  scale  of  data— 
from  current  satellites  to  geological  records—make  the  problem  complex.  Moreover, 
the  spatial  and  temporal  data  gaps  exacerbate  the  integration  problems.  Finally, 
there  is  simply  the  matter  of  scale.  The  volume  of  data  we  must  manage  and  inte-
grate grows daily. 

Question  2.  How  many  different  climate  models  do  we  need,  here  in  the  U.S.  as 
well  as  internationally?  Should  we  be  collaborating  with  international  partners  on 
model  development,  or  is  there  value  in  having  separate  models  in  separate  coun-
tries? 

Answer.  Again,  this  is  a  question  best  answered  by  my  climate  modeling  col-
leagues. However, it is important to remember that any model is an approximation 
of the actual system. By necessity, each model includes simplifying abstractions that 
may  fail  to  capture  salient  aspects  of  the  real  system.  These  simplifications  make 
the  model  tractable  and  allow  us  to  evaluate  the  models  computationally  in  a  rea-
sonable  time  on  available  high-performance  computing  systems.  Thus,  the  accuracy 
of any model depends on our current knowledge and understanding of climatic proc-
esses,  the  skill  of  the  model  builders  and  available  computing  resources.  Advances 
in  any  of  those  areas  can  improve  model  accuracy.  For  this  reason,  we  often  evalu-
ate  ensembles  of  models,  examining  the  common  and  differing  behaviors  to  illu-
minate potential errors. 

Thus,  I  believe  we  need  multiple  models,  with  differing  assumptions  and  ap-
proaches,  enabled  by  a  broad  international  collaboration.  These  models  can  and 
should be configured and specialized to understand regional climatic effects. 

Question 3. You describe the need for an integrated, interagency effort to address 
the  range  of  research,  software,  data  storage  and  computing  challenge  associated 
with  climate  modeling.  How  should  that  be  structured?  Should  it  be  led  by  NSF, 
NOAA or another agency? Is the Global Change Research Program capable of man-
aging such an effort? 

Answer.  Many  Federal  agencies  support  climate  change  research,  with  differing 
scales  and  approaches.  This  has  historically  been  the  strength  of  the  U.S.  research 
funding  environment.  However,  assessing  the  impact  of  climate  change  is  an  out-
come-driven activity. I believe this is best managed by a single agency with the re-
sources  and  the  mandate  to  deliver  detailed  global  and  regional  assessments,  not 
a basic research agency. 

Question  4.  In  your  written  testimony,  you  say  that  ‘‘climate  change  modeling  is 
a deep and challenging scientific problem that requires computing infrastructure at 
the  largest  scale.’’  The  National  Center  for  Atmospheric  Research  (NCAR)  in  Boul-
der  is  the  only  supercomputing  facility  focused  strictly  on  climate  change,  but  you 
indicate that NCAR cannot deliver timely and accurate predictions. Should we focus 
on  establishing  NCAR  as  the  premier  climate  modeling  center  in  the  country  and 
expanding our capabilities there? Or do we need a structure that supports advanced 
climate modeling at various institutions around the country? 

Answer.  We  need  a  balanced  approach  based  on  a  pyramid  model  of  computing. 
The  pyramid  apex  is  one  or  more  premier  high-performance  computing  systems  for 
climate change modeling—substantially larger than anything available today. How-
ever, the apex must be supported by a diverse set of smaller systems spread across 
our universities and national laboratories. 

NCAR is one of the possible sites for an apex climate change modeling supercom-
puting  center,  but  there  are  other  viable  sites  as  well  (e.g.,  at  one  of  our  national 
laboratories). The site selection should be derived from a national analysis of avail-
able infrastructure (people and facilities), costs and community engagement. 

Question  5.  Given  the  apparent  limitations  of  using  off-the-shelf  parallel  proc-
essors  for  the  purpose  of  climate  modeling,  should  we  be  building  special-purpose 
supercomputers  for  this  purpose?  If  so,  is  there  any  role  for  the  private  sector  in 
developing these systems? 

Answer.  Yes,  for  this  and  many  other  proposes  of  national  importance.  We  need 
greater  investment  in  purpose-built  supercomputers  that  have  been  architected  for 
critical national problems, just as we invest in purpose-built defense infrastructure. 
Climate  modeling  is  but  one  critical  example  of  such  a  national  scientific  problem; 
there are many others related to national security, biomedicine, energy research and 
other domains. 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00076 Fmt 6601 Sfmt 6621 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

73 

I believe a coordinated program of research, development and production must in-
volve  government,  academia  and  private  industry.  In  the  end,  the  systems  will  be 
built by industry if the government is a willing supporter and purchaser of the pur-
pose-built systems. 

Æ 

VerDate Nov 24 2008  14:13 Jul 31, 2012 Jkt 052754 PO 00000 Frm 00077 Fmt 6601 Sfmt 6611 S:\GPO\DOCS\75346.TXT SCOM1 PsN: JACKIE

